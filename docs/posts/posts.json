[
  {
    "path": "posts/5. 애증의 Dacon/",
    "title": "애증의 Dacon",
    "description": "How to execute Random Forest, Logistic, XGboost, LightGBM Model?",
    "author": [
      {
        "name": "WooramSeong",
        "url": {}
      }
    ],
    "date": "2021-05-31",
    "categories": [],
    "contents": "\n\nContents\n필요 라이브러리\n데이터 불러오기\n데이터 기본정보 확인\nskim(train)\nskim(test)\n중복값을 제거해보자.\n\nrecipe를 통한 전처리 입력\njuice를 통한 전처리 즙짜기\n튜닝 준비하기\n5 fold vs 10 fold를 실행해보자.\nstacking 준비하기\n\n랜덤 포레스트를 실행해보자,\n튜닝된 모델 학습하기\n\nLogitstic model을 돌려보자.\nXGboost 를 실행해보자\nlight GBM 를 실행해보자\nStacking 을 실행해보자.\n그리드 재설정\n\n예측하기\n\n이번 글은 코드를 실행하지는 않으려고 한다. 왜냐하면 너무 오래걸리기 때문이죠..\n코드만 감상하시길 바랍니다.\n필요 라이브러리\n\n\ncode\n\nlibrary(parsnip)\nlibrary(xgboost)\nlibrary(magrittr)\nlibrary(tidymodels)\nlibrary(tidyverse) \nlibrary(skimr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ranger)\nlibrary(yardstick)\nlibrary(stacks)\nlibrary(randomForestExplainer)\nlibrary(corrplot)\nlibrary(tictoc)\nlibrary(nnet)\n#install.packages('devtools')\n#remotes::install_github(\"curso-r/treesnip\")\nlibrary(treesnip)\n#devtools::install_github(\"curso-r/rightgbm\")\n#rightgbm::install_lightgbm()  \nlibrary(lightgbm)\n\n\n\n데이터 불러오기\n\n\ncode\n\n  file_path <- \"C:/Users/GIGABYTE/Desktop/응용통계학/Dacon\"\n  \n  files <-list.files(file_path)\n  \n  files\n  #각 변수의 이름을 `janitor` 패키지로 말끔하게 바꿔준다.\n  \n  \n  train <- read_csv(file.path(file_path, \"train.csv\"),\n                    col_types = cols(\n                      credit = col_factor(levels = c(\"0.0\", \"1.0\", \"2.0\"))\n                    )) %T>% \n    suppressMessages() %>% \n    janitor::clean_names()\n  test <- read_csv(file.path(file_path, \"test.csv\")) %T>%\n    suppressMessages() %>% \n    janitor::clean_names()\n\n\n\n데이터 기본정보 확인\n\n\ncode\n\ntrain %>% \n      head() %>% \n       kable() %>% \n     kableExtra::kable_styling(\"striped\") %>% \n     kableExtra::scroll_box(width = \"100%\")\n\n\n\n#각 데이터 셋의 변수명을 살펴보자.\n먼저 test 데이터에는 우리가 예측하고 싶은 변수인 credit 변수가 들어있지 않은 것을 알 수 있다.\n데이터를 훑어보기 위해서 skim() 함수를 이용하자. 이 함수는 데이터에 들어있는 변수들을 타입 별로 분석해서 리포트를 작성해준다.\nskim(train)\n결과를 살펴보자. 먼저 결측치가 상대적으로 많이 없는 착한? 데이터이다. character 변수의 complete rate를 살펴보면 모든 변수가 1이고, occyp_type 변수만이 결측치가 8171개가 존재하는 것을 알 수 있다. 또한 고맙게도 numeric 변수의 결측치는 하나도 없다!😆\n같은 함수를 사용해서 test 셋을 보면 똑같은 패턴을 가지고 있는 것을 알 수 있다.\nskim(test)\n\n\ncode\n\n# 시각화\n  #베이스 라인을 잡은 문서이니 간단하게 시각화 하나만 하고 넘어가자. (코드를 응용해서 다른 변수에 대한 상관 관계를 볼 수 있을 것이다.)\n  \npre_train<-train[,c(-1,-17)]\npre_train %<>% mutate(income_Per_Familymember = income_total/family_size)\n#변수 중요도가 높은 Begin_month, income_total, yrs_birth, income_PM\n#Preliminary_fit2 <- ranger(credit ~ ., data = pre_train , importance = \"impurity_corrected\")\n#pvalues<-importance_pvalues(Preliminary_fit2, method = \"janitza\") \n#pvalues\n#p-value가 0에 가까운 Begin_month, days_birth, days_employed 을 발견\n#train %>%\n  \n#  ggplot(aes(x = factor(credit), y = income_total)) +\n  \n#  geom_boxplot()\n## Credit이 바뀌어도 income_total의 유의미한 결과가 보이지 않음\n#train %>%\n  \n#  ggplot(aes(x = factor(credit), y = days_birth)) +\n  \n#  geom_boxplot()\n#train %>%\n  \n#  ggplot(aes(x = factor(credit), y = begin_month)) +\n  \n#  geom_boxplot()\ntrain %>%\n  \n  ggplot(aes(x = factor(credit), y = days_employed)) +\n  \n  geom_boxplot()\n## 데이터 365,243 처리를 어떻게 해야 하는가 365,243,을 제외한 최댓값 or 최솟값 or 중앙값\ntrain %>% filter(days_employed>300000)\ntrain %>% filter(days_employed>300000)%>%select(income_type,days_employed)\nmin(Days_employed)\nmax(Days_employed)\nmedian(Days_employed)\ntrain$days_employed[train$days_employed == 365243] <- -1977\ntest$days_employed[test$days_employed == 365243] <- -1977\n## Credit이 바뀌어도 income_total의 유의미한 결과가 보이지 않음\npost_train<-train\npost_train$credit<-as.numeric(post_train$credit)\ncorrplot(cor(post_train[,c(20,19,12,11,6)]))\n#begin_Month 데이터 전처리가 핵심인거 같다.\ntrain %>%\n  \n ggplot(aes(x = factor(credit), y = income_total)) +\n  \n  geom_boxplot() +\n  \n  facet_grid(. ~ occyp_type)\ndim(train)\nmin(train$income_total)\ntrain %<>% filter(income_total<1250000 & income_total>27000)\ndim(train)\ncolnames(train)\ntrain %>%\n  \n ggplot(aes(x = factor(credit), y = begin_month)) +\n  \n  geom_boxplot() +\n  \n  facet_grid(. ~ occyp_type)\n#  train %>%\n  \n#  ggplot(aes(x = factor(credit), y = begin_month)) +\n  \n#  geom_boxplot() +\n  \n#  facet_grid(. ~ income_type)\n  \n  train %>%\n    \n    ggplot(aes(x = factor(credit), y = begin_month)) +\n    \n    geom_boxplot() +\n    \n    facet_grid(. ~ income_type)\ntrain %>%\n ggplot(aes(x = factor(credit), y = family_size)) +\n  geom_boxplot() \ntrain %>% select(income_type,begin_month) %>% filter(income_type==\"Student\")\ntrain %>% select(credit,occyp_type,begin_month) %>% filter(occyp_type=='HR staff' & credit == \"1.0\") %>% arrange(desc(begin_month))\ntrain %>% select(credit,occyp_type,begin_month) %>% filter(occyp_type=='Realty agents' & credit == \"1.0\") %>% arrange(begin_month)\ntrain %>% select(credit,occyp_type,begin_month) %>% filter(occyp_type=='Waiters/barmen staff' & credit == \"1.0\") %>% arrange(begin_month)\nintersect(which(train$occyp_type == 'HR staff'),which(train$begin_month == -8))\nintersect(which(train$occyp_type == 'Realty agents'& train$credit== \"1.0\"),which(train$begin_month == -53))\nintersect(which(train$occyp_type == 'Realty agents' & train$credit== \"1.0\"),which(train$begin_month == -43))\nintersect(which(train$occyp_type == 'Waiters/barmen staff' & train$credit== \"1.0\"),which(train$begin_month == -56))\nintersect(which(train$income_type == 'Student'),which(train$begin_month == -60))\n#2990,11276,15649,20515,25602, 행을 제거하자\ntrain<-train[c(-2990,-11276,-15649,-20515,-25602),]\n## income_total이 상관계수가 0에 가까워서 사람수로 나누어 평균 수입으로 변수를 만들려 했으나 상관계수가 역시나 적었다. \n#train$credit<-as.numeric(train$credit)\n#train %<>% mutate(income_PM = income_total/family_size)\n#train$income_PM\n#sum(is.na(train$income_PM))\n#cor(train[,c(20,19,12,11,6,21)])\n#corrplot(cor(train[,c(20,19,12,11,21)]))\n#목표 변수인 credit은 낮을 수록 높은 신용의 신용카드 사용자를 의미 한다고 한다. Commercial associate 인 경우 신용이 제일 낮은 그룹의 수입의 중앙값이 제일 높다. 돈을 많이 벌수록 돈 갚은 개념이 없어지는 것인가? 재미있는 현상이다. 학생 클래스의 경우 train 데이터에 셋이 많이 없다는 것을 알 수 있다. 추후에 다른 클래스로 통합을 시키는 것이 좋을 것이다.\n\n\n\n중복값을 제거해보자.\n\n\ncode\n\nbirth_table<-as.data.frame(table(train$days_birth))  \nbirth_table%>%filter(Freq>10)%>% arrange(desc(Freq))\n#days_birth -12676,-15519,-14667 탐색!!\na1<-train%>%filter(days_birth == -12676) %>% select(index,days_employed,income_total,begin_month,days_birth,family_size,credit)%>%arrange(days_employed,income_total,begin_month,index)\na2<-train%>%filter(days_birth == -15519) %>% select(index,days_employed,income_total,begin_month,days_birth,family_size,credit)%>%arrange(days_employed,income_total,begin_month,index)\na3<-train%>%filter(days_birth == -14667) %>% select(index,days_employed,income_total,begin_month,days_birth,family_size,credit)%>%arrange(days_employed,income_total,begin_month,index)\ntrain$credit<-as.numeric(train$credit)\n#train_max<-train%>%group_by(days_employed,gender,income_total,credit,family_size,child_num) %>% mutate(begin_month=min(begin_month))\n#train_min<-train%>%group_by(days_employed,gender,income_total,credit,family_size,child_num) %>% mutate(begin_month=max(begin_month))\n#train_median<-train%>%group_by(days_employed,gender,income_total,credit,family_size,child_num) %>% mutate(begin_month=median(begin_month))\n#corrplot(cor(train_max[,c(20,19,12,11,6)]))\n#corrplot(cor(train_min[,c(20,19,12,11,6)]))\n#corrplot(cor(train_median[,c(20,19,12,11,6)]))\n#corrplot(cor(train[,c(20,19,12,11,6)]))\n#데이터 정렬 행번호가 가장 빠른 애들만 남게 되기 때문에 발급일이 가장 오래된 것들을 남기기로 하자.\ncolnames(train)\n#train_order <- train[order(train[,'begin_month'],decreasing = FALSE), ]\n#train_order <- train_order[!duplicated(train[,c('days_birth','work_phone','phone','reality','house_type','car','days_employed','edu_type','occyp_type','child_num','income_total','income_type','family_size','begin_month','credit')]),]\n#train_order$credit<-as.numeric(train_order$credit)\n#corrplot(cor(post_train[,c(20,19,12,11,6)]))\n#corrplot(cor(train_order[,c(20,19,12,11,6)]))\n#이번에는 최근 발급일을 기준으로 남기기로 하자.\n#train_order <- train[order(train[,'begin_month'],decreasing = TRUE), ]\n#train_order <- train_order[!duplicated(train[,c('days_birth','work_phone','phone','reality','house_type','car','days_employed','edu_type','occyp_type','child_num','income_total','income_type','family_size','begin_month','credit')]),]\n#train_order$credit<-as.numeric(train_order$credit)\n#corrplot(cor(post_train[,c(20,19,12,11,6)]))\n#corrplot(cor(train_order[,c(20,19,12,11,6)]))\n## Begin_month 중복값 중 최근 발급일을 남긴 결과 상관 관계가 높아지는 결과가 나타났다.\n#train_order_inc <- train[order(train[,'begin_month'],decreasing = FALSE), ]\n#train_order_inc <- train_order_inc[!duplicated(train[,c('gender','edu_type','flag_mobil','email','days_birth','work_phone','phone','reality','house_type','car','days_employed','edu_type','occyp_type','child_num','income_total','income_type','family_size','credit')]),]\n#train_order_dec <- train[order(train[,'begin_month'],decreasing = TRUE), ]\n#train_order_dec <- train_order_dec[!duplicated(train[,c('gender','edu_type','flag_mobil','email','days_birth','work_phone','phone','reality','house_type','car','days_employed','edu_type','occyp_type','child_num','income_total','income_type','family_size','credit')]),]\n#train_union<-union(train_order_dec,train_order_inc)\n#train_order$credit<-as.numeric(train_order$credit)\n#train_order_inc$credit<-as.numeric(train_order_inc$credit)\n#train_order_dec$credit<-as.numeric(train_order_dec$credit)\n#train_union$credit<-as.numeric(train_union$credit)\n#corrplot(cor(post_train[,c(20,19,12,11,6)]))\n#corrplot(cor(train_order_inc[,c(20,19,12,11,6)]))\n#corrplot(cor(train_order_dec[,c(20,19,12,11,6)]))\n#corrplot(cor(train_union[,c(20,19,12,11,6)]))\n#train_order_dec\n#train<-train_union\n## 재밌는점은 상관관계가 높다고 해서 mean_log_loss가 낮아지는건 아님\n## 오히려 상관관계가 높은 쪽에서 log loss 값이 더 크게 나옴 ~_~\n# 전처리 하기\n#`tidymodels`에서는 전처리를 할 때 `recipe` 라는 패키지를 사용한다. 이 패키지에는 전처리를 하는 방법을 음식 레피시 처럼 적어놓는다고 생각하면 쉽다.\n## 전처리 사항들\n# 결과값인 credit 변수와 character 타입의 변수들을 factor 변수로 바꿔주자.\n# 나이와 직업을 가진 기간을 년수로 바꿔준다.\n\n\n\nrecipe를 통한 전처리 입력\n\n\ncode\n\ncredit_recipe <- train %>% \n  \n  recipe(credit ~ .) %>% \n  \n  # age and employment period in yrs\n  \n  #step_mutate(yrs_birth = -ceiling(days_birth/365),\n              \n  #           yrs_employed = -ceiling(days_employed/365)) %>% \n  \n  step_rm(index,flag_mobil, child_num) %>%\n  \n  #child_num은 family size랑 상관계수가 매우 높아 제거해주었다. \n  \n  #flag_mobil은 다 값이 1로 동일하여 제거해주었다.\n  \n  step_unknown(occyp_type) %>% \n  \n  step_integer(all_nominal(), -all_outcomes()) %>% \n  \n  step_corr(all_predictors(), -all_outcomes()) %>% \n  \n  step_scale(all_predictors(), -all_outcomes()) %>%\n  \n  step_nzv(all_predictors(), -all_outcomes()) %>%\n  \n  step_center(all_predictors(), -all_outcomes()) %>% \n  \n  prep(training = train)\nprint(credit_recipe)\n\n\n\njuice를 통한 전처리 즙짜기\n\n\ncode\n\n#`juice()` 함수를 통해서 recipe에 입력된 전처리를 짜낸 데이터를 얻어온다.\ntrain2 <- juice(credit_recipe)\ntest2 <- bake(credit_recipe, new_data = test)\nhead(train2)\nhead(test2)\ntrain2$credit<-as.factor(train2$credit)\n#다음과 같이 결측치 없이 잘 코딩된 데이터를 얻었다는 것을 확인 할 수 있다.\ntrain2 %>%\n    \n  map_df(~sum(is.na(.))) %>%\n  \n  pivot_longer(cols = everything(),\n               \n               names_to = \"variable\",\n               \n               values_to = \"na_count\") %>% \n  \n  filter(na_count > 0)\n\n\n\n튜닝 준비하기\nvalidation_split() 함수를 사용하여 평가셋을 분리한다. 한 단계 더 나아간 cross validation은 vfold_cv()함수에서 제공하니 찾아보도록 하자.\n5 fold vs 10 fold를 실행해보자.\n\n\ncode\n\nset.seed(2002)\nvalidation_split <- vfold_cv(v=10, train2, strata = credit)\n#validation_split <- validation_split(train2, prop = 0.3,   strata = credit)\nvalidation_split\n\n\n\nstacking 준비하기\n\n\ncode\n\nctrl_res <- control_stack_resamples()\nctrl_grid <- control_stack_grid()\n\n\n\n랜덤 포레스트를 실행해보자,\nmtry와 min_n을 어떻게 정할지를 평가셋을 통해서 결정할 것이므로, tune()를 사용해서 tidymodels에게 알려주도록 한다.\n\n\ncode\n\ncores <- parallel::detectCores() -1\ncores\ntune_spec <- rand_forest(mtry = tune(),\n                         \n                         min_n = tune(),\n                         \n                         trees = 1000) %>% \n  \n  set_engine(\"ranger\",\n             \n             num.threads = cores) %>% \n  \n  set_mode(\"classification\")\n# from param tune\n#param_grid <- tibble(mtry = c(4,4,4,4), min_n=c(3,4,5,6)) # mtry=3으로 고정시키자\nparam_grid <- grid_random(finalize(mtry(), x = train2[,-1]), min_n(),size = 5000, filter = 2<mtry & mtry<5 & min_n<15 & 2<min_n)\nparam_grid\n# 워크 플로우 설정\nworkflow <- workflow() %>%\n  \n  add_model(tune_spec) %>% \n  \n  add_formula(credit ~ .)\n# 모델 튜닝 with tune_grid()\n# Tuning trees\ntic()\ntune_result <- workflow %>% \n  \n  tune_grid(validation_split,\n            grid = param_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res)\ntoc()\ntune_result %>% \n  \n  collect_metrics()\n# 튜닝결과 시각화\ntune_result %>%\n  \n  collect_metrics() %>%\n  \n  filter(.metric == \"mn_log_loss\") %>% \n  \n  ggplot(aes(mtry, mean, color = .metric)) +\n  \n  geom_line(size = 1.5) +\n  \n  scale_x_log10() +\n  \n  theme(legend.position = \"none\") +\n  \n  labs(title = \"Mean Log loss\")\ntune_result %>% show_best()\ntune_best <- tune_result %>% select_best(metric = \"mn_log_loss\")\ntune_best$mtry\ntune_best$min_n\n\n\n\n1차시도 (3,5), (3,6), (3,7), (3,8), (3,4) , minimum mean log loss 0.708 5 fold cv 최근 신용카드 발급 기록만 남기기  2차시도 (3,5), (3,7), (3,6), (3,4), (3,8) , minimum mean log loss 0.706 5 fold cv 첫번째 신용카드 발급 기록만 남기기  3차시도 (3,4), (3,5), (3,7), (3,6), (3,8) , minimum mean log loss 0.698 10 fold CV + 첫번째 신용카드 발급 기록만 남기기  4차시도 (4,4), (4,5) , minimum mean log loss 0.404 5 fold CV + 동일 고객이 여러번 발급할 경우 begin_month를 가장 오래된 날짜로 통일  5차시도 (4,4), (4,5) , minimum mean log loss 0.404 5 fold CV + 동일 고객이 여러번 발급할 경우 begin_month를 가장 늦은 날짜로 통일  6차시도 (3,5), (3,6) , minimum mean log loss 0.697 10 fold CV + 변주 제거를 index 랑 flag_mobil 만 함\n튜닝된 모델 학습하기\n\n\ncode\n\nrf_model <- \n  \n  rand_forest(mtry = tune_best$mtry,\n              \n              min_n = tune_best$min_n,\n              \n              trees = 1000) %>% \n  \n  set_engine(\"ranger\", seed = 2022, \n             \n             num.threads = cores, importance = 'impurity') %>% \n  \n  set_mode(\"classification\")\nrf_model2 <- \n  \n  rand_forest(mtry = tune_best$mtry,\n              \n              min_n = tune_best$min_n,\n              \n              trees = 1000) %>% \n  \n  set_engine(\"ranger\", seed = 2023, \n             \n             num.threads = cores, importance = 'impurity') %>% \n  \n  set_mode(\"classification\")\nrf_model3 <- \n  \n  rand_forest(mtry = tune_best$mtry,\n              \n              min_n = tune_best$min_n,\n              \n              trees = 1000) %>% \n  \n  set_engine(\"ranger\", seed = 2024, \n             \n             num.threads = cores, importance = 'impurity') %>% \n  \n  set_mode(\"classification\")\ntictoc::tic()\nrf_fit <- \n  \n  rf_model %>% \n  \n  fit(credit ~ ., data = train2)\ntictoc::toc()\nrf_fit2 <- \n  \n  rf_model2 %>% \n  \n  fit(credit ~ ., data = train2)\nrf_fit3 <- \n  \n  rf_model3 %>% \n  \n  fit(credit ~ ., data = train2)\nrf_fit\nrf_fit2\nrf_fit3\n\n\n\nRanger result\nType: Probability estimation\nNumber of trees: 1000\nSample size: 24806\nNumber of independent variables: 16\nMtry: 3\nTarget node size: 5\nVariable importance mode: impurity\nSplitrule: gini\nOOB prediction error (Brier s.): 0.2288811\nLogitstic model을 돌려보자.\n\n\ncode\n\nlogit_spec <- multinom_reg(penalty = tune(),\n                          mixture = tune()) %>%\n  set_engine(\"glmnet\")%>%\n  set_mode(\"classification\")\n  \nlambda_grid <- grid_regular(penalty(), \n                            mixture(),\n                            levels = list(penalty = 100,\n                                          mixture = 25))\nlambda_grid\nlogit_workflow <-workflow() %>% \n  add_model(logit_spec) %>%\n  add_formula(credit~.)\nlogit_result<- logit_workflow %>%\n  tune_grid(validation_split,\n            grid=lambda_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res) \nlogit_result %>% collect_metrics()\nlogit_result %>% show_best()\n\n\n\n1차시도 Penalty 0.00870 Mixture 0.111, Penalty 0.00775 Mixture 0.111, minimum mean log loss 0.859 10-fold CV 첫번째 신용카드 발급 기록만 남기기  2차시도 Penalty 0.00955 Mixture 0.0833, Penalty 0.00775 Mixture 0.111, minimum mean log loss 0.863 10-fold CV 신용카드 중복 제거 X \n로지스틱 회귀 모형은 중복을 제거하는게 낫고, 랜덤포레스트 모형은 중복값을 삭제 하지 않는게 mean log loss가 더 잘 나옴\n\n\ncode\n\nlogit_tune_best <- logit_result %>% select_best(metric=\"mn_log_loss\")\nlogit_tune_best$penalty\nlogit_model <- multinom_reg(penalty = logit_tune_best$penalty,\n                  mixture = logit_tune_best$mixture) %>% \n                  set_engine(\"glmnet\", seed = 2022, num.threads = cores) %>%            \n                  set_mode(\"classification\")\nlogit_fit <- logit_model %>% fit(credit~., data = train2)\nlogit_fit\nlogit_pred <- predict(logit_fit, test2, type=\"prob\")\nlogit_pred\n\n\n\nXGboost 를 실행해보자\n\n\ncode\n\nxgb_spec <- boost_tree(\n  \n  trees = 1000, \n  \n  tree_depth = tune(), min_n = tune(), \n  \n  loss_reduction = tune(),                     ## first three: model complexity\n  \n  sample_size = tune(), mtry = tune(),         ## randomness\n  \n  learn_rate = tune(),                         ## step size\n  \n) %>% \n  \n  set_engine(\"xgboost\") %>% \n  \n  set_mode(\"classification\")\nxgb_spec\nxgb_grid <- grid_latin_hypercube(\n  \n  tree_depth(),\n  \n  min_n(c(1,10)),\n  \n  loss_reduction(c(-1,0.1)),\n  \n  sample_size = sample_prop(range=c(0.5,1)),\n  \n  learn_rate(c(-1,0)),\n  \n  size = 100\n  \n)\nxgb_grid\nxgb_workflow <- workflow() %>%\n    add_model(xgb_spec) %>% \n    add_formula(credit ~ .)\nxgb_result<- xgb_workflow %>%\n  \n  tune_grid(validation_split,\n            grid=xgb_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res) \nxgb_result %>% collect_metrics()\nxgb_result %>% show_best()\nxgb_tune_best <- xgb_result %>% select_best(metric=\"mn_log_loss\")\n\n\n\n1차 결과 mtry 3 min_n 13 tree_depth 13 learn late 0.126 loss reduction 1.19, sample size 0.983 mean log loss 0.726  2차 결과 mtry 3 min_n 7 tree_depth 10 learn late 0.183 loss reduction 0.921, sample size 0.910 mean log loss 0.733  3차 결과 mtry 3 min_n 4 tree_depth 11 learn late 0.110 loss reduction 1.11, sample size 0.891 mean log loss 0.722 \n\n\ncode\n\nxgb_model <- boost_tree(\n   trees = 1000, \n   tree_depth = xgb_tune_best$tree_depth, \n   min_n = xgb_tune_best$min_n, \n   loss_reduction = xgb_tune_best$loss_reduction,## first three: model complexity\n   sample_size = xgb_tune_best$sample_size, mtry = xgb_tune_best$mtry,         ## randomness\n   learn_rate = xgb_tune_best$learn_rate,                         ## step size\n  ) %>% \n  set_engine(\"xgboost\") %>% \n  set_mode(\"classification\")\nxgb_fit <- \n    xgb_model %>% fit(credit ~ ., data = train2)\nxgb_fit\n\n\n\nlight GBM 를 실행해보자\n\n\ncode\n\ngbm_spec <- boost_tree(\n  \n  trees = 1000, \n  \n  tree_depth = tune(), min_n = tune(), \n  \n  loss_reduction = tune(),                     ## first three: model complexity\n  \n  sample_size = tune(), mtry = tune(),         ## randomness\n  \n  learn_rate = tune(),                         ## step size\n  \n) %>% \n  \n  set_engine(\"lightgbm\") %>% \n  \n  set_mode(\"classification\")\ngbm_spec\ngbm_grid <- grid_latin_hypercube(\n  \n  mtry(c(3,9)),\n  \n  tree_depth(c(11,15)),\n  \n  min_n(c(3,11)),\n  \n  loss_reduction(c(-1,0.1)),\n  \n  sample_size = sample_prop(range=c(0.5,1)),\n  \n  learn_rate(c(-1,0)),\n  \n  size = 100 \n)\ngbm_grid\ngbm_workflow <- workflow() %>%\n  add_model(gbm_spec) %>% \n  add_formula(credit ~ .)\ntic()\ngbm_result<- gbm_workflow %>%\n    tune_grid(validation_split,\n            grid=gbm_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res) \ntoc()\ngbm_result %>% collect_metrics()\ngbm_result %>% show_best()\ngbm_tune_best <- gbm_result %>% select_best(metric=\"mn_log_loss\")\n\n\n\ngbm 1차 결과 mtry 4 min_n 13 tree_depth 15 learn late 0.11 loss reduction 0.141, sample size 0.845 mean log loss 0.725  gbm 2차 결과 mtry 8 min_n 3 tree_depth 14 learn late 0.104 loss reduction 0.327, sample size 0.582 mean log loss 0.718  gbm 3차 결과 mtry 7 min_n 11 tree_depth 14 learn late 0.121 loss reduction 0.126, sample size 0.882 mean log loss 0.717  gbm 4차 결과 mtry 8 min_n 6 tree_depth 15 learn late 0.124 loss reduction 0.242, sample size 0.675 mean log loss 0.715  gbm 5차 결과 mtry 9 min_n 11 tree_depth 15 learn late 0.169 loss reduction 0.148, sample size 0.664 mean log loss 0.716\n\n\ncode\n**파라미터 튜닝을 좀 더 하면 유의미한 결과가 나올거 같다.**\ngbm_model <- boost_tree(\n  trees = 1000, \n  tree_depth = gbm_tune_best$tree_depth, \n  min_n = gbm_tune_best$min_n, \n  loss_reduction = gbm_tune_best$loss_reduction,## first three: model complexity\n  sample_size = gbm_tune_best$sample_size, mtry = gbm_tune_best$mtry,         ## randomness\n  learn_rate = gbm_tune_best$learn_rate,                         ## step size\n) %>% \n  set_engine(\"lightgbm\") %>% \n  set_mode(\"classification\")\ngbm_fit <- \n  gbm_model %>% fit(credit ~ ., data = train2)\ngbm_fit\n\nStacking 을 실행해보자.\n##랜덤포레스트에서 2개 + 로지스틱 모형에서 1개 + xgboost 에서 2개 + lightGBM 에서 2개\n그리드 재설정\n\n\ncode\n\nparam_grid <- tibble(mtry=c(3,3),min_n=c(5,6))\nlambda_grid <- tibble(mixture=c(0.1),penalty=c(0.0087))\nxgb_grid <- tibble(mtry=c(3,3),min_n=c(13,4),tree_depth=c(13,11),learn_rate=c(0.126,0.11),loss_reduction=c(1.19,1.11),sample_size=c(0.983,0.891))\ngbm_grid <- tibble(mtry=c(8,9),min_n=c(6,11),tree_depth=c(15,15),learn_rate=c(0.124,0.169),loss_reduction=c(0.242,0.148),sample_size=c(0.675,0.664))\ntic()\ntune_result <- workflow %>% \n  tune_grid(validation_split,\n            grid = param_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res)\nlogit_result<- logit_workflow %>%\n  tune_grid(validation_split,\n            grid=lambda_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res) \nxgb_result<- xgb_workflow %>%\n  tune_grid(validation_split,\n            grid=xgb_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res) \ngbm_result<- gbm_workflow %>%\n  tune_grid(validation_split,\n            grid=gbm_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res) \ntoc()\ncredit_stacking <- \n  stacks() %>% \n  add_candidates(tune_result) %>% \n  add_candidates(logit_result) %>%\n  add_candidates(xgb_result)  %>%\n  add_candidates(gbm_result)\ncredit_stacking\nas_tibble(credit_stacking)\nstacking_model <- credit_stacking %>% blend_predictions() %>% fit_members() \nstacking_model\nfinal_result <- predict(stacking_model, test2,type = \"prob\")\nfinal_result\n\n\n\n예측하기\n\n\ncode\n\nresult1 <- predict(rf_fit, test2, type = \"prob\")\nresult1 %>% head()\nresult2 <- predict(rf_fit2, test2, type = \"prob\")\nresult2 %>% head()\nresult3 <- predict(rf_fit3, test2, type = \"prob\")\nresult3 %>% head()\nresult4 <- predict(xgb_fit, test2, type = \"prob\")\nresult4 %>% head()\nresult5 <- predict(gbm_fit, test2, type = \"prob\")\nresult5 %>% head()\nresult<-(result1+result2+result3)/3\nresult %>% head()\nsoftresult<- (result3+logit_pred)/2\nsoftresult %>% head()\nsubmission <- read_csv(file.path(file_path, \"sample_submission.csv\"))\nsub_col <- names(submission)\nsubmission <- bind_cols(submission$index, final_result)\nnames(submission) <- sub_col\nwrite.csv(submission, row.names = FALSE,\n          \n          \"rammon33.csv\")\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-31T12:44:47+00:00",
    "input_file": "Dacon-code.utf8.md"
  },
  {
    "path": "posts/4. 경사하강법을 통한 최적의 Parameter 계산/",
    "title": "Gradient Descent",
    "description": "경사하강법을 통해 최적의 모수를 찾아내자.",
    "author": [
      {
        "name": "WooramSeong",
        "url": {}
      }
    ],
    "date": "2021-05-14",
    "categories": [],
    "contents": "\n\nContents\n2. Maximum Likelihood Estimator of Poisson Regression\n\n2. Maximum Likelihood Estimator of Poisson Regression\n\n\n\ncode\n\nmodel<-glm(Claims ~ Gender + Territory,\nfamily = poisson(link=log), data = example_data)\nmodel$coefficients\n\n\n  (Intercept)       GenderM TerritoryWest \n   -0.9808293     0.5108256     0.5108256 \n\n\nDefine Likelihood and Loglikelihood functionLikelihood function : \\(\\prod_{i=1}^n \\frac{e^{-\\lambda} + \\lambda^{y_i}} {y_i!}\\)Loglikelihood function : \\(\\sum_{i=1}^n -\\lambda + y_i ln \\lambda - ln y_i!\\)\nCalculate gradient of Loglikelihood function with repect to \\(\\beta\\)\\(\\lambda=e^{x_i^{T}\\beta}\\)\\(\\displaystyle \\frac{\\partial -l(\\beta)}{\\partial \\beta} = \\sum_{i=1}^n (y_i - e^{x_i^{T}\\beta}) x_i^{T}\\)\n\n\ncode\n\nset.seed(2022)\nbeta <- rnorm(3)\nsigma_f<-function(x){exp(x)}\nnll <- function(beta){\n  y<- example_data$Claims\n  pi_vec <- sigma_f(matrix(cbind(1, example_data$Gender,\n                                    example_data$Territory),ncol = 3) %*% matrix(beta, nrow = 3))\n  -sum((-pi_vec)+y*log(pi_vec)-log(factorial(y)))\n}\n\n\n\n\n\ncode\n\ngrad_nll <- function(beta){\ny<- example_data$Claims\nxbeta <- matrix(cbind(1,example_data$Gender,example_data$Territory),ncol=3) %*% beta\npi_vec <- sigma_f(xbeta)\n-colSums(as.vector(y-pi_vec)*matrix(cbind(1,example_data$Gender,example_data$Territory),ncol=3))\n}\ngrad_nll(beta)\n\n\n[1]  5.601110 -1.786728 -1.061560\n\n\n\ncode\n\nset.seed(2022)\nbeta<-rnorm(3)\niter_n <-1\nimprove <-1\nconv_threshold <- 1e-15\nmax_n <- 100000\nresult <- matrix(0, nrow = max_n, ncol = 3)\nwhile ((improve > conv_threshold) & (iter_n<= max_n)) {\n  beta_new <- beta - 0.001 * grad_nll(beta)\n  improve <- abs(nll(beta)-nll(beta_new))\n  result[iter_n,]<- beta_new\n  beta <- beta_new\n  iter_n <- iter_n +1\n}\nresult[iter_n-1,]\n\n\n[1] -0.9808283  0.5108249  0.5108249\n\ncode\n\nmodel$coefficients\n\n\n  (Intercept)       GenderM TerritoryWest \n   -0.9808293     0.5108256     0.5108256 \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-31T12:00:49+00:00",
    "input_file": {}
  },
  {
    "path": "posts/3. Ridge & Lasso /",
    "title": "Rigde-Lasso as to changing penalty values",
    "description": "Penalty에 따라 Lasso Ridge 계수가 어떻게 바뀌는지 살펴보자.",
    "author": [
      {
        "name": "WooramSeong",
        "url": {}
      }
    ],
    "date": "2021-04-16",
    "categories": [],
    "contents": "\n\nContents\n준비작업\n필요 라이브러리\n파일경로 라이브러리\n\nData overview (데이터 기본정보)\n기본 정보\nRecipe Code\nValidation Set\nTune Spec 설정\n\ncoefficient on Best Penalty in Lasso를 구하자.\n모든 람다(Penalty) 에 따른 계수값을 구해보자.\n데이터 정리\n\n준비작업\n필요 라이브러리\n\n\ncode\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(skimr)\nlibrary(knitr)\nlibrary(naniar)\nlibrary(car)\nlibrary(glmnet)\nlibrary(doParallel)\ntheme_set(theme_bw())\n\n\n\n파일경로 라이브러리\n\n\ncode\n\nfile_path <-  \"/cloud/project/input\"\nfiles <- list.files(file_path)\nfiles\n\n\n[1] \"test.csv\"  \"train.csv\"\n\ncode\n\ntrain <- read_csv(file.path(file_path, \"train.csv\"))\ntest <- read_csv(file.path(file_path, \"test.csv\"))\n\n\n\nData overview (데이터 기본정보)\n기본 정보\nTrain - 총 1460개의 행, 81개의 열로 이루어져있고,\nTest - 목표 변수인 SalePrice를 제외한 80개의 열로 이루어져있다. 행은 1459개이다.\nTrain의 정보를 통해 회귀분석을 실행하고, 해당 회귀분석 식이 Test의 정보에 얼마나 잘 들어맞는지 예측해보자.\nRecipe Code\n\n\ncode\n\nall_data <- bind_rows(train, test) %>% \n  janitor::clean_names()\nnames(all_data)[1:81]\n\n\n [1] \"id\"              \"ms_sub_class\"    \"ms_zoning\"      \n [4] \"lot_frontage\"    \"lot_area\"        \"street\"         \n [7] \"alley\"           \"lot_shape\"       \"land_contour\"   \n[10] \"utilities\"       \"lot_config\"      \"land_slope\"     \n[13] \"neighborhood\"    \"condition1\"      \"condition2\"     \n[16] \"bldg_type\"       \"house_style\"     \"overall_qual\"   \n[19] \"overall_cond\"    \"year_built\"      \"year_remod_add\" \n[22] \"roof_style\"      \"roof_matl\"       \"exterior1st\"    \n[25] \"exterior2nd\"     \"mas_vnr_type\"    \"mas_vnr_area\"   \n[28] \"exter_qual\"      \"exter_cond\"      \"foundation\"     \n[31] \"bsmt_qual\"       \"bsmt_cond\"       \"bsmt_exposure\"  \n[34] \"bsmt_fin_type1\"  \"bsmt_fin_sf1\"    \"bsmt_fin_type2\" \n[37] \"bsmt_fin_sf2\"    \"bsmt_unf_sf\"     \"total_bsmt_sf\"  \n[40] \"heating\"         \"heating_qc\"      \"central_air\"    \n[43] \"electrical\"      \"x1st_flr_sf\"     \"x2nd_flr_sf\"    \n[46] \"low_qual_fin_sf\" \"gr_liv_area\"     \"bsmt_full_bath\" \n[49] \"bsmt_half_bath\"  \"full_bath\"       \"half_bath\"      \n[52] \"bedroom_abv_gr\"  \"kitchen_abv_gr\"  \"kitchen_qual\"   \n[55] \"tot_rms_abv_grd\" \"functional\"      \"fireplaces\"     \n[58] \"fireplace_qu\"    \"garage_type\"     \"garage_yr_blt\"  \n[61] \"garage_finish\"   \"garage_cars\"     \"garage_area\"    \n[64] \"garage_qual\"     \"garage_cond\"     \"paved_drive\"    \n[67] \"wood_deck_sf\"    \"open_porch_sf\"   \"enclosed_porch\" \n[70] \"x3ssn_porch\"     \"screen_porch\"    \"pool_area\"      \n[73] \"pool_qc\"         \"fence\"           \"misc_feature\"   \n[76] \"misc_val\"        \"mo_sold\"         \"yr_sold\"        \n[79] \"sale_type\"       \"sale_condition\"  \"sale_price\"     \n\ncode\n\nhousing_recipe <- all_data %>% \n  recipe(sale_price ~ .) %>%\n  step_rm(id) %>% \n  step_log(sale_price) %>% \n  step_modeimpute(all_nominal()) %>% \n  step_dummy(all_nominal()) %>% \n  step_meanimpute(all_predictors()) %>%\n  step_normalize(all_predictors()) %>% \n  ##step_nzv(all_predictors()) %>% \n  prep(training = all_data)\n\nprint(housing_recipe)\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         80\n\nTraining data contained 2919 data points and 2919 incomplete rows. \n\nOperations:\n\nVariables removed id [trained]\nLog transformation on sale_price [trained]\nMode Imputation for ms_zoning, street, alley, ... [trained]\nDummy variables from ms_zoning, street, alley, ... [trained]\nMean Imputation for ms_sub_class, lot_frontage, ... [trained]\nCentering and scaling for ms_sub_class, lot_frontage, ... [trained]\n\ncode\n\nall_data2 <- juice(housing_recipe)\ntrain_index <- seq_len(nrow(train))\n\ntrain2 <- all_data2[train_index,]\ntest2 <- all_data2[-train_index,]\ntrain2 %>% \n  head() %>% \n  kable()\n\n\nms_sub_class\nlot_frontage\nlot_area\noverall_qual\noverall_cond\nyear_built\nyear_remod_add\nmas_vnr_area\nbsmt_fin_sf1\nbsmt_fin_sf2\nbsmt_unf_sf\ntotal_bsmt_sf\nx1st_flr_sf\nx2nd_flr_sf\nlow_qual_fin_sf\ngr_liv_area\nbsmt_full_bath\nbsmt_half_bath\nfull_bath\nhalf_bath\nbedroom_abv_gr\nkitchen_abv_gr\ntot_rms_abv_grd\nfireplaces\ngarage_yr_blt\ngarage_cars\ngarage_area\nwood_deck_sf\nopen_porch_sf\nenclosed_porch\nx3ssn_porch\nscreen_porch\npool_area\nmisc_val\nmo_sold\nyr_sold\nsale_price\nms_zoning_FV\nms_zoning_RH\nms_zoning_RL\nms_zoning_RM\nstreet_Pave\nalley_Pave\nlot_shape_IR2\nlot_shape_IR3\nlot_shape_Reg\nland_contour_HLS\nland_contour_Low\nland_contour_Lvl\nutilities_NoSeWa\nlot_config_CulDSac\nlot_config_FR2\nlot_config_FR3\nlot_config_Inside\nland_slope_Mod\nland_slope_Sev\nneighborhood_Blueste\nneighborhood_BrDale\nneighborhood_BrkSide\nneighborhood_ClearCr\nneighborhood_CollgCr\nneighborhood_Crawfor\nneighborhood_Edwards\nneighborhood_Gilbert\nneighborhood_IDOTRR\nneighborhood_MeadowV\nneighborhood_Mitchel\nneighborhood_NAmes\nneighborhood_NoRidge\nneighborhood_NPkVill\nneighborhood_NridgHt\nneighborhood_NWAmes\nneighborhood_OldTown\nneighborhood_Sawyer\nneighborhood_SawyerW\nneighborhood_Somerst\nneighborhood_StoneBr\nneighborhood_SWISU\nneighborhood_Timber\nneighborhood_Veenker\ncondition1_Feedr\ncondition1_Norm\ncondition1_PosA\ncondition1_PosN\ncondition1_RRAe\ncondition1_RRAn\ncondition1_RRNe\ncondition1_RRNn\ncondition2_Feedr\ncondition2_Norm\ncondition2_PosA\ncondition2_PosN\ncondition2_RRAe\ncondition2_RRAn\ncondition2_RRNn\nbldg_type_X2fmCon\nbldg_type_Duplex\nbldg_type_Twnhs\nbldg_type_TwnhsE\nhouse_style_X1.5Unf\nhouse_style_X1Story\nhouse_style_X2.5Fin\nhouse_style_X2.5Unf\nhouse_style_X2Story\nhouse_style_SFoyer\nhouse_style_SLvl\nroof_style_Gable\nroof_style_Gambrel\nroof_style_Hip\nroof_style_Mansard\nroof_style_Shed\nroof_matl_CompShg\nroof_matl_Membran\nroof_matl_Metal\nroof_matl_Roll\nroof_matl_Tar.Grv\nroof_matl_WdShake\nroof_matl_WdShngl\nexterior1st_AsphShn\nexterior1st_BrkComm\nexterior1st_BrkFace\nexterior1st_CBlock\nexterior1st_CemntBd\nexterior1st_HdBoard\nexterior1st_ImStucc\nexterior1st_MetalSd\nexterior1st_Plywood\nexterior1st_Stone\nexterior1st_Stucco\nexterior1st_VinylSd\nexterior1st_Wd.Sdng\nexterior1st_WdShing\nexterior2nd_AsphShn\nexterior2nd_Brk.Cmn\nexterior2nd_BrkFace\nexterior2nd_CBlock\nexterior2nd_CmentBd\nexterior2nd_HdBoard\nexterior2nd_ImStucc\nexterior2nd_MetalSd\nexterior2nd_Other\nexterior2nd_Plywood\nexterior2nd_Stone\nexterior2nd_Stucco\nexterior2nd_VinylSd\nexterior2nd_Wd.Sdng\nexterior2nd_Wd.Shng\nmas_vnr_type_BrkFace\nmas_vnr_type_None\nmas_vnr_type_Stone\nexter_qual_Fa\nexter_qual_Gd\nexter_qual_TA\nexter_cond_Fa\nexter_cond_Gd\nexter_cond_Po\nexter_cond_TA\nfoundation_CBlock\nfoundation_PConc\nfoundation_Slab\nfoundation_Stone\nfoundation_Wood\nbsmt_qual_Fa\nbsmt_qual_Gd\nbsmt_qual_TA\nbsmt_cond_Gd\nbsmt_cond_Po\nbsmt_cond_TA\nbsmt_exposure_Gd\nbsmt_exposure_Mn\nbsmt_exposure_No\nbsmt_fin_type1_BLQ\nbsmt_fin_type1_GLQ\nbsmt_fin_type1_LwQ\nbsmt_fin_type1_Rec\nbsmt_fin_type1_Unf\nbsmt_fin_type2_BLQ\nbsmt_fin_type2_GLQ\nbsmt_fin_type2_LwQ\nbsmt_fin_type2_Rec\nbsmt_fin_type2_Unf\nheating_GasA\nheating_GasW\nheating_Grav\nheating_OthW\nheating_Wall\nheating_qc_Fa\nheating_qc_Gd\nheating_qc_Po\nheating_qc_TA\ncentral_air_Y\nelectrical_FuseF\nelectrical_FuseP\nelectrical_Mix\nelectrical_SBrkr\nkitchen_qual_Fa\nkitchen_qual_Gd\nkitchen_qual_TA\nfunctional_Maj2\nfunctional_Min1\nfunctional_Min2\nfunctional_Mod\nfunctional_Sev\nfunctional_Typ\nfireplace_qu_Fa\nfireplace_qu_Gd\nfireplace_qu_Po\nfireplace_qu_TA\ngarage_type_Attchd\ngarage_type_Basment\ngarage_type_BuiltIn\ngarage_type_CarPort\ngarage_type_Detchd\ngarage_finish_RFn\ngarage_finish_Unf\ngarage_qual_Fa\ngarage_qual_Gd\ngarage_qual_Po\ngarage_qual_TA\ngarage_cond_Fa\ngarage_cond_Gd\ngarage_cond_Po\ngarage_cond_TA\npaved_drive_P\npaved_drive_Y\npool_qc_Fa\npool_qc_Gd\nfence_GdWo\nfence_MnPrv\nfence_MnWw\nmisc_feature_Othr\nmisc_feature_Shed\nmisc_feature_TenC\nsale_type_Con\nsale_type_ConLD\nsale_type_ConLI\nsale_type_ConLw\nsale_type_CWD\nsale_type_New\nsale_type_Oth\nsale_type_WD\nsale_condition_AdjLand\nsale_condition_Alloca\nsale_condition_Family\nsale_condition_Normal\nsale_condition_Partial\n0.0673199\n-0.2020329\n-0.2178414\n0.6460727\n-0.5071973\n1.0460784\n0.8966793\n0.5251119\n0.5808073\n-0.2930798\n-0.9347024\n-0.4442517\n-0.7737285\n1.2071717\n-0.1011797\n0.4134764\n1.0868363\n-0.2498524\n0.781232\n1.2323877\n0.169898\n-0.2076629\n0.9866803\n-0.9241529\n1.0007573\n0.3064753\n0.3488399\n-0.7406335\n0.1999717\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n-1.5519176\n0.1576185\n12.24769\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n0.7549859\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n0.606934\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n3.1510604\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n1.5231625\n-1.2373891\n-0.3053301\n-0.1101443\n1.4074569\n-1.2662447\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n1.1096078\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n-0.2985775\n0.6852938\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n0.5905687\n-0.1265135\n-0.5042992\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n-0.8734664\n0.5017845\n-0.0720317\n-0.0631737\n2.1879039\n0.1547375\n-0.3955364\n-0.5721522\n1.1779104\n-0.2930798\n-0.6297885\n0.4770294\n0.2610301\n-0.7848906\n-0.1011797\n-0.4718098\n-0.8195386\n3.8217640\n0.781232\n-0.7561915\n0.169898\n-0.2076629\n-0.2877090\n0.6235248\n-0.0849858\n0.3064753\n-0.0597822\n1.6146027\n-0.7027224\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n-0.4468483\n-0.6028583\n12.10901\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n0.7549859\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n5.773193\n-0.0694091\n-1.647061\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n10.9810599\n4.0979294\n-2.4803837\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n0.9919814\n-0.0524143\n-0.0910347\n-0.6525667\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n2.3419622\n-0.2861546\n-0.0261802\n-0.1222546\n-0.7360782\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n2.3512352\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n-0.7300038\n-0.3932108\n-0.1689125\n-0.6563038\n0.8078764\n-0.3053301\n-0.1101443\n-0.7102579\n0.7894662\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n1.1675169\n-0.9009106\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n3.093995\n-0.2985775\n-1.4587283\n-0.3185509\n-0.6403159\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n-0.8067187\n0.977137\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n-1.6927029\n-0.1265135\n1.9822706\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n0.0673199\n-0.0612694\n0.1371734\n0.6460727\n-0.5071973\n0.9800531\n0.8488195\n0.3347702\n0.0978563\n-0.2930798\n-0.2884670\n-0.2990251\n-0.6106138\n1.2351632\n-0.1011797\n0.5636589\n1.0868363\n-0.2498524\n0.781232\n1.2323877\n0.169898\n-0.2076629\n-0.2877090\n0.6235248\n0.9203319\n0.3064753\n0.6274459\n-0.7406335\n-0.0811953\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n1.0265775\n0.1576185\n12.31717\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n0.606934\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n3.1510604\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n1.5231625\n-1.2373891\n-0.3053301\n-0.1101443\n1.4074569\n-1.2662447\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n1.1096078\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n3.3480662\n-1.4587283\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n-1.6927029\n-0.1265135\n1.9822706\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n0.3025164\n-0.4366387\n-0.0783713\n0.6460727\n-0.5071973\n-1.8590326\n-0.6826955\n-0.5721522\n-0.4948563\n-0.2930798\n-0.0472664\n-0.6711682\n-0.5061185\n0.9785744\n-0.1011797\n0.4273090\n1.0868363\n-0.2498524\n-1.027187\n-0.7561915\n0.169898\n-0.2076629\n0.3494857\n0.6235248\n0.7996938\n1.6196836\n0.7853226\n-0.7406335\n-0.1847831\n3.8743031\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n-1.5519176\n-1.3633351\n11.84940\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n-1.647061\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n5.2278523\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n-0.7360782\n2.4698379\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n-0.7300038\n-0.3932108\n5.9181952\n-0.6563038\n0.8078764\n-0.3053301\n-0.1101443\n-0.7102579\n0.7894662\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n-0.9009106\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n-0.8406993\n1.0675387\n4.7873140\n-0.0414158\n-3.4106271\n-0.323096\n-0.2985775\n0.6852938\n-0.3185509\n-0.6403159\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n2.2707842\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n0.5905687\n-0.1265135\n-0.5042992\n-1.3449209\n-0.1117261\n-0.2608328\n-0.0718576\n1.6571574\n-0.6201557\n1.0493496\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n-2.1550970\n-0.3026411\n0.0673199\n0.6894691\n0.5188142\n1.3553191\n-0.5071973\n0.9470405\n0.7530998\n1.3872480\n0.4688505\n-0.2930798\n-0.1610403\n0.2115370\n-0.0371639\n1.6713642\n-0.1011797\n1.3778060\n1.0868363\n-0.2498524\n0.781232\n1.2323877\n1.385418\n-0.2076629\n1.6238750\n0.6235248\n0.8801192\n1.6196836\n1.6861486\n0.7768341\n0.5403318\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n2.1316468\n0.1576185\n12.42922\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n5.773193\n-0.0694091\n-1.647061\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n6.3323719\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n1.5231625\n-1.2373891\n-0.3053301\n-0.1101443\n1.4074569\n-1.2662447\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n1.1096078\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n-0.2985775\n-1.4587283\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n-1.6927029\n-0.1265135\n1.9822706\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n-0.1678767\n0.7363903\n0.5004295\n-0.7724201\n-0.5071973\n0.7159521\n0.5138006\n-0.5721522\n0.6378834\n-0.2930798\n-1.1303934\n-0.5804016\n-0.9266484\n0.5353755\n-0.1011797\n-0.2742013\n1.0868363\n-0.2498524\n-1.027187\n1.2323877\n-2.261142\n-0.2076629\n-0.9249036\n-0.9241529\n0.5986302\n0.3064753\n0.0330864\n-0.4244944\n-0.2587744\n-0.3595391\n12.6010642\n-0.2858865\n-0.0631394\n1.1441161\n1.3949339\n0.9180953\n11.87060\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n0.606934\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n4.9595195\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n-0.6525667\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n-0.6563038\n0.8078764\n-0.3053301\n-0.1101443\n-0.7102579\n0.7894662\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n-0.9009106\n-0.130642\n-0.0614929\n24.1371155\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n-0.2985775\n0.6852938\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n-0.8067187\n0.977137\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n0.5905687\n-0.1265135\n-0.5042992\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n-0.6201557\n1.0493496\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n\nValidation Set\n\n\ncode\n\nset.seed(2021)\n\nvalidation_set<- validation_split(train2, prop = 0.3)\n#0.3는 분석을 위해 남아 있을 표본의 비율 \n\nvalidation_set$splits[[1]]$in_id\n\n\n  [1]    1    4    5    6    9   14   19   21   22   25   26   30   31\n [14]   32   35   37   38   39   40   45   50   51   56   59   65   72\n [27]   78   82   85   86   94  102  110  111  117  125  126  130  131\n [40]  132  133  135  138  140  141  142  149  153  158  159  161  162\n [53]  180  182  186  189  190  193  197  201  202  204  207  208  212\n [66]  214  216  219  220  221  222  228  231  234  236  238  242  246\n [79]  248  249  256  257  261  265  266  267  268  271  272  273  274\n [92]  277  280  286  288  290  291  292  296  297  298  301  310  312\n[105]  315  318  319  322  332  333  334  338  353  354  355  358  359\n[118]  368  370  375  378  380  381  382  383  384  394  397  398  399\n[131]  411  412  415  418  421  429  433  437  439  440  441  448  452\n[144]  462  467  469  473  474  476  477  479  480  487  490  494  496\n[157]  498  499  500  502  505  507  508  512  514  518  521  525  526\n[170]  530  532  533  534  536  537  542  545  546  548  552  553  554\n[183]  556  557  560  566  567  571  576  579  584  585  586  590  594\n[196]  595  598  604  610  612  617  619  623  629  635  636  643  646\n[209]  647  650  662  675  676  677  685  688  692  693  696  700  711\n[222]  715  717  718  723  725  727  731  735  736  740  741  744  750\n[235]  756  759  760  761  767  774  777  779  780  783  787  794  799\n[248]  802  804  818  820  823  825  829  830  832  834  835  839  840\n[261]  848  849  854  856  857  858  859  860  868  869  871  874  876\n[274]  877  880  885  889  890  892  895  898  901  912  921  923  929\n[287]  930  938  942  948  951  957  958  960  964  965  966  969  975\n[300]  976  987  988  995  996  999 1001 1008 1010 1016 1018 1026 1030\n[313] 1031 1033 1038 1041 1043 1048 1053 1060 1061 1064 1067 1069 1070\n[326] 1072 1073 1077 1080 1081 1086 1092 1096 1098 1099 1100 1102 1104\n[339] 1105 1107 1108 1110 1117 1121 1123 1128 1130 1152 1155 1156 1159\n[352] 1163 1167 1178 1184 1187 1191 1192 1193 1194 1195 1198 1210 1211\n[365] 1217 1220 1225 1231 1232 1234 1241 1243 1245 1247 1249 1254 1268\n[378] 1270 1272 1275 1278 1285 1288 1293 1295 1305 1313 1321 1322 1325\n[391] 1333 1335 1337 1339 1340 1346 1348 1351 1354 1355 1358 1360 1362\n[404] 1365 1368 1371 1373 1374 1377 1382 1383 1388 1389 1394 1398 1400\n[417] 1403 1405 1407 1408 1409 1412 1416 1421 1422 1426 1428 1430 1435\n[430] 1438 1439 1441 1443 1446 1447 1448 1452 1456 1457\n\ncode\n\n#439개로 확인됨 \n\n439/nrow(train)\n\n\n[1] 0.3006849\n\ncode\n\n#0.3 확인\n\n\n\nTune Spec 설정\n\n\ncode\n\ntune_spec <- linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")\n\nlambda_grid <- grid_regular(penalty(), levels = 100)\n\nworkflow <- workflow() %>%\n  add_model(tune_spec) %>% \n  add_formula(sale_price ~ .)\n\ndoParallel::registerDoParallel()\n\ntune_result <- workflow %>% \n  tune_grid(validation_set,\n            grid = lambda_grid,\n            metrics = metric_set(rmse))\n\n\n\ncoefficient on Best Penalty in Lasso를 구하자.\nValidation set 을 설정하여 Root Mean Square (RMSE)가 가장 낮을 때의 람다값을 산출한 후 그에 따른 coefficient 값을 구하였다. Train data 중에서 30% 만 Validation set으로 남겨두었다.\n\nCoefficient 값을 산출하는 자체 함수를 만든 후 for 반복문을 적용시켰다. (람다의 표본은 100 개로 설정하였다)\n\n\ncode\n\ntune_result %>% \n  collect_metrics()\n\n\n# A tibble: 100 x 7\n    penalty .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1 1.00e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 2 1.26e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 3 1.59e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 4 2.01e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 5 2.54e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 6 3.20e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 7 4.04e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 8 5.09e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 9 6.43e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n10 8.11e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n# … with 90 more rows\n\ncode\n\ntune_result %>% show_best()\n\n\n# A tibble: 5 x 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1  0.0192 rmse    standard   0.166     1      NA Preprocessor1_Model0…\n2  0.0242 rmse    standard   0.166     1      NA Preprocessor1_Model0…\n3  0.0152 rmse    standard   0.167     1      NA Preprocessor1_Model0…\n4  0.0305 rmse    standard   0.168     1      NA Preprocessor1_Model0…\n5  0.0120 rmse    standard   0.168     1      NA Preprocessor1_Model0…\n\ncode\n\ntune_best <- tune_result %>% select_best(metric = \"rmse\")\ntune_best$penalty\n\n\n[1] 0.0191791\n\ncode\n\nlasso_model <- \n  linear_reg(penalty = tune_best$penalty, # tuned penalty\n             mixture = 1) %>% # lasso: 1, ridge: 0\n  set_engine(\"glmnet\")\n\nlasso_fit <- \n  lasso_model %>% \n  fit(sale_price ~ ., data = train2)\n\nresult_lasso <- lasso_fit %>% \n  tidy() %>% filter (estimate != 0)%>% select(term,estimate)\n\nresult_lasso\n\n\n# A tibble: 35 x 2\n   term           estimate\n   <chr>             <dbl>\n 1 (Intercept)    12.0    \n 2 ms_sub_class   -0.00193\n 3 lot_area        0.00571\n 4 overall_qual    0.130  \n 5 overall_cond    0.0173 \n 6 year_built      0.0310 \n 7 year_remod_add  0.0281 \n 8 bsmt_fin_sf1    0.00122\n 9 total_bsmt_sf   0.0259 \n10 x1st_flr_sf     0.00375\n# … with 25 more rows\n\ncode\n\nresult_lasso2 <- lasso_fit %>% \n  tidy() %>% select(term,estimate)\n\nresult_lasso2\n\n\n# A tibble: 246 x 2\n   term           estimate\n   <chr>             <dbl>\n 1 (Intercept)    12.0    \n 2 ms_sub_class   -0.00193\n 3 lot_frontage    0      \n 4 lot_area        0.00571\n 5 overall_qual    0.130  \n 6 overall_cond    0.0173 \n 7 year_built      0.0310 \n 8 year_remod_add  0.0281 \n 9 mas_vnr_area    0      \n10 bsmt_fin_sf1    0.00122\n# … with 236 more rows\n\n\n\ncode\n\n## coefficient on Best Penalty in Ridge를 구하자.\n\n\ntune_spec2 <- linear_reg(penalty = tune(), mixture = 0) %>%\n  set_engine(\"glmnet\")\n\nworkflow2 <- workflow() %>%\n  add_model(tune_spec2) %>% \n  add_formula(sale_price ~ .)\n\ndoParallel::registerDoParallel()\n\ntune_result2 <- workflow2 %>% \n  tune_grid(validation_set,\n            grid = lambda_grid,\n            metrics = metric_set(rmse))\n\n\n\n\ntune_result2 %>% \n  collect_metrics()\n\n\n# A tibble: 100 x 7\n    penalty .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1 1.00e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 2 1.26e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 3 1.59e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 4 2.01e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 5 2.54e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 6 3.20e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 7 4.04e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 8 5.09e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 9 6.43e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n10 8.11e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n# … with 90 more rows\n\ncode\n\ntune_result2 %>% show_best()\n\n\n# A tibble: 5 x 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1   0.394 rmse    standard   0.153     1      NA Preprocessor1_Model0…\n2   0.313 rmse    standard   0.153     1      NA Preprocessor1_Model0…\n3   0.248 rmse    standard   0.154     1      NA Preprocessor1_Model0…\n4   0.498 rmse    standard   0.154     1      NA Preprocessor1_Model0…\n5   0.196 rmse    standard   0.155     1      NA Preprocessor1_Model0…\n\ncode\n\ntune_best2 <- tune_result2 %>% select_best(metric = \"rmse\")\ntune_best2$penalty\n\n\n[1] 0.3944206\n\ncode\n\nRidge_model <- \n  linear_reg(penalty = tune_best2$penalty, # tuned penalty\n             mixture = 0) %>% # lasso: 1, ridge: 0\n  set_engine(\"glmnet\")\n\nRidge_fit <- \n  Ridge_model %>% \n  fit(sale_price ~ ., data = train2)\n\nresult_Ridge <- Ridge_fit %>% \n  tidy() %>% filter (estimate != 0)%>% select(term,estimate)\n\nresult_Ridge\n\n\n# A tibble: 246 x 2\n   term           estimate\n   <chr>             <dbl>\n 1 (Intercept)    12.0    \n 2 ms_sub_class   -0.00623\n 3 lot_frontage    0.00590\n 4 lot_area        0.00871\n 5 overall_qual    0.0372 \n 6 overall_cond    0.0173 \n 7 year_built      0.0110 \n 8 year_remod_add  0.0161 \n 9 mas_vnr_area    0.00962\n10 bsmt_fin_sf1    0.0119 \n# … with 236 more rows\n\n모든 람다(Penalty) 에 따른 계수값을 구해보자.\n\n\ncode\n\ncoefficient_lambda<- function(x,y) {\n  Ridge_model <- \n    linear_reg(penalty = x, # tuned penalty\n               mixture = y) %>% # lasso: 1, ridge: 0\n    set_engine(\"glmnet\")\n  \n  Ridge_fit <- \n    Ridge_model %>% \n    fit(sale_price ~ ., data = train2)\n  \n  result_Ridge <- Ridge_fit %>% \n    tidy() %>% select(estimate)\n  \n  result_Ridge\n  \n}\n\n\n\n데이터 정리\n데이터 정리하는 과정이 가장 힘들었는데 관련 코드를 첨부하도록 하겠다. 다음은 데이터를 만들기 위해 내가 정의한 함수이다.\n\n\ncode\n\nlambda_grid$penalty[85]\n\n\n[1] 0.03053856\n\ncode\n\nRidge<-train2[seq_len(nrow(lambda_grid)),]%>%select(-sale_price)\nLasso<-train2[seq_len(nrow(lambda_grid)),]%>%select(-sale_price)\n\nIntercept<-c(0)\nRidge<-cbind(Intercept,Ridge)\nLasso<-cbind(Intercept,Lasso)\n\nRidge<-cbind(lambda_grid,Ridge)\nLasso<-cbind(lambda_grid,Lasso)\n\nRidge_t<-as.data.frame(t(Ridge))\nLasso_t<-as.data.frame(t(Lasso))\n\nfor (i in 1:100) {\n  Ridge_t[-1,i]<-coefficient_lambda(lambda_grid$penalty[i],0)\n  Lasso_t[-1,i]<-coefficient_lambda(lambda_grid$penalty[i],1)\n}\n\nRidge_coeffi<-as.data.frame(t(Ridge_t))\nLasso_coeffi<-as.data.frame(t(Lasso_t))\n\n\nggplot(data=Ridge_coeffi) + \n  geom_line(aes(x=penalty,y=ms_sub_class), color=\"1\") + \n  geom_line(aes(x=penalty,y=lot_frontage), color=\"2\") +\n  geom_line(aes(x=penalty,y=lot_area), color=\"3\") +\n  geom_line(aes(x=penalty,y=overall_qual,), color=\"4\") + \n  geom_line(aes(x=penalty,y=overall_cond), color=\"5\") +\n  geom_line(aes(x=penalty,y=year_built), color=\"6\") +\n  geom_line(aes(x=penalty,y=year_remod_add), color=\"7\")+\n  geom_line(aes(x=penalty,y=mas_vnr_area), color=\"8\")+\n  geom_line(aes(x=penalty,y=bsmt_fin_sf1), color=\"9\")+\n  geom_line(aes(x=penalty,y=bsmt_fin_sf2), color=\"10\")+\n  geom_line(aes(x=penalty,y=bsmt_unf_sf), color=\"11\")+ \n  labs(title=\"Coefficients in Ridge\", x =\"Lambda\", y = \"Coefficients\")\n\n\n\ncode\n\nggplot(data=Lasso_coeffi) + \n  geom_line(aes(x=penalty,y=ms_sub_class), color=\"1\") + \n  geom_line(aes(x=penalty,y=lot_frontage), color=\"2\") +\n  geom_line(aes(x=penalty,y=lot_area), color=\"3\") +\n  geom_line(aes(x=penalty,y=overall_qual,), color=\"4\") + \n  geom_line(aes(x=penalty,y=overall_cond), color=\"5\") +\n  geom_line(aes(x=penalty,y=year_built), color=\"6\") +\n  geom_line(aes(x=penalty,y=year_remod_add), color=\"7\")+\n  geom_line(aes(x=penalty,y=mas_vnr_area), color=\"8\")+\n  geom_line(aes(x=penalty,y=bsmt_fin_sf1), color=\"9\")+\n  geom_line(aes(x=penalty,y=bsmt_fin_sf2), color=\"10\")+\n  geom_line(aes(x=penalty,y=bsmt_unf_sf), color=\"11\")+\n  labs(title=\"Coefficients in Lasso\", x =\"Lambda\", y = \"Coefficients\")\n\n\n\n\n위에서 만든 데이터 테이블을 토대로 그래프를 그려보았다. 아직 변수를 다 담는 방법을 몰라 11개 변수를 선정하여 넣었다.\nLasso 의 경우 Penalty가 증가할수록 선정한 모든 변수들이 나중에는 모두 0으로 수렴하는 것을 알 수 있다.\nLasso 와 달리 Ridge 에서는 완전 0으로 수렴하는 경우가 없는 것을 확인할 수 있다.\n\n\ncode\n\nwrite.csv(Ridge_coeffi, row.names = FALSE,\n          \"Ridge.csv\")\n\nwrite.csv(Lasso_coeffi, row.names = FALSE,\n          \"Lasso.csv\")\n\n\n\n\n\n\n",
    "preview": "posts/3. Ridge & Lasso /Ridge-and-Lasso_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-05-31T11:12:43+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2. 블랙숄즈 모형을 통한 옵션 가격 계산/",
    "title": "Bermuda option from Black Scholes",
    "description": "금융공학::블랙숄즈 모형을 통해 Bermuda Option 가격를 구해보자.",
    "author": [
      {
        "name": "WooramSeong",
        "url": {}
      }
    ],
    "date": "2020-09-18",
    "categories": [],
    "contents": "\n\nContents\nBermuda Option\n\nBermuda Option\nCase <- 100 시나리오 갯수Ini.Stock<-100 초기 주가StrikePrice<-100 옵션 행사 가격year<- 10 년도unit<- 12 단위 (12: 12개월, 365, 365일)interval <- year X unit (구간의 갯수)t <- 1/unit 년도를 구간으로 나눠준 수  r <-0.03sigma<-0.2mu <- r-0.5 X sigma^2Bermuda.Start <- 50Bermuda.End <- 110 버뮤다의 마지막 구간은 interval과 같거나 작아야함.\n\n\ncode\n\nBermuda_option <-function(CallorPut,Case,Ini.Stock,StrikePrice,year,unit,r,sigma,Bermuda.Start,Bermuda.End){\n  interval <- year * unit\n  t <- 1/unit\n  mu <- r-0.5*sigma^2\n  \n  set.seed(5)\n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  Bermuda.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  if (CallorPut == 1){  \n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,Ini.Stock*exp(Yield.table[i,j+1])-StrikePrice)  \n      }}} else if (CallorPut == 2){\n        for (i in 1:Case) {\n          for (j in 1:interval) {\n            \n            Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n            Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n          }}} else { print(\"Call =1 , Put =2 로 설정해주셔야 합니다.\")\n          }\n  \n  \n  \n  if (Bermuda.End>0 & Bermuda.End<=interval){\n    for (i in 1:Case) { \n      Bermuda.table[i,1]<-max(Payoff.table[i,Bermuda.Start:Bermuda.End],Payoff.table[i,interval])\n      \n    }  \n    value<-mean(Bermuda.table[,1])\n  }  else {value<-mean(Payoff.table[,interval])\n  }\n  \n  \n  return(value)\n}\n\n\n\n\n\ncode\n\nBermuda_putop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma,Bermuda.Start,Bermuda.End){\n  interval <- year * unit\n  t <- 1/unit\n  mu <- r-0.5*sigma^2\n  \n  set.seed(5)\n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  Bermuda.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  if (Bermuda.End>0 & Bermuda.End<=interval){\n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n      }\n      Bermuda.table[i,1]<-max(Payoff.table[i,Bermuda.Start:Bermuda.End],Payoff.table[i,interval])} \n    value<-mean(Bermuda.table[,1])\n  } else {\n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n      }}\n    value<-mean(Payoff.table[,interval])\n  }\n  \n  \n  return(value)\n}\n\nBermuda_callop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma,Bermuda.Start,Bermuda.End){\n  interval <- year * unit\n  t <- 1/unit\n  mu <- r-0.5*sigma^2\n  \nset.seed(5)\n\nrnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n\nYield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\nPayoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\nBermuda.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n\nif (Bermuda.End>0 & Bermuda.End<=interval){\n  for (i in 1:Case) {\n    for (j in 1:interval) {\n      \n      Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n      Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,Ini.Stock*exp(Yield.table[i,j+1])-StrikePrice)  \n    }\n    Bermuda.table[i,1]<-max(Payoff.table[i,Bermuda.Start:Bermuda.End],Payoff.table[i,interval])} \n  value<-mean(Bermuda.table[,1])\n} else {\n  for (i in 1:Case) {\n    for (j in 1:interval) {\n      \n      Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n      Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,Ini.Stock*exp(Yield.table[i,j+1])-StrikePrice)  \n    }}\n  value<-mean(Payoff.table[,interval])\n}\n\n\nreturn(value)\n}\n\nBermuda_putop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma,Bermuda.Start,Bermuda.End){\n  interval <- year * unit\n  t <- 1/unit\n  mu <- r-0.5*sigma^2\n  \n  set.seed(5)\n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  Bermuda.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  if (Bermuda.End>0 & Bermuda.End<=interval){\n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n      }\n      Bermuda.table[i,1]<-max(Payoff.table[i,Bermuda.Start:Bermuda.End],Payoff.table[i,interval])} \n    value<-mean(Bermuda.table[,1])\n  } else {\n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n      }}\n    value<-mean(Payoff.table[,interval])\n  }\n  \n  \n  return(value)\n}\n\n\n\n\n\ncode\n\nEuropean_callop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma){\n  \n  \n  interval <- year * unit\n  t <- year/interval\n  mu <- r-0.5*sigma^2\n  set.seed(5)\n  #시뮬레이션 난수 값을 고정시키려면 set.seed(10) <10은 page와 같음> 10페이지에 있는 난수값. \n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  European.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  for (i in 1:Case) {\n    for (j in 1:interval) {\n      \n      Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n      Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,Ini.Stock*exp(Yield.table[i,j+1])-StrikePrice)  \n    }\n    European.table[i,1]<-Payoff.table[i,interval]\n  }\n  \n  value<-mean(European.table[,1])\n  \n  return(value)\n}\n\nEuropean_putop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma){\n  \n  \n  interval <- year * unit\n  t <- year/interval\n  mu <- r-0.5*sigma^2\n  set.seed(5)\n  #시뮬레이션 난수 값을 고정시키려면 set.seed(10) <10은 page와 같음> 10페이지에 있는 난수값. \n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  European.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  for (i in 1:Case) {\n    for (j in 1:interval) {\n      \n      Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n      Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n    }\n    European.table[i,1]<-Payoff.table[i,interval]\n  }\n  \n  value<-mean(European.table[,1])\n  \n  return(value)\n}\n\nBermuda_option(3,100,100,100,5,12,0.01,0.1,0,0)\n\n\n[1] \"Call =1 , Put =2 로 설정해주셔야 합니다.\"\n[1] 0\n\ncode\n\nBermuda_option(2,100,100,100,5,12,0.02,0.1,0,0)\n\n\n[1] 2.920645\n\ncode\n\nBermuda_option(1,100,100,80,5,12,0.03,0.1,0,0)\n\n\n[1] 33.56165\n\ncode\n\nBermuda_callop(100,100,80,5,12,0.02,0.2,20,40)\n\n\n[1] 54.56337\n\ncode\n\nBermuda_putop(100,100,100,5,12,0.01,0.1,0,0)\n\n\n[1] 4.413762\n\ncode\n\nEuropean_callop(100,100,100,5,12,0.01,0.1)\n\n\n[1] 11.41853\n\ncode\n\nEuropean_putop(100,100,100,5,12,0.01,0.1)\n\n\n[1] 4.413762\n\nIf you want to get the plots about bermuda options, Write this down.\n\n\ncode\n\nresult.table.r<-c(1:100)\nresult.table.sigma<-c(1:100)\nresult.table.StrikePrice<-c(1:100)\nresult.table.r2<-c(1:100)\nresult.table.sigma2<-c(1:100)\nresult.table.StrikePrice2<-c(1:100)\nsample.r<- seq(0.01,0.03,length.out = 100)\nsample.sigma<-seq(0.1,0.3,length.out = 100)\nsample.StrikePrice<-seq(80,120,length.out = 100)\n\n\n\nfor (i in 1:100) {\n  result.table.r[i]<-Bermuda_option(1,100,100,100,5,12,sample.r[i],0.2,20,40)\n  result.table.sigma[i]<-Bermuda_option(1,100,100,100,5,12,0.02,sample.sigma[i],20,40)\n  result.table.StrikePrice[i]<-Bermuda_option(1,100,100,sample.StrikePrice[i],5,12,0.02,0.2,20,40)\n  result.table.r2[i]<-Bermuda_option(1,100,100,100,5,12,sample.r[i],0.2,0,0)\n  result.table.sigma2[i]<-Bermuda_option(1,100,100,100,5,12,0.02,sample.sigma[i],0,0)\n  result.table.StrikePrice2[i]<-Bermuda_option(1,100,100,sample.StrikePrice[i],5,12,0.02,0.2,0,0)\n  \n}\n\nplot(sample.r,result.table.r,type = \"l\",col=\"red\",xlab = \"무위험이자율\",ylab=\"콜옵션 가격\",xlim=c(0.009,0.031),ylim=c(10,50))\nlines(sample.r,result.table.r2,type = \"l\")\nlegend(0.026,48,c(\"European\",\"Bermuda\"),col=c(\"black\",\"red\"),lty=1,cex=0.9)\n\nplot(sample.sigma,result.table.sigma,type = \"l\",col=\"red\",xlab = \"표준편차(시그마)\",ylab=\"콜옵션 가격\",xlim=c(0.05,0.40),ylim=c(10,50))\nlines(sample.sigma,result.table.sigma2,type = \"l\")\nlegend(0.32,49,c(\"European\",\"Bermuda\"),col=c(\"black\",\"red\"),lty=1,cex=0.9)\n\nplot(sample.StrikePrice,result.table.StrikePrice,type = \"l\",col=\"red\",xlab = \"행사가격\",ylab=\"콜옵션 가격\",xlim=c(75,125),ylim=c(10,50))\nlines(sample.StrikePrice,result.table.StrikePrice2)\nlegend(114,49,c(\"European\",\"Bermuda\"),col=c(\"black\",\"red\"),lty=1,cex=0.9)\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-31T08:39:06+00:00",
    "input_file": {}
  },
  {
    "path": "posts/1. Monte Carlo 를 이용한 NPV 자동 계산/",
    "title": "NPV Monte carlo",
    "description": "재무관리::몬테카를로 시뮬레이션을 통해 Net Present Value를 구해보자.",
    "author": [
      {
        "name": "WooramSeong",
        "url": {}
      }
    ],
    "date": "2020-03-18",
    "categories": [],
    "contents": "\n\nContents\nlibrary\nNPV Function with Tax (Monte carlo Simulation)\n\n\nlibrary\n\n\ncode\n\nlibrary(dplyr)\n\n\n\nNPV Function with Tax (Monte carlo Simulation)\nInitial Investment = ICASHFLOW before tax = CFGrowth rate = GTax rate = T Cost of Captial = CPeriod = N\n\n\ncode\n\nNPVRAM <-function(I,CF,G,T,C,N) {\n  PVCF = 0 \n  ACF= CF*(1-T)  ##After tax cash flow\n  for(i in 1:N){\n    PVCF[i]<-ACF*(1+G)^(i-1) / (1+C)^i \n  }\n  \n  sum(PVCF)-I}\n\n### Monte Function ###\n\nMonte <- function(PROB,RV) {if(PROB<=0.25){\n  return(RV[1])}\n  else if(PROB<=0.75) {\n    return(RV[2])\n  }\n  else  { \n    return(RV[3])\n  }}      \n\n\n\nNPVRAM(5000000,1000000,0.03,0.25,0.06,10)\n\n\n[1] 1239103\n\ncode\n\nA<-c(5500000,5000000,4500000)\nB<-c(900000,1000000,1100000)\nC<-c(0.02,0.03,0.04)\nD<-c(0.35,0.25,0.15)\nE<-c(0.07,0.06,0.05)\nF<-c(8,10,12)\n\n\nResult<-0\n\n\n### NPVMONTE function ###\n\n\nNPVMONTE <- function(x) {\n  \n  NPV1<-c(1:x)  \n  for(i in 1:x){\n    \n    I1<-Monte(runif(1),A)\n    CF1<-Monte(runif(1),B)\n    G1<-Monte(runif(1),C)\n    T1<-Monte(runif(1),D)\n    C1<-Monte(runif(1),E)\n    N1<-Monte(runif(1),F)\n    NPV1[i] <- NPVRAM(I1,CF1,G1,T1,C1,N1)                   \n  }\n  return(NPV1) \n}\n\nResult1<-NPVMONTE(10000)\nmean(Result1)  ## NPV ???հ?\n\n\n[1] 1238165\n\ncode\n\nvar(Result1)   ## NPV var ??\n\n\n[1] 1.33932e+12\n\ncode\n\nResult1<-as.data.frame(Result1)\ncolnames(Result1) <- \"NPV\"\nResult1<-arrange(Result1,NPV)\nPositive<- filter(Result1,NPV>0)  \nnrow(Positive)/nrow(Result1)     ## NPV ?? ?????? Ȯ?? ##\n\n\n[1] 0.8516\n\ncode\n\nAM<- filter(Result1,NPV>1000000) ## Above  1Million\nBM<- filter(Result1,NPV<1000000) ## Below  1Million\nnrow(AM)/nrow(Result1)   ## NPV 1,000,000 ?ʰ??? Ȯ??\n\n\n[1] 0.5539\n\ncode\n\nnrow(BM)/nrow(Result1)   ## NPV 1,000,000 ?̸??? Ȯ??\n\n\n[1] 0.4461\n\ncode\n\nResult1[100,1]   ## 1  percentile ##\n\n\n[1] -1091259\n\ncode\n\nResult1[1000,1]  ## 10 percentile ##\n\n\n[1] -205871.8\n\ncode\n\nResult1[9000,1]  ## 90 percentile ##\n\n\n[1] 2786013\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-31T08:13:22+00:00",
    "input_file": {}
  }
]
