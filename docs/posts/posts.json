[
  {
    "path": "posts/3. Ridge & Lasso /",
    "title": "Rigde-Lasso as to changing penalty values",
    "description": "Penalty에 따라 Lasso Ridge 계수가 어떻게 바뀌는지 살펴보자.",
    "author": [
      {
        "name": "WooramSeong",
        "url": {}
      }
    ],
    "date": "2021-04-16",
    "categories": [],
    "contents": "\n\nContents\n준비작업\n필요 라이브러리\n파일경로 라이브러리\n\nData overview (데이터 기본정보)\n기본 정보\nRecipe Code\nValidation Set\nTune Spec 설정\n\ncoefficient on Best Penalty in Lasso를 구하자.\n모든 람다(Penalty) 에 따른 계수값을 구해보자.\n데이터 정리\n\n준비작업\n필요 라이브러리\n\n\ncode\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(skimr)\nlibrary(knitr)\nlibrary(naniar)\nlibrary(car)\nlibrary(glmnet)\nlibrary(doParallel)\ntheme_set(theme_bw())\n\n\n\n파일경로 라이브러리\n\n\ncode\n\nfile_path <-  \"/cloud/project/input\"\nfiles <- list.files(file_path)\nfiles\n\n\n[1] \"test.csv\"  \"train.csv\"\n\ncode\n\ntrain <- read_csv(file.path(file_path, \"train.csv\"))\ntest <- read_csv(file.path(file_path, \"test.csv\"))\n\n\n\nData overview (데이터 기본정보)\n기본 정보\nTrain - 총 1460개의 행, 81개의 열로 이루어져있고,\nTest - 목표 변수인 SalePrice를 제외한 80개의 열로 이루어져있다. 행은 1459개이다.\nTrain의 정보를 통해 회귀분석을 실행하고, 해당 회귀분석 식이 Test의 정보에 얼마나 잘 들어맞는지 예측해보자.\nRecipe Code\n\n\ncode\n\nall_data <- bind_rows(train, test) %>% \n  janitor::clean_names()\nnames(all_data)[1:81]\n\n\n [1] \"id\"              \"ms_sub_class\"    \"ms_zoning\"      \n [4] \"lot_frontage\"    \"lot_area\"        \"street\"         \n [7] \"alley\"           \"lot_shape\"       \"land_contour\"   \n[10] \"utilities\"       \"lot_config\"      \"land_slope\"     \n[13] \"neighborhood\"    \"condition1\"      \"condition2\"     \n[16] \"bldg_type\"       \"house_style\"     \"overall_qual\"   \n[19] \"overall_cond\"    \"year_built\"      \"year_remod_add\" \n[22] \"roof_style\"      \"roof_matl\"       \"exterior1st\"    \n[25] \"exterior2nd\"     \"mas_vnr_type\"    \"mas_vnr_area\"   \n[28] \"exter_qual\"      \"exter_cond\"      \"foundation\"     \n[31] \"bsmt_qual\"       \"bsmt_cond\"       \"bsmt_exposure\"  \n[34] \"bsmt_fin_type1\"  \"bsmt_fin_sf1\"    \"bsmt_fin_type2\" \n[37] \"bsmt_fin_sf2\"    \"bsmt_unf_sf\"     \"total_bsmt_sf\"  \n[40] \"heating\"         \"heating_qc\"      \"central_air\"    \n[43] \"electrical\"      \"x1st_flr_sf\"     \"x2nd_flr_sf\"    \n[46] \"low_qual_fin_sf\" \"gr_liv_area\"     \"bsmt_full_bath\" \n[49] \"bsmt_half_bath\"  \"full_bath\"       \"half_bath\"      \n[52] \"bedroom_abv_gr\"  \"kitchen_abv_gr\"  \"kitchen_qual\"   \n[55] \"tot_rms_abv_grd\" \"functional\"      \"fireplaces\"     \n[58] \"fireplace_qu\"    \"garage_type\"     \"garage_yr_blt\"  \n[61] \"garage_finish\"   \"garage_cars\"     \"garage_area\"    \n[64] \"garage_qual\"     \"garage_cond\"     \"paved_drive\"    \n[67] \"wood_deck_sf\"    \"open_porch_sf\"   \"enclosed_porch\" \n[70] \"x3ssn_porch\"     \"screen_porch\"    \"pool_area\"      \n[73] \"pool_qc\"         \"fence\"           \"misc_feature\"   \n[76] \"misc_val\"        \"mo_sold\"         \"yr_sold\"        \n[79] \"sale_type\"       \"sale_condition\"  \"sale_price\"     \n\ncode\n\nhousing_recipe <- all_data %>% \n  recipe(sale_price ~ .) %>%\n  step_rm(id) %>% \n  step_log(sale_price) %>% \n  step_modeimpute(all_nominal()) %>% \n  step_dummy(all_nominal()) %>% \n  step_meanimpute(all_predictors()) %>%\n  step_normalize(all_predictors()) %>% \n  ##step_nzv(all_predictors()) %>% \n  prep(training = all_data)\n\nprint(housing_recipe)\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         80\n\nTraining data contained 2919 data points and 2919 incomplete rows. \n\nOperations:\n\nVariables removed id [trained]\nLog transformation on sale_price [trained]\nMode Imputation for ms_zoning, street, alley, ... [trained]\nDummy variables from ms_zoning, street, alley, ... [trained]\nMean Imputation for ms_sub_class, lot_frontage, ... [trained]\nCentering and scaling for ms_sub_class, lot_frontage, ... [trained]\n\ncode\n\nall_data2 <- juice(housing_recipe)\ntrain_index <- seq_len(nrow(train))\n\ntrain2 <- all_data2[train_index,]\ntest2 <- all_data2[-train_index,]\ntrain2 %>% \n  head() %>% \n  kable()\n\n\nms_sub_class\nlot_frontage\nlot_area\noverall_qual\noverall_cond\nyear_built\nyear_remod_add\nmas_vnr_area\nbsmt_fin_sf1\nbsmt_fin_sf2\nbsmt_unf_sf\ntotal_bsmt_sf\nx1st_flr_sf\nx2nd_flr_sf\nlow_qual_fin_sf\ngr_liv_area\nbsmt_full_bath\nbsmt_half_bath\nfull_bath\nhalf_bath\nbedroom_abv_gr\nkitchen_abv_gr\ntot_rms_abv_grd\nfireplaces\ngarage_yr_blt\ngarage_cars\ngarage_area\nwood_deck_sf\nopen_porch_sf\nenclosed_porch\nx3ssn_porch\nscreen_porch\npool_area\nmisc_val\nmo_sold\nyr_sold\nsale_price\nms_zoning_FV\nms_zoning_RH\nms_zoning_RL\nms_zoning_RM\nstreet_Pave\nalley_Pave\nlot_shape_IR2\nlot_shape_IR3\nlot_shape_Reg\nland_contour_HLS\nland_contour_Low\nland_contour_Lvl\nutilities_NoSeWa\nlot_config_CulDSac\nlot_config_FR2\nlot_config_FR3\nlot_config_Inside\nland_slope_Mod\nland_slope_Sev\nneighborhood_Blueste\nneighborhood_BrDale\nneighborhood_BrkSide\nneighborhood_ClearCr\nneighborhood_CollgCr\nneighborhood_Crawfor\nneighborhood_Edwards\nneighborhood_Gilbert\nneighborhood_IDOTRR\nneighborhood_MeadowV\nneighborhood_Mitchel\nneighborhood_NAmes\nneighborhood_NoRidge\nneighborhood_NPkVill\nneighborhood_NridgHt\nneighborhood_NWAmes\nneighborhood_OldTown\nneighborhood_Sawyer\nneighborhood_SawyerW\nneighborhood_Somerst\nneighborhood_StoneBr\nneighborhood_SWISU\nneighborhood_Timber\nneighborhood_Veenker\ncondition1_Feedr\ncondition1_Norm\ncondition1_PosA\ncondition1_PosN\ncondition1_RRAe\ncondition1_RRAn\ncondition1_RRNe\ncondition1_RRNn\ncondition2_Feedr\ncondition2_Norm\ncondition2_PosA\ncondition2_PosN\ncondition2_RRAe\ncondition2_RRAn\ncondition2_RRNn\nbldg_type_X2fmCon\nbldg_type_Duplex\nbldg_type_Twnhs\nbldg_type_TwnhsE\nhouse_style_X1.5Unf\nhouse_style_X1Story\nhouse_style_X2.5Fin\nhouse_style_X2.5Unf\nhouse_style_X2Story\nhouse_style_SFoyer\nhouse_style_SLvl\nroof_style_Gable\nroof_style_Gambrel\nroof_style_Hip\nroof_style_Mansard\nroof_style_Shed\nroof_matl_CompShg\nroof_matl_Membran\nroof_matl_Metal\nroof_matl_Roll\nroof_matl_Tar.Grv\nroof_matl_WdShake\nroof_matl_WdShngl\nexterior1st_AsphShn\nexterior1st_BrkComm\nexterior1st_BrkFace\nexterior1st_CBlock\nexterior1st_CemntBd\nexterior1st_HdBoard\nexterior1st_ImStucc\nexterior1st_MetalSd\nexterior1st_Plywood\nexterior1st_Stone\nexterior1st_Stucco\nexterior1st_VinylSd\nexterior1st_Wd.Sdng\nexterior1st_WdShing\nexterior2nd_AsphShn\nexterior2nd_Brk.Cmn\nexterior2nd_BrkFace\nexterior2nd_CBlock\nexterior2nd_CmentBd\nexterior2nd_HdBoard\nexterior2nd_ImStucc\nexterior2nd_MetalSd\nexterior2nd_Other\nexterior2nd_Plywood\nexterior2nd_Stone\nexterior2nd_Stucco\nexterior2nd_VinylSd\nexterior2nd_Wd.Sdng\nexterior2nd_Wd.Shng\nmas_vnr_type_BrkFace\nmas_vnr_type_None\nmas_vnr_type_Stone\nexter_qual_Fa\nexter_qual_Gd\nexter_qual_TA\nexter_cond_Fa\nexter_cond_Gd\nexter_cond_Po\nexter_cond_TA\nfoundation_CBlock\nfoundation_PConc\nfoundation_Slab\nfoundation_Stone\nfoundation_Wood\nbsmt_qual_Fa\nbsmt_qual_Gd\nbsmt_qual_TA\nbsmt_cond_Gd\nbsmt_cond_Po\nbsmt_cond_TA\nbsmt_exposure_Gd\nbsmt_exposure_Mn\nbsmt_exposure_No\nbsmt_fin_type1_BLQ\nbsmt_fin_type1_GLQ\nbsmt_fin_type1_LwQ\nbsmt_fin_type1_Rec\nbsmt_fin_type1_Unf\nbsmt_fin_type2_BLQ\nbsmt_fin_type2_GLQ\nbsmt_fin_type2_LwQ\nbsmt_fin_type2_Rec\nbsmt_fin_type2_Unf\nheating_GasA\nheating_GasW\nheating_Grav\nheating_OthW\nheating_Wall\nheating_qc_Fa\nheating_qc_Gd\nheating_qc_Po\nheating_qc_TA\ncentral_air_Y\nelectrical_FuseF\nelectrical_FuseP\nelectrical_Mix\nelectrical_SBrkr\nkitchen_qual_Fa\nkitchen_qual_Gd\nkitchen_qual_TA\nfunctional_Maj2\nfunctional_Min1\nfunctional_Min2\nfunctional_Mod\nfunctional_Sev\nfunctional_Typ\nfireplace_qu_Fa\nfireplace_qu_Gd\nfireplace_qu_Po\nfireplace_qu_TA\ngarage_type_Attchd\ngarage_type_Basment\ngarage_type_BuiltIn\ngarage_type_CarPort\ngarage_type_Detchd\ngarage_finish_RFn\ngarage_finish_Unf\ngarage_qual_Fa\ngarage_qual_Gd\ngarage_qual_Po\ngarage_qual_TA\ngarage_cond_Fa\ngarage_cond_Gd\ngarage_cond_Po\ngarage_cond_TA\npaved_drive_P\npaved_drive_Y\npool_qc_Fa\npool_qc_Gd\nfence_GdWo\nfence_MnPrv\nfence_MnWw\nmisc_feature_Othr\nmisc_feature_Shed\nmisc_feature_TenC\nsale_type_Con\nsale_type_ConLD\nsale_type_ConLI\nsale_type_ConLw\nsale_type_CWD\nsale_type_New\nsale_type_Oth\nsale_type_WD\nsale_condition_AdjLand\nsale_condition_Alloca\nsale_condition_Family\nsale_condition_Normal\nsale_condition_Partial\n0.0673199\n-0.2020329\n-0.2178414\n0.6460727\n-0.5071973\n1.0460784\n0.8966793\n0.5251119\n0.5808073\n-0.2930798\n-0.9347024\n-0.4442517\n-0.7737285\n1.2071717\n-0.1011797\n0.4134764\n1.0868363\n-0.2498524\n0.781232\n1.2323877\n0.169898\n-0.2076629\n0.9866803\n-0.9241529\n1.0007573\n0.3064753\n0.3488399\n-0.7406335\n0.1999717\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n-1.5519176\n0.1576185\n12.24769\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n0.7549859\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n0.606934\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n3.1510604\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n1.5231625\n-1.2373891\n-0.3053301\n-0.1101443\n1.4074569\n-1.2662447\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n1.1096078\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n-0.2985775\n0.6852938\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n0.5905687\n-0.1265135\n-0.5042992\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n-0.8734664\n0.5017845\n-0.0720317\n-0.0631737\n2.1879039\n0.1547375\n-0.3955364\n-0.5721522\n1.1779104\n-0.2930798\n-0.6297885\n0.4770294\n0.2610301\n-0.7848906\n-0.1011797\n-0.4718098\n-0.8195386\n3.8217640\n0.781232\n-0.7561915\n0.169898\n-0.2076629\n-0.2877090\n0.6235248\n-0.0849858\n0.3064753\n-0.0597822\n1.6146027\n-0.7027224\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n-0.4468483\n-0.6028583\n12.10901\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n0.7549859\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n5.773193\n-0.0694091\n-1.647061\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n10.9810599\n4.0979294\n-2.4803837\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n0.9919814\n-0.0524143\n-0.0910347\n-0.6525667\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n2.3419622\n-0.2861546\n-0.0261802\n-0.1222546\n-0.7360782\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n2.3512352\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n-0.7300038\n-0.3932108\n-0.1689125\n-0.6563038\n0.8078764\n-0.3053301\n-0.1101443\n-0.7102579\n0.7894662\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n1.1675169\n-0.9009106\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n3.093995\n-0.2985775\n-1.4587283\n-0.3185509\n-0.6403159\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n-0.8067187\n0.977137\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n-1.6927029\n-0.1265135\n1.9822706\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n0.0673199\n-0.0612694\n0.1371734\n0.6460727\n-0.5071973\n0.9800531\n0.8488195\n0.3347702\n0.0978563\n-0.2930798\n-0.2884670\n-0.2990251\n-0.6106138\n1.2351632\n-0.1011797\n0.5636589\n1.0868363\n-0.2498524\n0.781232\n1.2323877\n0.169898\n-0.2076629\n-0.2877090\n0.6235248\n0.9203319\n0.3064753\n0.6274459\n-0.7406335\n-0.0811953\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n1.0265775\n0.1576185\n12.31717\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n0.606934\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n3.1510604\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n1.5231625\n-1.2373891\n-0.3053301\n-0.1101443\n1.4074569\n-1.2662447\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n1.1096078\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n3.3480662\n-1.4587283\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n-1.6927029\n-0.1265135\n1.9822706\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n0.3025164\n-0.4366387\n-0.0783713\n0.6460727\n-0.5071973\n-1.8590326\n-0.6826955\n-0.5721522\n-0.4948563\n-0.2930798\n-0.0472664\n-0.6711682\n-0.5061185\n0.9785744\n-0.1011797\n0.4273090\n1.0868363\n-0.2498524\n-1.027187\n-0.7561915\n0.169898\n-0.2076629\n0.3494857\n0.6235248\n0.7996938\n1.6196836\n0.7853226\n-0.7406335\n-0.1847831\n3.8743031\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n-1.5519176\n-1.3633351\n11.84940\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n-1.647061\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n5.2278523\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n-0.7360782\n2.4698379\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n-0.7300038\n-0.3932108\n5.9181952\n-0.6563038\n0.8078764\n-0.3053301\n-0.1101443\n-0.7102579\n0.7894662\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n-0.9009106\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n-0.8406993\n1.0675387\n4.7873140\n-0.0414158\n-3.4106271\n-0.323096\n-0.2985775\n0.6852938\n-0.3185509\n-0.6403159\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n2.2707842\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n0.5905687\n-0.1265135\n-0.5042992\n-1.3449209\n-0.1117261\n-0.2608328\n-0.0718576\n1.6571574\n-0.6201557\n1.0493496\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n-2.1550970\n-0.3026411\n0.0673199\n0.6894691\n0.5188142\n1.3553191\n-0.5071973\n0.9470405\n0.7530998\n1.3872480\n0.4688505\n-0.2930798\n-0.1610403\n0.2115370\n-0.0371639\n1.6713642\n-0.1011797\n1.3778060\n1.0868363\n-0.2498524\n0.781232\n1.2323877\n1.385418\n-0.2076629\n1.6238750\n0.6235248\n0.8801192\n1.6196836\n1.6861486\n0.7768341\n0.5403318\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n2.1316468\n0.1576185\n12.42922\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n5.773193\n-0.0694091\n-1.647061\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n6.3323719\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n1.5231625\n-1.2373891\n-0.3053301\n-0.1101443\n1.4074569\n-1.2662447\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n1.1096078\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n-0.2985775\n-1.4587283\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n-1.6927029\n-0.1265135\n1.9822706\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n-0.1678767\n0.7363903\n0.5004295\n-0.7724201\n-0.5071973\n0.7159521\n0.5138006\n-0.5721522\n0.6378834\n-0.2930798\n-1.1303934\n-0.5804016\n-0.9266484\n0.5353755\n-0.1011797\n-0.2742013\n1.0868363\n-0.2498524\n-1.027187\n1.2323877\n-2.261142\n-0.2076629\n-0.9249036\n-0.9241529\n0.5986302\n0.3064753\n0.0330864\n-0.4244944\n-0.2587744\n-0.3595391\n12.6010642\n-0.2858865\n-0.0631394\n1.1441161\n1.3949339\n0.9180953\n11.87060\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n0.606934\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n4.9595195\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n-0.6525667\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n-0.6563038\n0.8078764\n-0.3053301\n-0.1101443\n-0.7102579\n0.7894662\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n-0.9009106\n-0.130642\n-0.0614929\n24.1371155\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n-0.2985775\n0.6852938\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n-0.8067187\n0.977137\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n0.5905687\n-0.1265135\n-0.5042992\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n-0.6201557\n1.0493496\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n\nValidation Set\n\n\ncode\n\nset.seed(2021)\n\nvalidation_set<- validation_split(train2, prop = 0.3)\n#0.3는 분석을 위해 남아 있을 표본의 비율 \n\nvalidation_set$splits[[1]]$in_id\n\n\n  [1]    1    4    5    6    9   14   19   21   22   25   26   30   31\n [14]   32   35   37   38   39   40   45   50   51   56   59   65   72\n [27]   78   82   85   86   94  102  110  111  117  125  126  130  131\n [40]  132  133  135  138  140  141  142  149  153  158  159  161  162\n [53]  180  182  186  189  190  193  197  201  202  204  207  208  212\n [66]  214  216  219  220  221  222  228  231  234  236  238  242  246\n [79]  248  249  256  257  261  265  266  267  268  271  272  273  274\n [92]  277  280  286  288  290  291  292  296  297  298  301  310  312\n[105]  315  318  319  322  332  333  334  338  353  354  355  358  359\n[118]  368  370  375  378  380  381  382  383  384  394  397  398  399\n[131]  411  412  415  418  421  429  433  437  439  440  441  448  452\n[144]  462  467  469  473  474  476  477  479  480  487  490  494  496\n[157]  498  499  500  502  505  507  508  512  514  518  521  525  526\n[170]  530  532  533  534  536  537  542  545  546  548  552  553  554\n[183]  556  557  560  566  567  571  576  579  584  585  586  590  594\n[196]  595  598  604  610  612  617  619  623  629  635  636  643  646\n[209]  647  650  662  675  676  677  685  688  692  693  696  700  711\n[222]  715  717  718  723  725  727  731  735  736  740  741  744  750\n[235]  756  759  760  761  767  774  777  779  780  783  787  794  799\n[248]  802  804  818  820  823  825  829  830  832  834  835  839  840\n[261]  848  849  854  856  857  858  859  860  868  869  871  874  876\n[274]  877  880  885  889  890  892  895  898  901  912  921  923  929\n[287]  930  938  942  948  951  957  958  960  964  965  966  969  975\n[300]  976  987  988  995  996  999 1001 1008 1010 1016 1018 1026 1030\n[313] 1031 1033 1038 1041 1043 1048 1053 1060 1061 1064 1067 1069 1070\n[326] 1072 1073 1077 1080 1081 1086 1092 1096 1098 1099 1100 1102 1104\n[339] 1105 1107 1108 1110 1117 1121 1123 1128 1130 1152 1155 1156 1159\n[352] 1163 1167 1178 1184 1187 1191 1192 1193 1194 1195 1198 1210 1211\n[365] 1217 1220 1225 1231 1232 1234 1241 1243 1245 1247 1249 1254 1268\n[378] 1270 1272 1275 1278 1285 1288 1293 1295 1305 1313 1321 1322 1325\n[391] 1333 1335 1337 1339 1340 1346 1348 1351 1354 1355 1358 1360 1362\n[404] 1365 1368 1371 1373 1374 1377 1382 1383 1388 1389 1394 1398 1400\n[417] 1403 1405 1407 1408 1409 1412 1416 1421 1422 1426 1428 1430 1435\n[430] 1438 1439 1441 1443 1446 1447 1448 1452 1456 1457\n\ncode\n\n#439개로 확인됨 \n\n439/nrow(train)\n\n\n[1] 0.3006849\n\ncode\n\n#0.3 확인\n\n\n\nTune Spec 설정\n\n\ncode\n\ntune_spec <- linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")\n\nlambda_grid <- grid_regular(penalty(), levels = 100)\n\nworkflow <- workflow() %>%\n  add_model(tune_spec) %>% \n  add_formula(sale_price ~ .)\n\ndoParallel::registerDoParallel()\n\ntune_result <- workflow %>% \n  tune_grid(validation_set,\n            grid = lambda_grid,\n            metrics = metric_set(rmse))\n\n\n\ncoefficient on Best Penalty in Lasso를 구하자.\nValidation set 을 설정하여 Root Mean Square (RMSE)가 가장 낮을 때의 람다값을 산출한 후 그에 따른 coefficient 값을 구하였다. Train data 중에서 30% 만 Validation set으로 남겨두었다.\n\nCoefficient 값을 산출하는 자체 함수를 만든 후 for 반복문을 적용시켰다. (람다의 표본은 100 개로 설정하였다)\n\n\ncode\n\ntune_result %>% \n  collect_metrics()\n\n\n# A tibble: 100 x 7\n    penalty .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1 1.00e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 2 1.26e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 3 1.59e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 4 2.01e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 5 2.54e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 6 3.20e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 7 4.04e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 8 5.09e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n 9 6.43e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n10 8.11e-10 rmse    standard   0.201     1      NA Preprocessor1_Mode…\n# … with 90 more rows\n\ncode\n\ntune_result %>% show_best()\n\n\n# A tibble: 5 x 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1  0.0192 rmse    standard   0.166     1      NA Preprocessor1_Model0…\n2  0.0242 rmse    standard   0.166     1      NA Preprocessor1_Model0…\n3  0.0152 rmse    standard   0.167     1      NA Preprocessor1_Model0…\n4  0.0305 rmse    standard   0.168     1      NA Preprocessor1_Model0…\n5  0.0120 rmse    standard   0.168     1      NA Preprocessor1_Model0…\n\ncode\n\ntune_best <- tune_result %>% select_best(metric = \"rmse\")\ntune_best$penalty\n\n\n[1] 0.0191791\n\ncode\n\nlasso_model <- \n  linear_reg(penalty = tune_best$penalty, # tuned penalty\n             mixture = 1) %>% # lasso: 1, ridge: 0\n  set_engine(\"glmnet\")\n\nlasso_fit <- \n  lasso_model %>% \n  fit(sale_price ~ ., data = train2)\n\nresult_lasso <- lasso_fit %>% \n  tidy() %>% filter (estimate != 0)%>% select(term,estimate)\n\nresult_lasso\n\n\n# A tibble: 35 x 2\n   term           estimate\n   <chr>             <dbl>\n 1 (Intercept)    12.0    \n 2 ms_sub_class   -0.00193\n 3 lot_area        0.00571\n 4 overall_qual    0.130  \n 5 overall_cond    0.0173 \n 6 year_built      0.0310 \n 7 year_remod_add  0.0281 \n 8 bsmt_fin_sf1    0.00122\n 9 total_bsmt_sf   0.0259 \n10 x1st_flr_sf     0.00375\n# … with 25 more rows\n\ncode\n\nresult_lasso2 <- lasso_fit %>% \n  tidy() %>% select(term,estimate)\n\nresult_lasso2\n\n\n# A tibble: 246 x 2\n   term           estimate\n   <chr>             <dbl>\n 1 (Intercept)    12.0    \n 2 ms_sub_class   -0.00193\n 3 lot_frontage    0      \n 4 lot_area        0.00571\n 5 overall_qual    0.130  \n 6 overall_cond    0.0173 \n 7 year_built      0.0310 \n 8 year_remod_add  0.0281 \n 9 mas_vnr_area    0      \n10 bsmt_fin_sf1    0.00122\n# … with 236 more rows\n\n\n\ncode\n\n## coefficient on Best Penalty in Ridge를 구하자.\n\n\ntune_spec2 <- linear_reg(penalty = tune(), mixture = 0) %>%\n  set_engine(\"glmnet\")\n\nworkflow2 <- workflow() %>%\n  add_model(tune_spec2) %>% \n  add_formula(sale_price ~ .)\n\ndoParallel::registerDoParallel()\n\ntune_result2 <- workflow2 %>% \n  tune_grid(validation_set,\n            grid = lambda_grid,\n            metrics = metric_set(rmse))\n\n\n\n\ntune_result2 %>% \n  collect_metrics()\n\n\n# A tibble: 100 x 7\n    penalty .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1 1.00e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 2 1.26e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 3 1.59e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 4 2.01e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 5 2.54e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 6 3.20e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 7 4.04e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 8 5.09e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n 9 6.43e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n10 8.11e-10 rmse    standard   0.169     1      NA Preprocessor1_Mode…\n# … with 90 more rows\n\ncode\n\ntune_result2 %>% show_best()\n\n\n# A tibble: 5 x 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1   0.394 rmse    standard   0.153     1      NA Preprocessor1_Model0…\n2   0.313 rmse    standard   0.153     1      NA Preprocessor1_Model0…\n3   0.248 rmse    standard   0.154     1      NA Preprocessor1_Model0…\n4   0.498 rmse    standard   0.154     1      NA Preprocessor1_Model0…\n5   0.196 rmse    standard   0.155     1      NA Preprocessor1_Model0…\n\ncode\n\ntune_best2 <- tune_result2 %>% select_best(metric = \"rmse\")\ntune_best2$penalty\n\n\n[1] 0.3944206\n\ncode\n\nRidge_model <- \n  linear_reg(penalty = tune_best2$penalty, # tuned penalty\n             mixture = 0) %>% # lasso: 1, ridge: 0\n  set_engine(\"glmnet\")\n\nRidge_fit <- \n  Ridge_model %>% \n  fit(sale_price ~ ., data = train2)\n\nresult_Ridge <- Ridge_fit %>% \n  tidy() %>% filter (estimate != 0)%>% select(term,estimate)\n\nresult_Ridge\n\n\n# A tibble: 246 x 2\n   term           estimate\n   <chr>             <dbl>\n 1 (Intercept)    12.0    \n 2 ms_sub_class   -0.00623\n 3 lot_frontage    0.00590\n 4 lot_area        0.00871\n 5 overall_qual    0.0372 \n 6 overall_cond    0.0173 \n 7 year_built      0.0110 \n 8 year_remod_add  0.0161 \n 9 mas_vnr_area    0.00962\n10 bsmt_fin_sf1    0.0119 \n# … with 236 more rows\n\n모든 람다(Penalty) 에 따른 계수값을 구해보자.\n\n\ncode\n\ncoefficient_lambda<- function(x,y) {\n  Ridge_model <- \n    linear_reg(penalty = x, # tuned penalty\n               mixture = y) %>% # lasso: 1, ridge: 0\n    set_engine(\"glmnet\")\n  \n  Ridge_fit <- \n    Ridge_model %>% \n    fit(sale_price ~ ., data = train2)\n  \n  result_Ridge <- Ridge_fit %>% \n    tidy() %>% select(estimate)\n  \n  result_Ridge\n  \n}\n\n\n\n데이터 정리\n데이터 정리하는 과정이 가장 힘들었는데 관련 코드를 첨부하도록 하겠다. 다음은 데이터를 만들기 위해 내가 정의한 함수이다.\n\n\ncode\n\nlambda_grid$penalty[85]\n\n\n[1] 0.03053856\n\ncode\n\nRidge<-train2[seq_len(nrow(lambda_grid)),]%>%select(-sale_price)\nLasso<-train2[seq_len(nrow(lambda_grid)),]%>%select(-sale_price)\n\nIntercept<-c(0)\nRidge<-cbind(Intercept,Ridge)\nLasso<-cbind(Intercept,Lasso)\n\nRidge<-cbind(lambda_grid,Ridge)\nLasso<-cbind(lambda_grid,Lasso)\n\nRidge_t<-as.data.frame(t(Ridge))\nLasso_t<-as.data.frame(t(Lasso))\n\nfor (i in 1:100) {\n  Ridge_t[-1,i]<-coefficient_lambda(lambda_grid$penalty[i],0)\n  Lasso_t[-1,i]<-coefficient_lambda(lambda_grid$penalty[i],1)\n}\n\nRidge_coeffi<-as.data.frame(t(Ridge_t))\nLasso_coeffi<-as.data.frame(t(Lasso_t))\n\n\nggplot(data=Ridge_coeffi) + \n  geom_line(aes(x=penalty,y=ms_sub_class), color=\"1\") + \n  geom_line(aes(x=penalty,y=lot_frontage), color=\"2\") +\n  geom_line(aes(x=penalty,y=lot_area), color=\"3\") +\n  geom_line(aes(x=penalty,y=overall_qual,), color=\"4\") + \n  geom_line(aes(x=penalty,y=overall_cond), color=\"5\") +\n  geom_line(aes(x=penalty,y=year_built), color=\"6\") +\n  geom_line(aes(x=penalty,y=year_remod_add), color=\"7\")+\n  geom_line(aes(x=penalty,y=mas_vnr_area), color=\"8\")+\n  geom_line(aes(x=penalty,y=bsmt_fin_sf1), color=\"9\")+\n  geom_line(aes(x=penalty,y=bsmt_fin_sf2), color=\"10\")+\n  geom_line(aes(x=penalty,y=bsmt_unf_sf), color=\"11\")+ \n  labs(title=\"Coefficients in Ridge\", x =\"Lambda\", y = \"Coefficients\")\n\n\n\ncode\n\nggplot(data=Lasso_coeffi) + \n  geom_line(aes(x=penalty,y=ms_sub_class), color=\"1\") + \n  geom_line(aes(x=penalty,y=lot_frontage), color=\"2\") +\n  geom_line(aes(x=penalty,y=lot_area), color=\"3\") +\n  geom_line(aes(x=penalty,y=overall_qual,), color=\"4\") + \n  geom_line(aes(x=penalty,y=overall_cond), color=\"5\") +\n  geom_line(aes(x=penalty,y=year_built), color=\"6\") +\n  geom_line(aes(x=penalty,y=year_remod_add), color=\"7\")+\n  geom_line(aes(x=penalty,y=mas_vnr_area), color=\"8\")+\n  geom_line(aes(x=penalty,y=bsmt_fin_sf1), color=\"9\")+\n  geom_line(aes(x=penalty,y=bsmt_fin_sf2), color=\"10\")+\n  geom_line(aes(x=penalty,y=bsmt_unf_sf), color=\"11\")+\n  labs(title=\"Coefficients in Lasso\", x =\"Lambda\", y = \"Coefficients\")\n\n\n\n\n위에서 만든 데이터 테이블을 토대로 그래프를 그려보았다. 아직 변수를 다 담는 방법을 몰라 11개 변수를 선정하여 넣었다.\nLasso 의 경우 Penalty가 증가할수록 선정한 모든 변수들이 나중에는 모두 0으로 수렴하는 것을 알 수 있다.\nLasso 와 달리 Ridge 에서는 완전 0으로 수렴하는 경우가 없는 것을 확인할 수 있다.\n\n\ncode\n\nwrite.csv(Ridge_coeffi, row.names = FALSE,\n          \"Ridge.csv\")\n\nwrite.csv(Lasso_coeffi, row.names = FALSE,\n          \"Lasso.csv\")\n\n\n\n\n\n\n",
    "preview": "posts/3. Ridge & Lasso /Ridge-and-Lasso_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-05-31T11:12:43+00:00",
    "input_file": "Ridge-and-Lasso.utf8.md"
  },
  {
    "path": "posts/2. 블랙숄즈 모형을 통한 옵션 가격 계산/",
    "title": "Bermuda option from Black Scholes",
    "description": "금융공학::블랙숄즈 모형을 통해 Bermuda Option 가격를 구해보자.",
    "author": [
      {
        "name": "WooramSeong",
        "url": {}
      }
    ],
    "date": "2020-09-18",
    "categories": [],
    "contents": "\n\nContents\nBermuda Option\n\nBermuda Option\nCase <- 100 시나리오 갯수Ini.Stock<-100 초기 주가StrikePrice<-100 옵션 행사 가격year<- 10 년도unit<- 12 단위 (12: 12개월, 365, 365일)interval <- year X unit (구간의 갯수)t <- 1/unit 년도를 구간으로 나눠준 수  r <-0.03sigma<-0.2mu <- r-0.5 X sigma^2Bermuda.Start <- 50Bermuda.End <- 110 버뮤다의 마지막 구간은 interval과 같거나 작아야함.\n\n\ncode\n\nBermuda_option <-function(CallorPut,Case,Ini.Stock,StrikePrice,year,unit,r,sigma,Bermuda.Start,Bermuda.End){\n  interval <- year * unit\n  t <- 1/unit\n  mu <- r-0.5*sigma^2\n  \n  set.seed(5)\n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  Bermuda.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  if (CallorPut == 1){  \n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,Ini.Stock*exp(Yield.table[i,j+1])-StrikePrice)  \n      }}} else if (CallorPut == 2){\n        for (i in 1:Case) {\n          for (j in 1:interval) {\n            \n            Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n            Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n          }}} else { print(\"Call =1 , Put =2 로 설정해주셔야 합니다.\")\n          }\n  \n  \n  \n  if (Bermuda.End>0 & Bermuda.End<=interval){\n    for (i in 1:Case) { \n      Bermuda.table[i,1]<-max(Payoff.table[i,Bermuda.Start:Bermuda.End],Payoff.table[i,interval])\n      \n    }  \n    value<-mean(Bermuda.table[,1])\n  }  else {value<-mean(Payoff.table[,interval])\n  }\n  \n  \n  return(value)\n}\n\n\n\n\n\ncode\n\nBermuda_putop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma,Bermuda.Start,Bermuda.End){\n  interval <- year * unit\n  t <- 1/unit\n  mu <- r-0.5*sigma^2\n  \n  set.seed(5)\n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  Bermuda.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  if (Bermuda.End>0 & Bermuda.End<=interval){\n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n      }\n      Bermuda.table[i,1]<-max(Payoff.table[i,Bermuda.Start:Bermuda.End],Payoff.table[i,interval])} \n    value<-mean(Bermuda.table[,1])\n  } else {\n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n      }}\n    value<-mean(Payoff.table[,interval])\n  }\n  \n  \n  return(value)\n}\n\nBermuda_callop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma,Bermuda.Start,Bermuda.End){\n  interval <- year * unit\n  t <- 1/unit\n  mu <- r-0.5*sigma^2\n  \nset.seed(5)\n\nrnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n\nYield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\nPayoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\nBermuda.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n\nif (Bermuda.End>0 & Bermuda.End<=interval){\n  for (i in 1:Case) {\n    for (j in 1:interval) {\n      \n      Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n      Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,Ini.Stock*exp(Yield.table[i,j+1])-StrikePrice)  \n    }\n    Bermuda.table[i,1]<-max(Payoff.table[i,Bermuda.Start:Bermuda.End],Payoff.table[i,interval])} \n  value<-mean(Bermuda.table[,1])\n} else {\n  for (i in 1:Case) {\n    for (j in 1:interval) {\n      \n      Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n      Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,Ini.Stock*exp(Yield.table[i,j+1])-StrikePrice)  \n    }}\n  value<-mean(Payoff.table[,interval])\n}\n\n\nreturn(value)\n}\n\nBermuda_putop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma,Bermuda.Start,Bermuda.End){\n  interval <- year * unit\n  t <- 1/unit\n  mu <- r-0.5*sigma^2\n  \n  set.seed(5)\n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  Bermuda.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  if (Bermuda.End>0 & Bermuda.End<=interval){\n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n      }\n      Bermuda.table[i,1]<-max(Payoff.table[i,Bermuda.Start:Bermuda.End],Payoff.table[i,interval])} \n    value<-mean(Bermuda.table[,1])\n  } else {\n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n      }}\n    value<-mean(Payoff.table[,interval])\n  }\n  \n  \n  return(value)\n}\n\n\n\n\n\ncode\n\nEuropean_callop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma){\n  \n  \n  interval <- year * unit\n  t <- year/interval\n  mu <- r-0.5*sigma^2\n  set.seed(5)\n  #시뮬레이션 난수 값을 고정시키려면 set.seed(10) <10은 page와 같음> 10페이지에 있는 난수값. \n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  European.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  for (i in 1:Case) {\n    for (j in 1:interval) {\n      \n      Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n      Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,Ini.Stock*exp(Yield.table[i,j+1])-StrikePrice)  \n    }\n    European.table[i,1]<-Payoff.table[i,interval]\n  }\n  \n  value<-mean(European.table[,1])\n  \n  return(value)\n}\n\nEuropean_putop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma){\n  \n  \n  interval <- year * unit\n  t <- year/interval\n  mu <- r-0.5*sigma^2\n  set.seed(5)\n  #시뮬레이션 난수 값을 고정시키려면 set.seed(10) <10은 page와 같음> 10페이지에 있는 난수값. \n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  European.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  for (i in 1:Case) {\n    for (j in 1:interval) {\n      \n      Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n      Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n    }\n    European.table[i,1]<-Payoff.table[i,interval]\n  }\n  \n  value<-mean(European.table[,1])\n  \n  return(value)\n}\n\nBermuda_option(3,100,100,100,5,12,0.01,0.1,0,0)\n\n\n[1] \"Call =1 , Put =2 로 설정해주셔야 합니다.\"\n[1] 0\n\ncode\n\nBermuda_option(2,100,100,100,5,12,0.02,0.1,0,0)\n\n\n[1] 2.920645\n\ncode\n\nBermuda_option(1,100,100,80,5,12,0.03,0.1,0,0)\n\n\n[1] 33.56165\n\ncode\n\nBermuda_callop(100,100,80,5,12,0.02,0.2,20,40)\n\n\n[1] 54.56337\n\ncode\n\nBermuda_putop(100,100,100,5,12,0.01,0.1,0,0)\n\n\n[1] 4.413762\n\ncode\n\nEuropean_callop(100,100,100,5,12,0.01,0.1)\n\n\n[1] 11.41853\n\ncode\n\nEuropean_putop(100,100,100,5,12,0.01,0.1)\n\n\n[1] 4.413762\n\nIf you want to get the plots about bermuda options, Write this down.\n\n\ncode\n\nresult.table.r<-c(1:100)\nresult.table.sigma<-c(1:100)\nresult.table.StrikePrice<-c(1:100)\nresult.table.r2<-c(1:100)\nresult.table.sigma2<-c(1:100)\nresult.table.StrikePrice2<-c(1:100)\nsample.r<- seq(0.01,0.03,length.out = 100)\nsample.sigma<-seq(0.1,0.3,length.out = 100)\nsample.StrikePrice<-seq(80,120,length.out = 100)\n\n\n\nfor (i in 1:100) {\n  result.table.r[i]<-Bermuda_option(1,100,100,100,5,12,sample.r[i],0.2,20,40)\n  result.table.sigma[i]<-Bermuda_option(1,100,100,100,5,12,0.02,sample.sigma[i],20,40)\n  result.table.StrikePrice[i]<-Bermuda_option(1,100,100,sample.StrikePrice[i],5,12,0.02,0.2,20,40)\n  result.table.r2[i]<-Bermuda_option(1,100,100,100,5,12,sample.r[i],0.2,0,0)\n  result.table.sigma2[i]<-Bermuda_option(1,100,100,100,5,12,0.02,sample.sigma[i],0,0)\n  result.table.StrikePrice2[i]<-Bermuda_option(1,100,100,sample.StrikePrice[i],5,12,0.02,0.2,0,0)\n  \n}\n\nplot(sample.r,result.table.r,type = \"l\",col=\"red\",xlab = \"무위험이자율\",ylab=\"콜옵션 가격\",xlim=c(0.009,0.031),ylim=c(10,50))\nlines(sample.r,result.table.r2,type = \"l\")\nlegend(0.026,48,c(\"European\",\"Bermuda\"),col=c(\"black\",\"red\"),lty=1,cex=0.9)\n\nplot(sample.sigma,result.table.sigma,type = \"l\",col=\"red\",xlab = \"표준편차(시그마)\",ylab=\"콜옵션 가격\",xlim=c(0.05,0.40),ylim=c(10,50))\nlines(sample.sigma,result.table.sigma2,type = \"l\")\nlegend(0.32,49,c(\"European\",\"Bermuda\"),col=c(\"black\",\"red\"),lty=1,cex=0.9)\n\nplot(sample.StrikePrice,result.table.StrikePrice,type = \"l\",col=\"red\",xlab = \"행사가격\",ylab=\"콜옵션 가격\",xlim=c(75,125),ylim=c(10,50))\nlines(sample.StrikePrice,result.table.StrikePrice2)\nlegend(114,49,c(\"European\",\"Bermuda\"),col=c(\"black\",\"red\"),lty=1,cex=0.9)\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-31T08:39:06+00:00",
    "input_file": "Bermuda-Option-with-Black-Scholes.utf8.md"
  },
  {
    "path": "posts/1. Monte Carlo 를 이용한 NPV 자동 계산/",
    "title": "NPV Monte carlo",
    "description": "재무관리::몬테카를로 시뮬레이션을 통해 Net Present Value를 구해보자.",
    "author": [
      {
        "name": "WooramSeong",
        "url": {}
      }
    ],
    "date": "2020-03-18",
    "categories": [],
    "contents": "\n\nContents\nlibrary\nNPV Function with Tax (Monte carlo Simulation)\n\n\nlibrary\n\n\ncode\n\nlibrary(dplyr)\n\n\n\nNPV Function with Tax (Monte carlo Simulation)\nInitial Investment = ICASHFLOW before tax = CFGrowth rate = GTax rate = T Cost of Captial = CPeriod = N\n\n\ncode\n\nNPVRAM <-function(I,CF,G,T,C,N) {\n  PVCF = 0 \n  ACF= CF*(1-T)  ##After tax cash flow\n  for(i in 1:N){\n    PVCF[i]<-ACF*(1+G)^(i-1) / (1+C)^i \n  }\n  \n  sum(PVCF)-I}\n\n### Monte Function ###\n\nMonte <- function(PROB,RV) {if(PROB<=0.25){\n  return(RV[1])}\n  else if(PROB<=0.75) {\n    return(RV[2])\n  }\n  else  { \n    return(RV[3])\n  }}      \n\n\n\nNPVRAM(5000000,1000000,0.03,0.25,0.06,10)\n\n\n[1] 1239103\n\ncode\n\nA<-c(5500000,5000000,4500000)\nB<-c(900000,1000000,1100000)\nC<-c(0.02,0.03,0.04)\nD<-c(0.35,0.25,0.15)\nE<-c(0.07,0.06,0.05)\nF<-c(8,10,12)\n\n\nResult<-0\n\n\n### NPVMONTE function ###\n\n\nNPVMONTE <- function(x) {\n  \n  NPV1<-c(1:x)  \n  for(i in 1:x){\n    \n    I1<-Monte(runif(1),A)\n    CF1<-Monte(runif(1),B)\n    G1<-Monte(runif(1),C)\n    T1<-Monte(runif(1),D)\n    C1<-Monte(runif(1),E)\n    N1<-Monte(runif(1),F)\n    NPV1[i] <- NPVRAM(I1,CF1,G1,T1,C1,N1)                   \n  }\n  return(NPV1) \n}\n\nResult1<-NPVMONTE(10000)\nmean(Result1)  ## NPV ???հ?\n\n\n[1] 1238165\n\ncode\n\nvar(Result1)   ## NPV var ??\n\n\n[1] 1.33932e+12\n\ncode\n\nResult1<-as.data.frame(Result1)\ncolnames(Result1) <- \"NPV\"\nResult1<-arrange(Result1,NPV)\nPositive<- filter(Result1,NPV>0)  \nnrow(Positive)/nrow(Result1)     ## NPV ?? ?????? Ȯ?? ##\n\n\n[1] 0.8516\n\ncode\n\nAM<- filter(Result1,NPV>1000000) ## Above  1Million\nBM<- filter(Result1,NPV<1000000) ## Below  1Million\nnrow(AM)/nrow(Result1)   ## NPV 1,000,000 ?ʰ??? Ȯ??\n\n\n[1] 0.5539\n\ncode\n\nnrow(BM)/nrow(Result1)   ## NPV 1,000,000 ?̸??? Ȯ??\n\n\n[1] 0.4461\n\ncode\n\nResult1[100,1]   ## 1  percentile ##\n\n\n[1] -1091259\n\ncode\n\nResult1[1000,1]  ## 10 percentile ##\n\n\n[1] -205871.8\n\ncode\n\nResult1[9000,1]  ## 90 percentile ##\n\n\n[1] 2786013\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-31T08:13:21+00:00",
    "input_file": "NPV_Monte_Carlo.utf8.md"
  }
]
