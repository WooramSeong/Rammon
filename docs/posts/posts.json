[
  {
    "path": "posts/5. ì• ì¦ì˜ Dacon/",
    "title": "ì• ì¦ì˜ Dacon",
    "description": "How to execute Random Forest, Logistic, XGboost, LightGBM Model?",
    "author": [
      {
        "name": "WooramSeong",
        "url": {}
      }
    ],
    "date": "2021-05-31",
    "categories": [],
    "contents": "\n\nContents\ní•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬\në°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\në°ì´í„° ê¸°ë³¸ì •ë³´ í™•ì¸\nskim(train)\nskim(test)\nì¤‘ë³µê°’ì„ ì œê±°í•´ë³´ì.\n\nrecipeë¥¼ í†µí•œ ì „ì²˜ë¦¬ ì…ë ¥\njuiceë¥¼ í†µí•œ ì „ì²˜ë¦¬ ì¦™ì§œê¸°\níŠœë‹ ì¤€ë¹„í•˜ê¸°\n5 fold vs 10 foldë¥¼ ì‹¤í–‰í•´ë³´ì.\nstacking ì¤€ë¹„í•˜ê¸°\n\nëœë¤ í¬ë ˆìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•´ë³´ì,\níŠœë‹ëœ ëª¨ë¸ í•™ìŠµí•˜ê¸°\n\nLogitstic modelì„ ëŒë ¤ë³´ì.\nXGboost ë¥¼ ì‹¤í–‰í•´ë³´ì\nlight GBM ë¥¼ ì‹¤í–‰í•´ë³´ì\nStacking ì„ ì‹¤í–‰í•´ë³´ì.\nê·¸ë¦¬ë“œ ì¬ì„¤ì •\n\nì˜ˆì¸¡í•˜ê¸°\n\nì´ë²ˆ ê¸€ì€ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì§€ëŠ” ì•Šìœ¼ë ¤ê³  í•œë‹¤. ì™œëƒí•˜ë©´ ë„ˆë¬´ ì˜¤ë˜ê±¸ë¦¬ê¸° ë•Œë¬¸ì´ì£ ..\nì½”ë“œë§Œ ê°ìƒí•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.\ní•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬\n\n\ncode\n\nlibrary(parsnip)\nlibrary(xgboost)\nlibrary(magrittr)\nlibrary(tidymodels)\nlibrary(tidyverse) \nlibrary(skimr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ranger)\nlibrary(yardstick)\nlibrary(stacks)\nlibrary(randomForestExplainer)\nlibrary(corrplot)\nlibrary(tictoc)\nlibrary(nnet)\n#install.packages('devtools')\n#remotes::install_github(\"curso-r/treesnip\")\nlibrary(treesnip)\n#devtools::install_github(\"curso-r/rightgbm\")\n#rightgbm::install_lightgbm()  \nlibrary(lightgbm)\n\n\n\në°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n\n\ncode\n\n  file_path <- \"C:/Users/GIGABYTE/Desktop/ì‘ìš©í†µê³„í•™/Dacon\"\n  \n  files <-list.files(file_path)\n  \n  files\n  #ê° ë³€ìˆ˜ì˜ ì´ë¦„ì„ `janitor` íŒ¨í‚¤ì§€ë¡œ ë§ë”í•˜ê²Œ ë°”ê¿”ì¤€ë‹¤.\n  \n  \n  train <- read_csv(file.path(file_path, \"train.csv\"),\n                    col_types = cols(\n                      credit = col_factor(levels = c(\"0.0\", \"1.0\", \"2.0\"))\n                    )) %T>% \n    suppressMessages() %>% \n    janitor::clean_names()\n  test <- read_csv(file.path(file_path, \"test.csv\")) %T>%\n    suppressMessages() %>% \n    janitor::clean_names()\n\n\n\në°ì´í„° ê¸°ë³¸ì •ë³´ í™•ì¸\n\n\ncode\n\ntrain %>% \n      head() %>% \n       kable() %>% \n     kableExtra::kable_styling(\"striped\") %>% \n     kableExtra::scroll_box(width = \"100%\")\n\n\n\n#ê° ë°ì´í„° ì…‹ì˜ ë³€ìˆ˜ëª…ì„ ì‚´í´ë³´ì.\në¨¼ì € test ë°ì´í„°ì—ëŠ” ìš°ë¦¬ê°€ ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ ë³€ìˆ˜ì¸ credit ë³€ìˆ˜ê°€ ë“¤ì–´ìˆì§€ ì•Šì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\në°ì´í„°ë¥¼ í›‘ì–´ë³´ê¸° ìœ„í•´ì„œ skim() í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì. ì´ í•¨ìˆ˜ëŠ” ë°ì´í„°ì— ë“¤ì–´ìˆëŠ” ë³€ìˆ˜ë“¤ì„ íƒ€ì… ë³„ë¡œ ë¶„ì„í•´ì„œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì¤€ë‹¤.\nskim(train)\nê²°ê³¼ë¥¼ ì‚´í´ë³´ì. ë¨¼ì € ê²°ì¸¡ì¹˜ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë§ì´ ì—†ëŠ” ì°©í•œ? ë°ì´í„°ì´ë‹¤. character ë³€ìˆ˜ì˜ complete rateë¥¼ ì‚´í´ë³´ë©´ ëª¨ë“  ë³€ìˆ˜ê°€ 1ì´ê³ , occyp_type ë³€ìˆ˜ë§Œì´ ê²°ì¸¡ì¹˜ê°€ 8171ê°œê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë˜í•œ ê³ ë§™ê²Œë„ numeric ë³€ìˆ˜ì˜ ê²°ì¸¡ì¹˜ëŠ” í•˜ë‚˜ë„ ì—†ë‹¤!ğŸ˜†\nê°™ì€ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ test ì…‹ì„ ë³´ë©´ ë˜‘ê°™ì€ íŒ¨í„´ì„ ê°€ì§€ê³  ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\nskim(test)\n\n\ncode\n\n# ì‹œê°í™”\n  #ë² ì´ìŠ¤ ë¼ì¸ì„ ì¡ì€ ë¬¸ì„œì´ë‹ˆ ê°„ë‹¨í•˜ê²Œ ì‹œê°í™” í•˜ë‚˜ë§Œ í•˜ê³  ë„˜ì–´ê°€ì. (ì½”ë“œë¥¼ ì‘ìš©í•´ì„œ ë‹¤ë¥¸ ë³€ìˆ˜ì— ëŒ€í•œ ìƒê´€ ê´€ê³„ë¥¼ ë³¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.)\n  \npre_train<-train[,c(-1,-17)]\npre_train %<>% mutate(income_Per_Familymember = income_total/family_size)\n#ë³€ìˆ˜ ì¤‘ìš”ë„ê°€ ë†’ì€ Begin_month, income_total, yrs_birth, income_PM\n#Preliminary_fit2 <- ranger(credit ~ ., data = pre_train , importance = \"impurity_corrected\")\n#pvalues<-importance_pvalues(Preliminary_fit2, method = \"janitza\") \n#pvalues\n#p-valueê°€ 0ì— ê°€ê¹Œìš´ Begin_month, days_birth, days_employed ì„ ë°œê²¬\n#train %>%\n  \n#  ggplot(aes(x = factor(credit), y = income_total)) +\n  \n#  geom_boxplot()\n## Creditì´ ë°”ë€Œì–´ë„ income_totalì˜ ìœ ì˜ë¯¸í•œ ê²°ê³¼ê°€ ë³´ì´ì§€ ì•ŠìŒ\n#train %>%\n  \n#  ggplot(aes(x = factor(credit), y = days_birth)) +\n  \n#  geom_boxplot()\n#train %>%\n  \n#  ggplot(aes(x = factor(credit), y = begin_month)) +\n  \n#  geom_boxplot()\ntrain %>%\n  \n  ggplot(aes(x = factor(credit), y = days_employed)) +\n  \n  geom_boxplot()\n## ë°ì´í„° 365,243 ì²˜ë¦¬ë¥¼ ì–´ë–»ê²Œ í•´ì•¼ í•˜ëŠ”ê°€ 365,243,ì„ ì œì™¸í•œ ìµœëŒ“ê°’ or ìµœì†Ÿê°’ or ì¤‘ì•™ê°’\ntrain %>% filter(days_employed>300000)\ntrain %>% filter(days_employed>300000)%>%select(income_type,days_employed)\nmin(Days_employed)\nmax(Days_employed)\nmedian(Days_employed)\ntrain$days_employed[train$days_employed == 365243] <- -1977\ntest$days_employed[test$days_employed == 365243] <- -1977\n## Creditì´ ë°”ë€Œì–´ë„ income_totalì˜ ìœ ì˜ë¯¸í•œ ê²°ê³¼ê°€ ë³´ì´ì§€ ì•ŠìŒ\npost_train<-train\npost_train$credit<-as.numeric(post_train$credit)\ncorrplot(cor(post_train[,c(20,19,12,11,6)]))\n#begin_Month ë°ì´í„° ì „ì²˜ë¦¬ê°€ í•µì‹¬ì¸ê±° ê°™ë‹¤.\ntrain %>%\n  \n ggplot(aes(x = factor(credit), y = income_total)) +\n  \n  geom_boxplot() +\n  \n  facet_grid(. ~ occyp_type)\ndim(train)\nmin(train$income_total)\ntrain %<>% filter(income_total<1250000 & income_total>27000)\ndim(train)\ncolnames(train)\ntrain %>%\n  \n ggplot(aes(x = factor(credit), y = begin_month)) +\n  \n  geom_boxplot() +\n  \n  facet_grid(. ~ occyp_type)\n#  train %>%\n  \n#  ggplot(aes(x = factor(credit), y = begin_month)) +\n  \n#  geom_boxplot() +\n  \n#  facet_grid(. ~ income_type)\n  \n  train %>%\n    \n    ggplot(aes(x = factor(credit), y = begin_month)) +\n    \n    geom_boxplot() +\n    \n    facet_grid(. ~ income_type)\ntrain %>%\n ggplot(aes(x = factor(credit), y = family_size)) +\n  geom_boxplot() \ntrain %>% select(income_type,begin_month) %>% filter(income_type==\"Student\")\ntrain %>% select(credit,occyp_type,begin_month) %>% filter(occyp_type=='HR staff' & credit == \"1.0\") %>% arrange(desc(begin_month))\ntrain %>% select(credit,occyp_type,begin_month) %>% filter(occyp_type=='Realty agents' & credit == \"1.0\") %>% arrange(begin_month)\ntrain %>% select(credit,occyp_type,begin_month) %>% filter(occyp_type=='Waiters/barmen staff' & credit == \"1.0\") %>% arrange(begin_month)\nintersect(which(train$occyp_type == 'HR staff'),which(train$begin_month == -8))\nintersect(which(train$occyp_type == 'Realty agents'& train$credit== \"1.0\"),which(train$begin_month == -53))\nintersect(which(train$occyp_type == 'Realty agents' & train$credit== \"1.0\"),which(train$begin_month == -43))\nintersect(which(train$occyp_type == 'Waiters/barmen staff' & train$credit== \"1.0\"),which(train$begin_month == -56))\nintersect(which(train$income_type == 'Student'),which(train$begin_month == -60))\n#2990,11276,15649,20515,25602, í–‰ì„ ì œê±°í•˜ì\ntrain<-train[c(-2990,-11276,-15649,-20515,-25602),]\n## income_totalì´ ìƒê´€ê³„ìˆ˜ê°€ 0ì— ê°€ê¹Œì›Œì„œ ì‚¬ëŒìˆ˜ë¡œ ë‚˜ëˆ„ì–´ í‰ê·  ìˆ˜ì…ìœ¼ë¡œ ë³€ìˆ˜ë¥¼ ë§Œë“¤ë ¤ í–ˆìœ¼ë‚˜ ìƒê´€ê³„ìˆ˜ê°€ ì—­ì‹œë‚˜ ì ì—ˆë‹¤. \n#train$credit<-as.numeric(train$credit)\n#train %<>% mutate(income_PM = income_total/family_size)\n#train$income_PM\n#sum(is.na(train$income_PM))\n#cor(train[,c(20,19,12,11,6,21)])\n#corrplot(cor(train[,c(20,19,12,11,21)]))\n#ëª©í‘œ ë³€ìˆ˜ì¸ creditì€ ë‚®ì„ ìˆ˜ë¡ ë†’ì€ ì‹ ìš©ì˜ ì‹ ìš©ì¹´ë“œ ì‚¬ìš©ìë¥¼ ì˜ë¯¸ í•œë‹¤ê³  í•œë‹¤. Commercial associate ì¸ ê²½ìš° ì‹ ìš©ì´ ì œì¼ ë‚®ì€ ê·¸ë£¹ì˜ ìˆ˜ì…ì˜ ì¤‘ì•™ê°’ì´ ì œì¼ ë†’ë‹¤. ëˆì„ ë§ì´ ë²Œìˆ˜ë¡ ëˆ ê°šì€ ê°œë…ì´ ì—†ì–´ì§€ëŠ” ê²ƒì¸ê°€? ì¬ë¯¸ìˆëŠ” í˜„ìƒì´ë‹¤. í•™ìƒ í´ë˜ìŠ¤ì˜ ê²½ìš° train ë°ì´í„°ì— ì…‹ì´ ë§ì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¶”í›„ì— ë‹¤ë¥¸ í´ë˜ìŠ¤ë¡œ í†µí•©ì„ ì‹œí‚¤ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒì´ë‹¤.\n\n\n\nì¤‘ë³µê°’ì„ ì œê±°í•´ë³´ì.\n\n\ncode\n\nbirth_table<-as.data.frame(table(train$days_birth))  \nbirth_table%>%filter(Freq>10)%>% arrange(desc(Freq))\n#days_birth -12676,-15519,-14667 íƒìƒ‰!!\na1<-train%>%filter(days_birth == -12676) %>% select(index,days_employed,income_total,begin_month,days_birth,family_size,credit)%>%arrange(days_employed,income_total,begin_month,index)\na2<-train%>%filter(days_birth == -15519) %>% select(index,days_employed,income_total,begin_month,days_birth,family_size,credit)%>%arrange(days_employed,income_total,begin_month,index)\na3<-train%>%filter(days_birth == -14667) %>% select(index,days_employed,income_total,begin_month,days_birth,family_size,credit)%>%arrange(days_employed,income_total,begin_month,index)\ntrain$credit<-as.numeric(train$credit)\n#train_max<-train%>%group_by(days_employed,gender,income_total,credit,family_size,child_num) %>% mutate(begin_month=min(begin_month))\n#train_min<-train%>%group_by(days_employed,gender,income_total,credit,family_size,child_num) %>% mutate(begin_month=max(begin_month))\n#train_median<-train%>%group_by(days_employed,gender,income_total,credit,family_size,child_num) %>% mutate(begin_month=median(begin_month))\n#corrplot(cor(train_max[,c(20,19,12,11,6)]))\n#corrplot(cor(train_min[,c(20,19,12,11,6)]))\n#corrplot(cor(train_median[,c(20,19,12,11,6)]))\n#corrplot(cor(train[,c(20,19,12,11,6)]))\n#ë°ì´í„° ì •ë ¬ í–‰ë²ˆí˜¸ê°€ ê°€ì¥ ë¹ ë¥¸ ì• ë“¤ë§Œ ë‚¨ê²Œ ë˜ê¸° ë•Œë¬¸ì— ë°œê¸‰ì¼ì´ ê°€ì¥ ì˜¤ë˜ëœ ê²ƒë“¤ì„ ë‚¨ê¸°ê¸°ë¡œ í•˜ì.\ncolnames(train)\n#train_order <- train[order(train[,'begin_month'],decreasing = FALSE), ]\n#train_order <- train_order[!duplicated(train[,c('days_birth','work_phone','phone','reality','house_type','car','days_employed','edu_type','occyp_type','child_num','income_total','income_type','family_size','begin_month','credit')]),]\n#train_order$credit<-as.numeric(train_order$credit)\n#corrplot(cor(post_train[,c(20,19,12,11,6)]))\n#corrplot(cor(train_order[,c(20,19,12,11,6)]))\n#ì´ë²ˆì—ëŠ” ìµœê·¼ ë°œê¸‰ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚¨ê¸°ê¸°ë¡œ í•˜ì.\n#train_order <- train[order(train[,'begin_month'],decreasing = TRUE), ]\n#train_order <- train_order[!duplicated(train[,c('days_birth','work_phone','phone','reality','house_type','car','days_employed','edu_type','occyp_type','child_num','income_total','income_type','family_size','begin_month','credit')]),]\n#train_order$credit<-as.numeric(train_order$credit)\n#corrplot(cor(post_train[,c(20,19,12,11,6)]))\n#corrplot(cor(train_order[,c(20,19,12,11,6)]))\n## Begin_month ì¤‘ë³µê°’ ì¤‘ ìµœê·¼ ë°œê¸‰ì¼ì„ ë‚¨ê¸´ ê²°ê³¼ ìƒê´€ ê´€ê³„ê°€ ë†’ì•„ì§€ëŠ” ê²°ê³¼ê°€ ë‚˜íƒ€ë‚¬ë‹¤.\n#train_order_inc <- train[order(train[,'begin_month'],decreasing = FALSE), ]\n#train_order_inc <- train_order_inc[!duplicated(train[,c('gender','edu_type','flag_mobil','email','days_birth','work_phone','phone','reality','house_type','car','days_employed','edu_type','occyp_type','child_num','income_total','income_type','family_size','credit')]),]\n#train_order_dec <- train[order(train[,'begin_month'],decreasing = TRUE), ]\n#train_order_dec <- train_order_dec[!duplicated(train[,c('gender','edu_type','flag_mobil','email','days_birth','work_phone','phone','reality','house_type','car','days_employed','edu_type','occyp_type','child_num','income_total','income_type','family_size','credit')]),]\n#train_union<-union(train_order_dec,train_order_inc)\n#train_order$credit<-as.numeric(train_order$credit)\n#train_order_inc$credit<-as.numeric(train_order_inc$credit)\n#train_order_dec$credit<-as.numeric(train_order_dec$credit)\n#train_union$credit<-as.numeric(train_union$credit)\n#corrplot(cor(post_train[,c(20,19,12,11,6)]))\n#corrplot(cor(train_order_inc[,c(20,19,12,11,6)]))\n#corrplot(cor(train_order_dec[,c(20,19,12,11,6)]))\n#corrplot(cor(train_union[,c(20,19,12,11,6)]))\n#train_order_dec\n#train<-train_union\n## ì¬ë°ŒëŠ”ì ì€ ìƒê´€ê´€ê³„ê°€ ë†’ë‹¤ê³  í•´ì„œ mean_log_lossê°€ ë‚®ì•„ì§€ëŠ”ê±´ ì•„ë‹˜\n## ì˜¤íˆë ¤ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìª½ì—ì„œ log loss ê°’ì´ ë” í¬ê²Œ ë‚˜ì˜´ ~_~\n# ì „ì²˜ë¦¬ í•˜ê¸°\n#`tidymodels`ì—ì„œëŠ” ì „ì²˜ë¦¬ë¥¼ í•  ë•Œ `recipe` ë¼ëŠ” íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•œë‹¤. ì´ íŒ¨í‚¤ì§€ì—ëŠ” ì „ì²˜ë¦¬ë¥¼ í•˜ëŠ” ë°©ë²•ì„ ìŒì‹ ë ˆí”¼ì‹œ ì²˜ëŸ¼ ì ì–´ë†“ëŠ”ë‹¤ê³  ìƒê°í•˜ë©´ ì‰½ë‹¤.\n## ì „ì²˜ë¦¬ ì‚¬í•­ë“¤\n# ê²°ê³¼ê°’ì¸ credit ë³€ìˆ˜ì™€ character íƒ€ì…ì˜ ë³€ìˆ˜ë“¤ì„ factor ë³€ìˆ˜ë¡œ ë°”ê¿”ì£¼ì.\n# ë‚˜ì´ì™€ ì§ì—…ì„ ê°€ì§„ ê¸°ê°„ì„ ë…„ìˆ˜ë¡œ ë°”ê¿”ì¤€ë‹¤.\n\n\n\nrecipeë¥¼ í†µí•œ ì „ì²˜ë¦¬ ì…ë ¥\n\n\ncode\n\ncredit_recipe <- train %>% \n  \n  recipe(credit ~ .) %>% \n  \n  # age and employment period in yrs\n  \n  #step_mutate(yrs_birth = -ceiling(days_birth/365),\n              \n  #           yrs_employed = -ceiling(days_employed/365)) %>% \n  \n  step_rm(index,flag_mobil, child_num) %>%\n  \n  #child_numì€ family sizeë‘ ìƒê´€ê³„ìˆ˜ê°€ ë§¤ìš° ë†’ì•„ ì œê±°í•´ì£¼ì—ˆë‹¤. \n  \n  #flag_mobilì€ ë‹¤ ê°’ì´ 1ë¡œ ë™ì¼í•˜ì—¬ ì œê±°í•´ì£¼ì—ˆë‹¤.\n  \n  step_unknown(occyp_type) %>% \n  \n  step_integer(all_nominal(), -all_outcomes()) %>% \n  \n  step_corr(all_predictors(), -all_outcomes()) %>% \n  \n  step_scale(all_predictors(), -all_outcomes()) %>%\n  \n  step_nzv(all_predictors(), -all_outcomes()) %>%\n  \n  step_center(all_predictors(), -all_outcomes()) %>% \n  \n  prep(training = train)\nprint(credit_recipe)\n\n\n\njuiceë¥¼ í†µí•œ ì „ì²˜ë¦¬ ì¦™ì§œê¸°\n\n\ncode\n\n#`juice()` í•¨ìˆ˜ë¥¼ í†µí•´ì„œ recipeì— ì…ë ¥ëœ ì „ì²˜ë¦¬ë¥¼ ì§œë‚¸ ë°ì´í„°ë¥¼ ì–»ì–´ì˜¨ë‹¤.\ntrain2 <- juice(credit_recipe)\ntest2 <- bake(credit_recipe, new_data = test)\nhead(train2)\nhead(test2)\ntrain2$credit<-as.factor(train2$credit)\n#ë‹¤ìŒê³¼ ê°™ì´ ê²°ì¸¡ì¹˜ ì—†ì´ ì˜ ì½”ë”©ëœ ë°ì´í„°ë¥¼ ì–»ì—ˆë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.\ntrain2 %>%\n    \n  map_df(~sum(is.na(.))) %>%\n  \n  pivot_longer(cols = everything(),\n               \n               names_to = \"variable\",\n               \n               values_to = \"na_count\") %>% \n  \n  filter(na_count > 0)\n\n\n\níŠœë‹ ì¤€ë¹„í•˜ê¸°\nvalidation_split() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ì…‹ì„ ë¶„ë¦¬í•œë‹¤. í•œ ë‹¨ê³„ ë” ë‚˜ì•„ê°„ cross validationì€ vfold_cv()í•¨ìˆ˜ì—ì„œ ì œê³µí•˜ë‹ˆ ì°¾ì•„ë³´ë„ë¡ í•˜ì.\n5 fold vs 10 foldë¥¼ ì‹¤í–‰í•´ë³´ì.\n\n\ncode\n\nset.seed(2002)\nvalidation_split <- vfold_cv(v=10, train2, strata = credit)\n#validation_split <- validation_split(train2, prop = 0.3,   strata = credit)\nvalidation_split\n\n\n\nstacking ì¤€ë¹„í•˜ê¸°\n\n\ncode\n\nctrl_res <- control_stack_resamples()\nctrl_grid <- control_stack_grid()\n\n\n\nëœë¤ í¬ë ˆìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•´ë³´ì,\nmtryì™€ min_nì„ ì–´ë–»ê²Œ ì •í• ì§€ë¥¼ í‰ê°€ì…‹ì„ í†µí•´ì„œ ê²°ì •í•  ê²ƒì´ë¯€ë¡œ, tune()ë¥¼ ì‚¬ìš©í•´ì„œ tidymodelsì—ê²Œ ì•Œë ¤ì£¼ë„ë¡ í•œë‹¤.\n\n\ncode\n\ncores <- parallel::detectCores() -1\ncores\ntune_spec <- rand_forest(mtry = tune(),\n                         \n                         min_n = tune(),\n                         \n                         trees = 1000) %>% \n  \n  set_engine(\"ranger\",\n             \n             num.threads = cores) %>% \n  \n  set_mode(\"classification\")\n# from param tune\n#param_grid <- tibble(mtry = c(4,4,4,4), min_n=c(3,4,5,6)) # mtry=3ìœ¼ë¡œ ê³ ì •ì‹œí‚¤ì\nparam_grid <- grid_random(finalize(mtry(), x = train2[,-1]), min_n(),size = 5000, filter = 2<mtry & mtry<5 & min_n<15 & 2<min_n)\nparam_grid\n# ì›Œí¬ í”Œë¡œìš° ì„¤ì •\nworkflow <- workflow() %>%\n  \n  add_model(tune_spec) %>% \n  \n  add_formula(credit ~ .)\n# ëª¨ë¸ íŠœë‹ with tune_grid()\n# Tuning trees\ntic()\ntune_result <- workflow %>% \n  \n  tune_grid(validation_split,\n            grid = param_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res)\ntoc()\ntune_result %>% \n  \n  collect_metrics()\n# íŠœë‹ê²°ê³¼ ì‹œê°í™”\ntune_result %>%\n  \n  collect_metrics() %>%\n  \n  filter(.metric == \"mn_log_loss\") %>% \n  \n  ggplot(aes(mtry, mean, color = .metric)) +\n  \n  geom_line(size = 1.5) +\n  \n  scale_x_log10() +\n  \n  theme(legend.position = \"none\") +\n  \n  labs(title = \"Mean Log loss\")\ntune_result %>% show_best()\ntune_best <- tune_result %>% select_best(metric = \"mn_log_loss\")\ntune_best$mtry\ntune_best$min_n\n\n\n\n1ì°¨ì‹œë„ (3,5), (3,6), (3,7), (3,8), (3,4) , minimum mean log loss 0.708 5 fold cv ìµœê·¼ ì‹ ìš©ì¹´ë“œ ë°œê¸‰ ê¸°ë¡ë§Œ ë‚¨ê¸°ê¸°  2ì°¨ì‹œë„ (3,5), (3,7), (3,6), (3,4), (3,8) , minimum mean log loss 0.706 5 fold cv ì²«ë²ˆì§¸ ì‹ ìš©ì¹´ë“œ ë°œê¸‰ ê¸°ë¡ë§Œ ë‚¨ê¸°ê¸°  3ì°¨ì‹œë„ (3,4), (3,5), (3,7), (3,6), (3,8) , minimum mean log loss 0.698 10 fold CV + ì²«ë²ˆì§¸ ì‹ ìš©ì¹´ë“œ ë°œê¸‰ ê¸°ë¡ë§Œ ë‚¨ê¸°ê¸°  4ì°¨ì‹œë„ (4,4), (4,5) , minimum mean log loss 0.404 5 fold CV + ë™ì¼ ê³ ê°ì´ ì—¬ëŸ¬ë²ˆ ë°œê¸‰í•  ê²½ìš° begin_monthë¥¼ ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œë¡œ í†µì¼  5ì°¨ì‹œë„ (4,4), (4,5) , minimum mean log loss 0.404 5 fold CV + ë™ì¼ ê³ ê°ì´ ì—¬ëŸ¬ë²ˆ ë°œê¸‰í•  ê²½ìš° begin_monthë¥¼ ê°€ì¥ ëŠ¦ì€ ë‚ ì§œë¡œ í†µì¼  6ì°¨ì‹œë„ (3,5), (3,6) , minimum mean log loss 0.697 10 fold CV + ë³€ì£¼ ì œê±°ë¥¼ index ë‘ flag_mobil ë§Œ í•¨\níŠœë‹ëœ ëª¨ë¸ í•™ìŠµí•˜ê¸°\n\n\ncode\n\nrf_model <- \n  \n  rand_forest(mtry = tune_best$mtry,\n              \n              min_n = tune_best$min_n,\n              \n              trees = 1000) %>% \n  \n  set_engine(\"ranger\", seed = 2022, \n             \n             num.threads = cores, importance = 'impurity') %>% \n  \n  set_mode(\"classification\")\nrf_model2 <- \n  \n  rand_forest(mtry = tune_best$mtry,\n              \n              min_n = tune_best$min_n,\n              \n              trees = 1000) %>% \n  \n  set_engine(\"ranger\", seed = 2023, \n             \n             num.threads = cores, importance = 'impurity') %>% \n  \n  set_mode(\"classification\")\nrf_model3 <- \n  \n  rand_forest(mtry = tune_best$mtry,\n              \n              min_n = tune_best$min_n,\n              \n              trees = 1000) %>% \n  \n  set_engine(\"ranger\", seed = 2024, \n             \n             num.threads = cores, importance = 'impurity') %>% \n  \n  set_mode(\"classification\")\ntictoc::tic()\nrf_fit <- \n  \n  rf_model %>% \n  \n  fit(credit ~ ., data = train2)\ntictoc::toc()\nrf_fit2 <- \n  \n  rf_model2 %>% \n  \n  fit(credit ~ ., data = train2)\nrf_fit3 <- \n  \n  rf_model3 %>% \n  \n  fit(credit ~ ., data = train2)\nrf_fit\nrf_fit2\nrf_fit3\n\n\n\nRanger result\nType: Probability estimation\nNumber of trees: 1000\nSample size: 24806\nNumber of independent variables: 16\nMtry: 3\nTarget node size: 5\nVariable importance mode: impurity\nSplitrule: gini\nOOB prediction error (Brier s.): 0.2288811\nLogitstic modelì„ ëŒë ¤ë³´ì.\n\n\ncode\n\nlogit_spec <- multinom_reg(penalty = tune(),\n                          mixture = tune()) %>%\n  set_engine(\"glmnet\")%>%\n  set_mode(\"classification\")\n  \nlambda_grid <- grid_regular(penalty(), \n                            mixture(),\n                            levels = list(penalty = 100,\n                                          mixture = 25))\nlambda_grid\nlogit_workflow <-workflow() %>% \n  add_model(logit_spec) %>%\n  add_formula(credit~.)\nlogit_result<- logit_workflow %>%\n  tune_grid(validation_split,\n            grid=lambda_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res) \nlogit_result %>% collect_metrics()\nlogit_result %>% show_best()\n\n\n\n1ì°¨ì‹œë„ Penalty 0.00870 Mixture 0.111, Penalty 0.00775 Mixture 0.111, minimum mean log loss 0.859 10-fold CV ì²«ë²ˆì§¸ ì‹ ìš©ì¹´ë“œ ë°œê¸‰ ê¸°ë¡ë§Œ ë‚¨ê¸°ê¸°  2ì°¨ì‹œë„ Penalty 0.00955 Mixture 0.0833, Penalty 0.00775 Mixture 0.111, minimum mean log loss 0.863 10-fold CV ì‹ ìš©ì¹´ë“œ ì¤‘ë³µ ì œê±° X \në¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨í˜•ì€ ì¤‘ë³µì„ ì œê±°í•˜ëŠ”ê²Œ ë‚«ê³ , ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨í˜•ì€ ì¤‘ë³µê°’ì„ ì‚­ì œ í•˜ì§€ ì•ŠëŠ”ê²Œ mean log lossê°€ ë” ì˜ ë‚˜ì˜´\n\n\ncode\n\nlogit_tune_best <- logit_result %>% select_best(metric=\"mn_log_loss\")\nlogit_tune_best$penalty\nlogit_model <- multinom_reg(penalty = logit_tune_best$penalty,\n                  mixture = logit_tune_best$mixture) %>% \n                  set_engine(\"glmnet\", seed = 2022, num.threads = cores) %>%            \n                  set_mode(\"classification\")\nlogit_fit <- logit_model %>% fit(credit~., data = train2)\nlogit_fit\nlogit_pred <- predict(logit_fit, test2, type=\"prob\")\nlogit_pred\n\n\n\nXGboost ë¥¼ ì‹¤í–‰í•´ë³´ì\n\n\ncode\n\nxgb_spec <- boost_tree(\n  \n  trees = 1000, \n  \n  tree_depth = tune(), min_n = tune(), \n  \n  loss_reduction = tune(),                     ## first three: model complexity\n  \n  sample_size = tune(), mtry = tune(),         ## randomness\n  \n  learn_rate = tune(),                         ## step size\n  \n) %>% \n  \n  set_engine(\"xgboost\") %>% \n  \n  set_mode(\"classification\")\nxgb_spec\nxgb_grid <- grid_latin_hypercube(\n  \n  tree_depth(),\n  \n  min_n(c(1,10)),\n  \n  loss_reduction(c(-1,0.1)),\n  \n  sample_size = sample_prop(range=c(0.5,1)),\n  \n  learn_rate(c(-1,0)),\n  \n  size = 100\n  \n)\nxgb_grid\nxgb_workflow <- workflow() %>%\n    add_model(xgb_spec) %>% \n    add_formula(credit ~ .)\nxgb_result<- xgb_workflow %>%\n  \n  tune_grid(validation_split,\n            grid=xgb_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res) \nxgb_result %>% collect_metrics()\nxgb_result %>% show_best()\nxgb_tune_best <- xgb_result %>% select_best(metric=\"mn_log_loss\")\n\n\n\n1ì°¨ ê²°ê³¼ mtry 3 min_n 13 tree_depth 13 learn late 0.126 loss reduction 1.19, sample size 0.983 mean log loss 0.726  2ì°¨ ê²°ê³¼ mtry 3 min_n 7 tree_depth 10 learn late 0.183 loss reduction 0.921, sample size 0.910 mean log loss 0.733  3ì°¨ ê²°ê³¼ mtry 3 min_n 4 tree_depth 11 learn late 0.110 loss reduction 1.11, sample size 0.891 mean log loss 0.722 \n\n\ncode\n\nxgb_model <- boost_tree(\n   trees = 1000, \n   tree_depth = xgb_tune_best$tree_depth, \n   min_n = xgb_tune_best$min_n, \n   loss_reduction = xgb_tune_best$loss_reduction,## first three: model complexity\n   sample_size = xgb_tune_best$sample_size, mtry = xgb_tune_best$mtry,         ## randomness\n   learn_rate = xgb_tune_best$learn_rate,                         ## step size\n  ) %>% \n  set_engine(\"xgboost\") %>% \n  set_mode(\"classification\")\nxgb_fit <- \n    xgb_model %>% fit(credit ~ ., data = train2)\nxgb_fit\n\n\n\nlight GBM ë¥¼ ì‹¤í–‰í•´ë³´ì\n\n\ncode\n\ngbm_spec <- boost_tree(\n  \n  trees = 1000, \n  \n  tree_depth = tune(), min_n = tune(), \n  \n  loss_reduction = tune(),                     ## first three: model complexity\n  \n  sample_size = tune(), mtry = tune(),         ## randomness\n  \n  learn_rate = tune(),                         ## step size\n  \n) %>% \n  \n  set_engine(\"lightgbm\") %>% \n  \n  set_mode(\"classification\")\ngbm_spec\ngbm_grid <- grid_latin_hypercube(\n  \n  mtry(c(3,9)),\n  \n  tree_depth(c(11,15)),\n  \n  min_n(c(3,11)),\n  \n  loss_reduction(c(-1,0.1)),\n  \n  sample_size = sample_prop(range=c(0.5,1)),\n  \n  learn_rate(c(-1,0)),\n  \n  size = 100 \n)\ngbm_grid\ngbm_workflow <- workflow() %>%\n  add_model(gbm_spec) %>% \n  add_formula(credit ~ .)\ntic()\ngbm_result<- gbm_workflow %>%\n    tune_grid(validation_split,\n            grid=gbm_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res) \ntoc()\ngbm_result %>% collect_metrics()\ngbm_result %>% show_best()\ngbm_tune_best <- gbm_result %>% select_best(metric=\"mn_log_loss\")\n\n\n\ngbm 1ì°¨ ê²°ê³¼ mtry 4 min_n 13 tree_depth 15 learn late 0.11 loss reduction 0.141, sample size 0.845 mean log loss 0.725  gbm 2ì°¨ ê²°ê³¼ mtry 8 min_n 3 tree_depth 14 learn late 0.104 loss reduction 0.327, sample size 0.582 mean log loss 0.718  gbm 3ì°¨ ê²°ê³¼ mtry 7 min_n 11 tree_depth 14 learn late 0.121 loss reduction 0.126, sample size 0.882 mean log loss 0.717  gbm 4ì°¨ ê²°ê³¼ mtry 8 min_n 6 tree_depth 15 learn late 0.124 loss reduction 0.242, sample size 0.675 mean log loss 0.715  gbm 5ì°¨ ê²°ê³¼ mtry 9 min_n 11 tree_depth 15 learn late 0.169 loss reduction 0.148, sample size 0.664 mean log loss 0.716\n\n\ncode\n**íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì¢€ ë” í•˜ë©´ ìœ ì˜ë¯¸í•œ ê²°ê³¼ê°€ ë‚˜ì˜¬ê±° ê°™ë‹¤.**\ngbm_model <- boost_tree(\n  trees = 1000, \n  tree_depth = gbm_tune_best$tree_depth, \n  min_n = gbm_tune_best$min_n, \n  loss_reduction = gbm_tune_best$loss_reduction,## first three: model complexity\n  sample_size = gbm_tune_best$sample_size, mtry = gbm_tune_best$mtry,         ## randomness\n  learn_rate = gbm_tune_best$learn_rate,                         ## step size\n) %>% \n  set_engine(\"lightgbm\") %>% \n  set_mode(\"classification\")\ngbm_fit <- \n  gbm_model %>% fit(credit ~ ., data = train2)\ngbm_fit\n\nStacking ì„ ì‹¤í–‰í•´ë³´ì.\n##ëœë¤í¬ë ˆìŠ¤íŠ¸ì—ì„œ 2ê°œ + ë¡œì§€ìŠ¤í‹± ëª¨í˜•ì—ì„œ 1ê°œ + xgboost ì—ì„œ 2ê°œ + lightGBM ì—ì„œ 2ê°œ\nê·¸ë¦¬ë“œ ì¬ì„¤ì •\n\n\ncode\n\nparam_grid <- tibble(mtry=c(3,3),min_n=c(5,6))\nlambda_grid <- tibble(mixture=c(0.1),penalty=c(0.0087))\nxgb_grid <- tibble(mtry=c(3,3),min_n=c(13,4),tree_depth=c(13,11),learn_rate=c(0.126,0.11),loss_reduction=c(1.19,1.11),sample_size=c(0.983,0.891))\ngbm_grid <- tibble(mtry=c(8,9),min_n=c(6,11),tree_depth=c(15,15),learn_rate=c(0.124,0.169),loss_reduction=c(0.242,0.148),sample_size=c(0.675,0.664))\ntic()\ntune_result <- workflow %>% \n  tune_grid(validation_split,\n            grid = param_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res)\nlogit_result<- logit_workflow %>%\n  tune_grid(validation_split,\n            grid=lambda_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res) \nxgb_result<- xgb_workflow %>%\n  tune_grid(validation_split,\n            grid=xgb_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res) \ngbm_result<- gbm_workflow %>%\n  tune_grid(validation_split,\n            grid=gbm_grid,\n            metrics = metric_set(mn_log_loss),\n            control = ctrl_res) \ntoc()\ncredit_stacking <- \n  stacks() %>% \n  add_candidates(tune_result) %>% \n  add_candidates(logit_result) %>%\n  add_candidates(xgb_result)  %>%\n  add_candidates(gbm_result)\ncredit_stacking\nas_tibble(credit_stacking)\nstacking_model <- credit_stacking %>% blend_predictions() %>% fit_members() \nstacking_model\nfinal_result <- predict(stacking_model, test2,type = \"prob\")\nfinal_result\n\n\n\nì˜ˆì¸¡í•˜ê¸°\n\n\ncode\n\nresult1 <- predict(rf_fit, test2, type = \"prob\")\nresult1 %>% head()\nresult2 <- predict(rf_fit2, test2, type = \"prob\")\nresult2 %>% head()\nresult3 <- predict(rf_fit3, test2, type = \"prob\")\nresult3 %>% head()\nresult4 <- predict(xgb_fit, test2, type = \"prob\")\nresult4 %>% head()\nresult5 <- predict(gbm_fit, test2, type = \"prob\")\nresult5 %>% head()\nresult<-(result1+result2+result3)/3\nresult %>% head()\nsoftresult<- (result3+logit_pred)/2\nsoftresult %>% head()\nsubmission <- read_csv(file.path(file_path, \"sample_submission.csv\"))\nsub_col <- names(submission)\nsubmission <- bind_cols(submission$index, final_result)\nnames(submission) <- sub_col\nwrite.csv(submission, row.names = FALSE,\n          \n          \"rammon33.csv\")\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-31T12:44:47+00:00",
    "input_file": "Dacon-code.utf8.md"
  },
  {
    "path": "posts/4. ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•œ ìµœì ì˜ Parameter ê³„ì‚°/",
    "title": "Gradient Descent",
    "description": "ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•´ ìµœì ì˜ ëª¨ìˆ˜ë¥¼ ì°¾ì•„ë‚´ì.",
    "author": [
      {
        "name": "WooramSeong",
        "url": {}
      }
    ],
    "date": "2021-05-14",
    "categories": [],
    "contents": "\n\nContents\n2. Maximum Likelihood Estimator of Poisson Regression\n\n2. Maximum Likelihood Estimator of Poisson Regression\n\n\n\ncode\n\nmodel<-glm(Claims ~ Gender + Territory,\nfamily = poisson(link=log), data = example_data)\nmodel$coefficients\n\n\n  (Intercept)       GenderM TerritoryWest \n   -0.9808293     0.5108256     0.5108256 \n\n\nDefine Likelihood and Loglikelihood functionLikelihood function : \\(\\prod_{i=1}^n \\frac{e^{-\\lambda} + \\lambda^{y_i}} {y_i!}\\)Loglikelihood function : \\(\\sum_{i=1}^n -\\lambda + y_i ln \\lambda - ln y_i!\\)\nCalculate gradient of Loglikelihood function with repect to \\(\\beta\\)\\(\\lambda=e^{x_i^{T}\\beta}\\)\\(\\displaystyle \\frac{\\partial -l(\\beta)}{\\partial \\beta} = \\sum_{i=1}^n (y_i - e^{x_i^{T}\\beta}) x_i^{T}\\)\n\n\ncode\n\nset.seed(2022)\nbeta <- rnorm(3)\nsigma_f<-function(x){exp(x)}\nnll <- function(beta){\n  y<- example_data$Claims\n  pi_vec <- sigma_f(matrix(cbind(1, example_data$Gender,\n                                    example_data$Territory),ncol = 3) %*% matrix(beta, nrow = 3))\n  -sum((-pi_vec)+y*log(pi_vec)-log(factorial(y)))\n}\n\n\n\n\n\ncode\n\ngrad_nll <- function(beta){\ny<- example_data$Claims\nxbeta <- matrix(cbind(1,example_data$Gender,example_data$Territory),ncol=3) %*% beta\npi_vec <- sigma_f(xbeta)\n-colSums(as.vector(y-pi_vec)*matrix(cbind(1,example_data$Gender,example_data$Territory),ncol=3))\n}\ngrad_nll(beta)\n\n\n[1]  5.601110 -1.786728 -1.061560\n\n\n\ncode\n\nset.seed(2022)\nbeta<-rnorm(3)\niter_n <-1\nimprove <-1\nconv_threshold <- 1e-15\nmax_n <- 100000\nresult <- matrix(0, nrow = max_n, ncol = 3)\nwhile ((improve > conv_threshold) & (iter_n<= max_n)) {\n  beta_new <- beta - 0.001 * grad_nll(beta)\n  improve <- abs(nll(beta)-nll(beta_new))\n  result[iter_n,]<- beta_new\n  beta <- beta_new\n  iter_n <- iter_n +1\n}\nresult[iter_n-1,]\n\n\n[1] -0.9808283  0.5108249  0.5108249\n\ncode\n\nmodel$coefficients\n\n\n  (Intercept)       GenderM TerritoryWest \n   -0.9808293     0.5108256     0.5108256 \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-31T12:00:49+00:00",
    "input_file": {}
  },
  {
    "path": "posts/3. Ridge & Lasso /",
    "title": "Rigde-Lasso as to changing penalty values",
    "description": "Penaltyì— ë”°ë¼ Lasso Ridge ê³„ìˆ˜ê°€ ì–´ë–»ê²Œ ë°”ë€ŒëŠ”ì§€ ì‚´í´ë³´ì.",
    "author": [
      {
        "name": "WooramSeong",
        "url": {}
      }
    ],
    "date": "2021-04-16",
    "categories": [],
    "contents": "\n\nContents\nì¤€ë¹„ì‘ì—…\ní•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬\níŒŒì¼ê²½ë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n\nData overview (ë°ì´í„° ê¸°ë³¸ì •ë³´)\nê¸°ë³¸ ì •ë³´\nRecipe Code\nValidation Set\nTune Spec ì„¤ì •\n\ncoefficient on Best Penalty in Lassoë¥¼ êµ¬í•˜ì.\nëª¨ë“  ëŒë‹¤(Penalty) ì— ë”°ë¥¸ ê³„ìˆ˜ê°’ì„ êµ¬í•´ë³´ì.\në°ì´í„° ì •ë¦¬\n\nì¤€ë¹„ì‘ì—…\ní•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬\n\n\ncode\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(skimr)\nlibrary(knitr)\nlibrary(naniar)\nlibrary(car)\nlibrary(glmnet)\nlibrary(doParallel)\ntheme_set(theme_bw())\n\n\n\níŒŒì¼ê²½ë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n\n\ncode\n\nfile_path <-  \"/cloud/project/input\"\nfiles <- list.files(file_path)\nfiles\n\n\n[1] \"test.csv\"  \"train.csv\"\n\ncode\n\ntrain <- read_csv(file.path(file_path, \"train.csv\"))\ntest <- read_csv(file.path(file_path, \"test.csv\"))\n\n\n\nData overview (ë°ì´í„° ê¸°ë³¸ì •ë³´)\nê¸°ë³¸ ì •ë³´\nTrain - ì´ 1460ê°œì˜ í–‰, 81ê°œì˜ ì—´ë¡œ ì´ë£¨ì–´ì ¸ìˆê³ ,\nTest - ëª©í‘œ ë³€ìˆ˜ì¸ SalePriceë¥¼ ì œì™¸í•œ 80ê°œì˜ ì—´ë¡œ ì´ë£¨ì–´ì ¸ìˆë‹¤. í–‰ì€ 1459ê°œì´ë‹¤.\nTrainì˜ ì •ë³´ë¥¼ í†µí•´ íšŒê·€ë¶„ì„ì„ ì‹¤í–‰í•˜ê³ , í•´ë‹¹ íšŒê·€ë¶„ì„ ì‹ì´ Testì˜ ì •ë³´ì— ì–¼ë§ˆë‚˜ ì˜ ë“¤ì–´ë§ëŠ”ì§€ ì˜ˆì¸¡í•´ë³´ì.\nRecipe Code\n\n\ncode\n\nall_data <- bind_rows(train, test) %>% \n  janitor::clean_names()\nnames(all_data)[1:81]\n\n\n [1] \"id\"              \"ms_sub_class\"    \"ms_zoning\"      \n [4] \"lot_frontage\"    \"lot_area\"        \"street\"         \n [7] \"alley\"           \"lot_shape\"       \"land_contour\"   \n[10] \"utilities\"       \"lot_config\"      \"land_slope\"     \n[13] \"neighborhood\"    \"condition1\"      \"condition2\"     \n[16] \"bldg_type\"       \"house_style\"     \"overall_qual\"   \n[19] \"overall_cond\"    \"year_built\"      \"year_remod_add\" \n[22] \"roof_style\"      \"roof_matl\"       \"exterior1st\"    \n[25] \"exterior2nd\"     \"mas_vnr_type\"    \"mas_vnr_area\"   \n[28] \"exter_qual\"      \"exter_cond\"      \"foundation\"     \n[31] \"bsmt_qual\"       \"bsmt_cond\"       \"bsmt_exposure\"  \n[34] \"bsmt_fin_type1\"  \"bsmt_fin_sf1\"    \"bsmt_fin_type2\" \n[37] \"bsmt_fin_sf2\"    \"bsmt_unf_sf\"     \"total_bsmt_sf\"  \n[40] \"heating\"         \"heating_qc\"      \"central_air\"    \n[43] \"electrical\"      \"x1st_flr_sf\"     \"x2nd_flr_sf\"    \n[46] \"low_qual_fin_sf\" \"gr_liv_area\"     \"bsmt_full_bath\" \n[49] \"bsmt_half_bath\"  \"full_bath\"       \"half_bath\"      \n[52] \"bedroom_abv_gr\"  \"kitchen_abv_gr\"  \"kitchen_qual\"   \n[55] \"tot_rms_abv_grd\" \"functional\"      \"fireplaces\"     \n[58] \"fireplace_qu\"    \"garage_type\"     \"garage_yr_blt\"  \n[61] \"garage_finish\"   \"garage_cars\"     \"garage_area\"    \n[64] \"garage_qual\"     \"garage_cond\"     \"paved_drive\"    \n[67] \"wood_deck_sf\"    \"open_porch_sf\"   \"enclosed_porch\" \n[70] \"x3ssn_porch\"     \"screen_porch\"    \"pool_area\"      \n[73] \"pool_qc\"         \"fence\"           \"misc_feature\"   \n[76] \"misc_val\"        \"mo_sold\"         \"yr_sold\"        \n[79] \"sale_type\"       \"sale_condition\"  \"sale_price\"     \n\ncode\n\nhousing_recipe <- all_data %>% \n  recipe(sale_price ~ .) %>%\n  step_rm(id) %>% \n  step_log(sale_price) %>% \n  step_modeimpute(all_nominal()) %>% \n  step_dummy(all_nominal()) %>% \n  step_meanimpute(all_predictors()) %>%\n  step_normalize(all_predictors()) %>% \n  ##step_nzv(all_predictors()) %>% \n  prep(training = all_data)\n\nprint(housing_recipe)\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         80\n\nTraining data contained 2919 data points and 2919 incomplete rows. \n\nOperations:\n\nVariables removed id [trained]\nLog transformation on sale_price [trained]\nMode Imputation for ms_zoning, street, alley, ... [trained]\nDummy variables from ms_zoning, street, alley, ... [trained]\nMean Imputation for ms_sub_class, lot_frontage, ... [trained]\nCentering and scaling for ms_sub_class, lot_frontage, ... [trained]\n\ncode\n\nall_data2 <- juice(housing_recipe)\ntrain_index <- seq_len(nrow(train))\n\ntrain2 <- all_data2[train_index,]\ntest2 <- all_data2[-train_index,]\ntrain2 %>% \n  head() %>% \n  kable()\n\n\nms_sub_class\nlot_frontage\nlot_area\noverall_qual\noverall_cond\nyear_built\nyear_remod_add\nmas_vnr_area\nbsmt_fin_sf1\nbsmt_fin_sf2\nbsmt_unf_sf\ntotal_bsmt_sf\nx1st_flr_sf\nx2nd_flr_sf\nlow_qual_fin_sf\ngr_liv_area\nbsmt_full_bath\nbsmt_half_bath\nfull_bath\nhalf_bath\nbedroom_abv_gr\nkitchen_abv_gr\ntot_rms_abv_grd\nfireplaces\ngarage_yr_blt\ngarage_cars\ngarage_area\nwood_deck_sf\nopen_porch_sf\nenclosed_porch\nx3ssn_porch\nscreen_porch\npool_area\nmisc_val\nmo_sold\nyr_sold\nsale_price\nms_zoning_FV\nms_zoning_RH\nms_zoning_RL\nms_zoning_RM\nstreet_Pave\nalley_Pave\nlot_shape_IR2\nlot_shape_IR3\nlot_shape_Reg\nland_contour_HLS\nland_contour_Low\nland_contour_Lvl\nutilities_NoSeWa\nlot_config_CulDSac\nlot_config_FR2\nlot_config_FR3\nlot_config_Inside\nland_slope_Mod\nland_slope_Sev\nneighborhood_Blueste\nneighborhood_BrDale\nneighborhood_BrkSide\nneighborhood_ClearCr\nneighborhood_CollgCr\nneighborhood_Crawfor\nneighborhood_Edwards\nneighborhood_Gilbert\nneighborhood_IDOTRR\nneighborhood_MeadowV\nneighborhood_Mitchel\nneighborhood_NAmes\nneighborhood_NoRidge\nneighborhood_NPkVill\nneighborhood_NridgHt\nneighborhood_NWAmes\nneighborhood_OldTown\nneighborhood_Sawyer\nneighborhood_SawyerW\nneighborhood_Somerst\nneighborhood_StoneBr\nneighborhood_SWISU\nneighborhood_Timber\nneighborhood_Veenker\ncondition1_Feedr\ncondition1_Norm\ncondition1_PosA\ncondition1_PosN\ncondition1_RRAe\ncondition1_RRAn\ncondition1_RRNe\ncondition1_RRNn\ncondition2_Feedr\ncondition2_Norm\ncondition2_PosA\ncondition2_PosN\ncondition2_RRAe\ncondition2_RRAn\ncondition2_RRNn\nbldg_type_X2fmCon\nbldg_type_Duplex\nbldg_type_Twnhs\nbldg_type_TwnhsE\nhouse_style_X1.5Unf\nhouse_style_X1Story\nhouse_style_X2.5Fin\nhouse_style_X2.5Unf\nhouse_style_X2Story\nhouse_style_SFoyer\nhouse_style_SLvl\nroof_style_Gable\nroof_style_Gambrel\nroof_style_Hip\nroof_style_Mansard\nroof_style_Shed\nroof_matl_CompShg\nroof_matl_Membran\nroof_matl_Metal\nroof_matl_Roll\nroof_matl_Tar.Grv\nroof_matl_WdShake\nroof_matl_WdShngl\nexterior1st_AsphShn\nexterior1st_BrkComm\nexterior1st_BrkFace\nexterior1st_CBlock\nexterior1st_CemntBd\nexterior1st_HdBoard\nexterior1st_ImStucc\nexterior1st_MetalSd\nexterior1st_Plywood\nexterior1st_Stone\nexterior1st_Stucco\nexterior1st_VinylSd\nexterior1st_Wd.Sdng\nexterior1st_WdShing\nexterior2nd_AsphShn\nexterior2nd_Brk.Cmn\nexterior2nd_BrkFace\nexterior2nd_CBlock\nexterior2nd_CmentBd\nexterior2nd_HdBoard\nexterior2nd_ImStucc\nexterior2nd_MetalSd\nexterior2nd_Other\nexterior2nd_Plywood\nexterior2nd_Stone\nexterior2nd_Stucco\nexterior2nd_VinylSd\nexterior2nd_Wd.Sdng\nexterior2nd_Wd.Shng\nmas_vnr_type_BrkFace\nmas_vnr_type_None\nmas_vnr_type_Stone\nexter_qual_Fa\nexter_qual_Gd\nexter_qual_TA\nexter_cond_Fa\nexter_cond_Gd\nexter_cond_Po\nexter_cond_TA\nfoundation_CBlock\nfoundation_PConc\nfoundation_Slab\nfoundation_Stone\nfoundation_Wood\nbsmt_qual_Fa\nbsmt_qual_Gd\nbsmt_qual_TA\nbsmt_cond_Gd\nbsmt_cond_Po\nbsmt_cond_TA\nbsmt_exposure_Gd\nbsmt_exposure_Mn\nbsmt_exposure_No\nbsmt_fin_type1_BLQ\nbsmt_fin_type1_GLQ\nbsmt_fin_type1_LwQ\nbsmt_fin_type1_Rec\nbsmt_fin_type1_Unf\nbsmt_fin_type2_BLQ\nbsmt_fin_type2_GLQ\nbsmt_fin_type2_LwQ\nbsmt_fin_type2_Rec\nbsmt_fin_type2_Unf\nheating_GasA\nheating_GasW\nheating_Grav\nheating_OthW\nheating_Wall\nheating_qc_Fa\nheating_qc_Gd\nheating_qc_Po\nheating_qc_TA\ncentral_air_Y\nelectrical_FuseF\nelectrical_FuseP\nelectrical_Mix\nelectrical_SBrkr\nkitchen_qual_Fa\nkitchen_qual_Gd\nkitchen_qual_TA\nfunctional_Maj2\nfunctional_Min1\nfunctional_Min2\nfunctional_Mod\nfunctional_Sev\nfunctional_Typ\nfireplace_qu_Fa\nfireplace_qu_Gd\nfireplace_qu_Po\nfireplace_qu_TA\ngarage_type_Attchd\ngarage_type_Basment\ngarage_type_BuiltIn\ngarage_type_CarPort\ngarage_type_Detchd\ngarage_finish_RFn\ngarage_finish_Unf\ngarage_qual_Fa\ngarage_qual_Gd\ngarage_qual_Po\ngarage_qual_TA\ngarage_cond_Fa\ngarage_cond_Gd\ngarage_cond_Po\ngarage_cond_TA\npaved_drive_P\npaved_drive_Y\npool_qc_Fa\npool_qc_Gd\nfence_GdWo\nfence_MnPrv\nfence_MnWw\nmisc_feature_Othr\nmisc_feature_Shed\nmisc_feature_TenC\nsale_type_Con\nsale_type_ConLD\nsale_type_ConLI\nsale_type_ConLw\nsale_type_CWD\nsale_type_New\nsale_type_Oth\nsale_type_WD\nsale_condition_AdjLand\nsale_condition_Alloca\nsale_condition_Family\nsale_condition_Normal\nsale_condition_Partial\n0.0673199\n-0.2020329\n-0.2178414\n0.6460727\n-0.5071973\n1.0460784\n0.8966793\n0.5251119\n0.5808073\n-0.2930798\n-0.9347024\n-0.4442517\n-0.7737285\n1.2071717\n-0.1011797\n0.4134764\n1.0868363\n-0.2498524\n0.781232\n1.2323877\n0.169898\n-0.2076629\n0.9866803\n-0.9241529\n1.0007573\n0.3064753\n0.3488399\n-0.7406335\n0.1999717\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n-1.5519176\n0.1576185\n12.24769\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n0.7549859\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n0.606934\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n3.1510604\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n1.5231625\n-1.2373891\n-0.3053301\n-0.1101443\n1.4074569\n-1.2662447\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n1.1096078\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n-0.2985775\n0.6852938\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n0.5905687\n-0.1265135\n-0.5042992\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n-0.8734664\n0.5017845\n-0.0720317\n-0.0631737\n2.1879039\n0.1547375\n-0.3955364\n-0.5721522\n1.1779104\n-0.2930798\n-0.6297885\n0.4770294\n0.2610301\n-0.7848906\n-0.1011797\n-0.4718098\n-0.8195386\n3.8217640\n0.781232\n-0.7561915\n0.169898\n-0.2076629\n-0.2877090\n0.6235248\n-0.0849858\n0.3064753\n-0.0597822\n1.6146027\n-0.7027224\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n-0.4468483\n-0.6028583\n12.10901\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n0.7549859\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n5.773193\n-0.0694091\n-1.647061\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n10.9810599\n4.0979294\n-2.4803837\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n0.9919814\n-0.0524143\n-0.0910347\n-0.6525667\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n2.3419622\n-0.2861546\n-0.0261802\n-0.1222546\n-0.7360782\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n2.3512352\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n-0.7300038\n-0.3932108\n-0.1689125\n-0.6563038\n0.8078764\n-0.3053301\n-0.1101443\n-0.7102579\n0.7894662\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n1.1675169\n-0.9009106\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n3.093995\n-0.2985775\n-1.4587283\n-0.3185509\n-0.6403159\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n-0.8067187\n0.977137\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n-1.6927029\n-0.1265135\n1.9822706\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n0.0673199\n-0.0612694\n0.1371734\n0.6460727\n-0.5071973\n0.9800531\n0.8488195\n0.3347702\n0.0978563\n-0.2930798\n-0.2884670\n-0.2990251\n-0.6106138\n1.2351632\n-0.1011797\n0.5636589\n1.0868363\n-0.2498524\n0.781232\n1.2323877\n0.169898\n-0.2076629\n-0.2877090\n0.6235248\n0.9203319\n0.3064753\n0.6274459\n-0.7406335\n-0.0811953\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n1.0265775\n0.1576185\n12.31717\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n0.606934\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n3.1510604\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n1.5231625\n-1.2373891\n-0.3053301\n-0.1101443\n1.4074569\n-1.2662447\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n1.1096078\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n3.3480662\n-1.4587283\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n-1.6927029\n-0.1265135\n1.9822706\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n0.3025164\n-0.4366387\n-0.0783713\n0.6460727\n-0.5071973\n-1.8590326\n-0.6826955\n-0.5721522\n-0.4948563\n-0.2930798\n-0.0472664\n-0.6711682\n-0.5061185\n0.9785744\n-0.1011797\n0.4273090\n1.0868363\n-0.2498524\n-1.027187\n-0.7561915\n0.169898\n-0.2076629\n0.3494857\n0.6235248\n0.7996938\n1.6196836\n0.7853226\n-0.7406335\n-0.1847831\n3.8743031\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n-1.5519176\n-1.3633351\n11.84940\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n-1.647061\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n5.2278523\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n-0.7360782\n2.4698379\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n-0.7300038\n-0.3932108\n5.9181952\n-0.6563038\n0.8078764\n-0.3053301\n-0.1101443\n-0.7102579\n0.7894662\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n-0.9009106\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n-0.8406993\n1.0675387\n4.7873140\n-0.0414158\n-3.4106271\n-0.323096\n-0.2985775\n0.6852938\n-0.3185509\n-0.6403159\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n2.2707842\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n0.5905687\n-0.1265135\n-0.5042992\n-1.3449209\n-0.1117261\n-0.2608328\n-0.0718576\n1.6571574\n-0.6201557\n1.0493496\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n-2.1550970\n-0.3026411\n0.0673199\n0.6894691\n0.5188142\n1.3553191\n-0.5071973\n0.9470405\n0.7530998\n1.3872480\n0.4688505\n-0.2930798\n-0.1610403\n0.2115370\n-0.0371639\n1.6713642\n-0.1011797\n1.3778060\n1.0868363\n-0.2498524\n0.781232\n1.2323877\n1.385418\n-0.2076629\n1.6238750\n0.6235248\n0.8801192\n1.6196836\n1.6861486\n0.7768341\n0.5403318\n-0.3595391\n-0.1033128\n-0.2858865\n-0.0631394\n-0.0895766\n2.1316468\n0.1576185\n12.42922\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n5.773193\n-0.0694091\n-1.647061\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n-0.2015634\n-0.4229141\n6.3323719\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n1.5318854\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n1.5231625\n-1.2373891\n-0.3053301\n-0.1101443\n1.4074569\n-1.2662447\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n1.1096078\n-0.130642\n-0.0614929\n-0.0414158\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n-0.2985775\n-1.4587283\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n1.2391648\n-1.023047\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n-1.6927029\n-0.1265135\n1.9822706\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n1.6119459\n-0.9526448\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n-0.1678767\n0.7363903\n0.5004295\n-0.7724201\n-0.5071973\n0.7159521\n0.5138006\n-0.5721522\n0.6378834\n-0.2930798\n-1.1303934\n-0.5804016\n-0.9266484\n0.5353755\n-0.1011797\n-0.2742013\n1.0868363\n-0.2498524\n-1.027187\n1.2323877\n-2.261142\n-0.2076629\n-0.9249036\n-0.9241529\n0.5986302\n0.3064753\n0.0330864\n-0.4244944\n-0.2587744\n-0.3595391\n12.6010642\n-0.2858865\n-0.0631394\n1.1441161\n1.3949339\n0.9180953\n11.87060\n-0.2235685\n-0.0947847\n0.5351371\n-0.4324394\n0.0642383\n-0.1656675\n-0.1634722\n-0.074227\n-1.3240743\n-0.2070212\n-0.1448419\n0.3365019\n-0.018509\n-0.2532614\n-0.173155\n-0.0694091\n0.606934\n-0.2114791\n-0.074227\n-0.0586211\n-0.1018855\n-0.1959779\n-0.1236896\n-0.3172448\n-0.1912176\n-0.2667738\n-0.2447291\n-0.1813765\n-0.1132868\n4.9595195\n-0.4229141\n-0.1578646\n-0.0891026\n-0.2455142\n-0.2167279\n-0.2985775\n-0.2335237\n-0.2114791\n-0.2578243\n-0.1333279\n-0.1292795\n-0.1590004\n-0.0910347\n-0.2439421\n0.4030253\n-0.0830456\n-0.1163487\n-0.0983967\n-0.1319913\n-0.0453765\n-0.0556033\n-0.0668728\n0.1018855\n-0.037037\n-0.037037\n-0.018509\n-0.018509\n-0.0261802\n-0.1472876\n-0.1969181\n-0.1843766\n-0.2903361\n-0.0809289\n-1.0077380\n-0.0524143\n-0.0910347\n-0.6525667\n-0.1710455\n-0.2141168\n0.5133674\n-0.087129\n-0.4822925\n-0.0614929\n-0.0414158\n0.1222546\n-0.018509\n-0.018509\n-0.018509\n-0.0891026\n-0.0556033\n-0.0490206\n-0.0261802\n-0.0453765\n-0.1752422\n-0.0261802\n-0.2123613\n-0.4223512\n-0.018509\n-0.4268461\n-0.2861546\n-0.0261802\n-0.1222546\n1.3580858\n-0.4047462\n-0.1398328\n-0.037037\n-0.087129\n-0.1279035\n-0.0320695\n-0.2123613\n-0.4018763\n-0.0718576\n-0.4251627\n-0.018509\n-0.3192027\n-0.0453765\n-0.1279035\n1.3693865\n-0.3932108\n-0.1689125\n-0.6563038\n0.8078764\n-0.3053301\n-0.1101443\n-0.7102579\n0.7894662\n-0.1532457\n-0.3377618\n-0.0320695\n0.3873845\n-0.8562253\n-0.9009106\n-0.130642\n-0.0614929\n24.1371155\n-0.1762775\n1.1890784\n-0.9364133\n-0.2088138\n-0.0414158\n0.2931008\n-0.323096\n-0.2985775\n0.6852938\n-0.3185509\n1.5611942\n-0.23596\n-0.3307969\n-0.683675\n-0.1544121\n-0.1085406\n-0.1752422\n-0.1931338\n0.3666433\n0.125109\n-0.0966069\n-0.0556033\n-0.0261802\n-0.0453765\n-0.1803668\n-0.4402256\n-0.0320695\n-0.6445724\n0.2682439\n-0.1319913\n-0.0524143\n-0.018509\n0.3039876\n-0.1567214\n-0.8067187\n0.977137\n-0.0556033\n-0.1508882\n-0.1567214\n-0.1101443\n-0.0261802\n0.2711665\n-0.1612502\n0.5905687\n-0.1265135\n-0.5042992\n0.7432834\n-0.1117261\n-0.2608328\n-0.0718576\n-0.6032363\n-0.6201557\n1.0493496\n-0.2105938\n-0.0910347\n-0.0414158\n0.2375732\n-0.1612502\n-0.0718576\n-0.0694091\n0.1940858\n-0.1472876\n0.3243873\n-0.0261802\n0.0453765\n-0.1997162\n0.3006139\n-0.0642383\n-0.037037\n0.0586211\n-0.018509\n-0.0414158\n-0.0947847\n-0.0556033\n-0.0524143\n-0.0642383\n-0.2985775\n-0.0490206\n0.3943712\n-0.0642383\n-0.0910347\n-0.1265135\n0.4638573\n-0.3026411\n\nValidation Set\n\n\ncode\n\nset.seed(2021)\n\nvalidation_set<- validation_split(train2, prop = 0.3)\n#0.3ëŠ” ë¶„ì„ì„ ìœ„í•´ ë‚¨ì•„ ìˆì„ í‘œë³¸ì˜ ë¹„ìœ¨ \n\nvalidation_set$splits[[1]]$in_id\n\n\n  [1]    1    4    5    6    9   14   19   21   22   25   26   30   31\n [14]   32   35   37   38   39   40   45   50   51   56   59   65   72\n [27]   78   82   85   86   94  102  110  111  117  125  126  130  131\n [40]  132  133  135  138  140  141  142  149  153  158  159  161  162\n [53]  180  182  186  189  190  193  197  201  202  204  207  208  212\n [66]  214  216  219  220  221  222  228  231  234  236  238  242  246\n [79]  248  249  256  257  261  265  266  267  268  271  272  273  274\n [92]  277  280  286  288  290  291  292  296  297  298  301  310  312\n[105]  315  318  319  322  332  333  334  338  353  354  355  358  359\n[118]  368  370  375  378  380  381  382  383  384  394  397  398  399\n[131]  411  412  415  418  421  429  433  437  439  440  441  448  452\n[144]  462  467  469  473  474  476  477  479  480  487  490  494  496\n[157]  498  499  500  502  505  507  508  512  514  518  521  525  526\n[170]  530  532  533  534  536  537  542  545  546  548  552  553  554\n[183]  556  557  560  566  567  571  576  579  584  585  586  590  594\n[196]  595  598  604  610  612  617  619  623  629  635  636  643  646\n[209]  647  650  662  675  676  677  685  688  692  693  696  700  711\n[222]  715  717  718  723  725  727  731  735  736  740  741  744  750\n[235]  756  759  760  761  767  774  777  779  780  783  787  794  799\n[248]  802  804  818  820  823  825  829  830  832  834  835  839  840\n[261]  848  849  854  856  857  858  859  860  868  869  871  874  876\n[274]  877  880  885  889  890  892  895  898  901  912  921  923  929\n[287]  930  938  942  948  951  957  958  960  964  965  966  969  975\n[300]  976  987  988  995  996  999 1001 1008 1010 1016 1018 1026 1030\n[313] 1031 1033 1038 1041 1043 1048 1053 1060 1061 1064 1067 1069 1070\n[326] 1072 1073 1077 1080 1081 1086 1092 1096 1098 1099 1100 1102 1104\n[339] 1105 1107 1108 1110 1117 1121 1123 1128 1130 1152 1155 1156 1159\n[352] 1163 1167 1178 1184 1187 1191 1192 1193 1194 1195 1198 1210 1211\n[365] 1217 1220 1225 1231 1232 1234 1241 1243 1245 1247 1249 1254 1268\n[378] 1270 1272 1275 1278 1285 1288 1293 1295 1305 1313 1321 1322 1325\n[391] 1333 1335 1337 1339 1340 1346 1348 1351 1354 1355 1358 1360 1362\n[404] 1365 1368 1371 1373 1374 1377 1382 1383 1388 1389 1394 1398 1400\n[417] 1403 1405 1407 1408 1409 1412 1416 1421 1422 1426 1428 1430 1435\n[430] 1438 1439 1441 1443 1446 1447 1448 1452 1456 1457\n\ncode\n\n#439ê°œë¡œ í™•ì¸ë¨ \n\n439/nrow(train)\n\n\n[1] 0.3006849\n\ncode\n\n#0.3 í™•ì¸\n\n\n\nTune Spec ì„¤ì •\n\n\ncode\n\ntune_spec <- linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")\n\nlambda_grid <- grid_regular(penalty(), levels = 100)\n\nworkflow <- workflow() %>%\n  add_model(tune_spec) %>% \n  add_formula(sale_price ~ .)\n\ndoParallel::registerDoParallel()\n\ntune_result <- workflow %>% \n  tune_grid(validation_set,\n            grid = lambda_grid,\n            metrics = metric_set(rmse))\n\n\n\ncoefficient on Best Penalty in Lassoë¥¼ êµ¬í•˜ì.\nValidation set ì„ ì„¤ì •í•˜ì—¬ Root Mean Square (RMSE)ê°€ ê°€ì¥ ë‚®ì„ ë•Œì˜ ëŒë‹¤ê°’ì„ ì‚°ì¶œí•œ í›„ ê·¸ì— ë”°ë¥¸ coefficient ê°’ì„ êµ¬í•˜ì˜€ë‹¤. Train data ì¤‘ì—ì„œ 30% ë§Œ Validation setìœ¼ë¡œ ë‚¨ê²¨ë‘ì—ˆë‹¤.\n\nCoefficient ê°’ì„ ì‚°ì¶œí•˜ëŠ” ìì²´ í•¨ìˆ˜ë¥¼ ë§Œë“  í›„ for ë°˜ë³µë¬¸ì„ ì ìš©ì‹œì¼°ë‹¤. (ëŒë‹¤ì˜ í‘œë³¸ì€ 100 ê°œë¡œ ì„¤ì •í•˜ì˜€ë‹¤)\n\n\ncode\n\ntune_result %>% \n  collect_metrics()\n\n\n# A tibble: 100 x 7\n    penalty .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1 1.00e-10 rmse    standard   0.201     1      NA Preprocessor1_Modeâ€¦\n 2 1.26e-10 rmse    standard   0.201     1      NA Preprocessor1_Modeâ€¦\n 3 1.59e-10 rmse    standard   0.201     1      NA Preprocessor1_Modeâ€¦\n 4 2.01e-10 rmse    standard   0.201     1      NA Preprocessor1_Modeâ€¦\n 5 2.54e-10 rmse    standard   0.201     1      NA Preprocessor1_Modeâ€¦\n 6 3.20e-10 rmse    standard   0.201     1      NA Preprocessor1_Modeâ€¦\n 7 4.04e-10 rmse    standard   0.201     1      NA Preprocessor1_Modeâ€¦\n 8 5.09e-10 rmse    standard   0.201     1      NA Preprocessor1_Modeâ€¦\n 9 6.43e-10 rmse    standard   0.201     1      NA Preprocessor1_Modeâ€¦\n10 8.11e-10 rmse    standard   0.201     1      NA Preprocessor1_Modeâ€¦\n# â€¦ with 90 more rows\n\ncode\n\ntune_result %>% show_best()\n\n\n# A tibble: 5 x 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1  0.0192 rmse    standard   0.166     1      NA Preprocessor1_Model0â€¦\n2  0.0242 rmse    standard   0.166     1      NA Preprocessor1_Model0â€¦\n3  0.0152 rmse    standard   0.167     1      NA Preprocessor1_Model0â€¦\n4  0.0305 rmse    standard   0.168     1      NA Preprocessor1_Model0â€¦\n5  0.0120 rmse    standard   0.168     1      NA Preprocessor1_Model0â€¦\n\ncode\n\ntune_best <- tune_result %>% select_best(metric = \"rmse\")\ntune_best$penalty\n\n\n[1] 0.0191791\n\ncode\n\nlasso_model <- \n  linear_reg(penalty = tune_best$penalty, # tuned penalty\n             mixture = 1) %>% # lasso: 1, ridge: 0\n  set_engine(\"glmnet\")\n\nlasso_fit <- \n  lasso_model %>% \n  fit(sale_price ~ ., data = train2)\n\nresult_lasso <- lasso_fit %>% \n  tidy() %>% filter (estimate != 0)%>% select(term,estimate)\n\nresult_lasso\n\n\n# A tibble: 35 x 2\n   term           estimate\n   <chr>             <dbl>\n 1 (Intercept)    12.0    \n 2 ms_sub_class   -0.00193\n 3 lot_area        0.00571\n 4 overall_qual    0.130  \n 5 overall_cond    0.0173 \n 6 year_built      0.0310 \n 7 year_remod_add  0.0281 \n 8 bsmt_fin_sf1    0.00122\n 9 total_bsmt_sf   0.0259 \n10 x1st_flr_sf     0.00375\n# â€¦ with 25 more rows\n\ncode\n\nresult_lasso2 <- lasso_fit %>% \n  tidy() %>% select(term,estimate)\n\nresult_lasso2\n\n\n# A tibble: 246 x 2\n   term           estimate\n   <chr>             <dbl>\n 1 (Intercept)    12.0    \n 2 ms_sub_class   -0.00193\n 3 lot_frontage    0      \n 4 lot_area        0.00571\n 5 overall_qual    0.130  \n 6 overall_cond    0.0173 \n 7 year_built      0.0310 \n 8 year_remod_add  0.0281 \n 9 mas_vnr_area    0      \n10 bsmt_fin_sf1    0.00122\n# â€¦ with 236 more rows\n\n\n\ncode\n\n## coefficient on Best Penalty in Ridgeë¥¼ êµ¬í•˜ì.\n\n\ntune_spec2 <- linear_reg(penalty = tune(), mixture = 0) %>%\n  set_engine(\"glmnet\")\n\nworkflow2 <- workflow() %>%\n  add_model(tune_spec2) %>% \n  add_formula(sale_price ~ .)\n\ndoParallel::registerDoParallel()\n\ntune_result2 <- workflow2 %>% \n  tune_grid(validation_set,\n            grid = lambda_grid,\n            metrics = metric_set(rmse))\n\n\n\n\ntune_result2 %>% \n  collect_metrics()\n\n\n# A tibble: 100 x 7\n    penalty .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1 1.00e-10 rmse    standard   0.169     1      NA Preprocessor1_Modeâ€¦\n 2 1.26e-10 rmse    standard   0.169     1      NA Preprocessor1_Modeâ€¦\n 3 1.59e-10 rmse    standard   0.169     1      NA Preprocessor1_Modeâ€¦\n 4 2.01e-10 rmse    standard   0.169     1      NA Preprocessor1_Modeâ€¦\n 5 2.54e-10 rmse    standard   0.169     1      NA Preprocessor1_Modeâ€¦\n 6 3.20e-10 rmse    standard   0.169     1      NA Preprocessor1_Modeâ€¦\n 7 4.04e-10 rmse    standard   0.169     1      NA Preprocessor1_Modeâ€¦\n 8 5.09e-10 rmse    standard   0.169     1      NA Preprocessor1_Modeâ€¦\n 9 6.43e-10 rmse    standard   0.169     1      NA Preprocessor1_Modeâ€¦\n10 8.11e-10 rmse    standard   0.169     1      NA Preprocessor1_Modeâ€¦\n# â€¦ with 90 more rows\n\ncode\n\ntune_result2 %>% show_best()\n\n\n# A tibble: 5 x 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1   0.394 rmse    standard   0.153     1      NA Preprocessor1_Model0â€¦\n2   0.313 rmse    standard   0.153     1      NA Preprocessor1_Model0â€¦\n3   0.248 rmse    standard   0.154     1      NA Preprocessor1_Model0â€¦\n4   0.498 rmse    standard   0.154     1      NA Preprocessor1_Model0â€¦\n5   0.196 rmse    standard   0.155     1      NA Preprocessor1_Model0â€¦\n\ncode\n\ntune_best2 <- tune_result2 %>% select_best(metric = \"rmse\")\ntune_best2$penalty\n\n\n[1] 0.3944206\n\ncode\n\nRidge_model <- \n  linear_reg(penalty = tune_best2$penalty, # tuned penalty\n             mixture = 0) %>% # lasso: 1, ridge: 0\n  set_engine(\"glmnet\")\n\nRidge_fit <- \n  Ridge_model %>% \n  fit(sale_price ~ ., data = train2)\n\nresult_Ridge <- Ridge_fit %>% \n  tidy() %>% filter (estimate != 0)%>% select(term,estimate)\n\nresult_Ridge\n\n\n# A tibble: 246 x 2\n   term           estimate\n   <chr>             <dbl>\n 1 (Intercept)    12.0    \n 2 ms_sub_class   -0.00623\n 3 lot_frontage    0.00590\n 4 lot_area        0.00871\n 5 overall_qual    0.0372 \n 6 overall_cond    0.0173 \n 7 year_built      0.0110 \n 8 year_remod_add  0.0161 \n 9 mas_vnr_area    0.00962\n10 bsmt_fin_sf1    0.0119 \n# â€¦ with 236 more rows\n\nëª¨ë“  ëŒë‹¤(Penalty) ì— ë”°ë¥¸ ê³„ìˆ˜ê°’ì„ êµ¬í•´ë³´ì.\n\n\ncode\n\ncoefficient_lambda<- function(x,y) {\n  Ridge_model <- \n    linear_reg(penalty = x, # tuned penalty\n               mixture = y) %>% # lasso: 1, ridge: 0\n    set_engine(\"glmnet\")\n  \n  Ridge_fit <- \n    Ridge_model %>% \n    fit(sale_price ~ ., data = train2)\n  \n  result_Ridge <- Ridge_fit %>% \n    tidy() %>% select(estimate)\n  \n  result_Ridge\n  \n}\n\n\n\në°ì´í„° ì •ë¦¬\në°ì´í„° ì •ë¦¬í•˜ëŠ” ê³¼ì •ì´ ê°€ì¥ í˜ë“¤ì—ˆëŠ”ë° ê´€ë ¨ ì½”ë“œë¥¼ ì²¨ë¶€í•˜ë„ë¡ í•˜ê² ë‹¤. ë‹¤ìŒì€ ë°ì´í„°ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ë‚´ê°€ ì •ì˜í•œ í•¨ìˆ˜ì´ë‹¤.\n\n\ncode\n\nlambda_grid$penalty[85]\n\n\n[1] 0.03053856\n\ncode\n\nRidge<-train2[seq_len(nrow(lambda_grid)),]%>%select(-sale_price)\nLasso<-train2[seq_len(nrow(lambda_grid)),]%>%select(-sale_price)\n\nIntercept<-c(0)\nRidge<-cbind(Intercept,Ridge)\nLasso<-cbind(Intercept,Lasso)\n\nRidge<-cbind(lambda_grid,Ridge)\nLasso<-cbind(lambda_grid,Lasso)\n\nRidge_t<-as.data.frame(t(Ridge))\nLasso_t<-as.data.frame(t(Lasso))\n\nfor (i in 1:100) {\n  Ridge_t[-1,i]<-coefficient_lambda(lambda_grid$penalty[i],0)\n  Lasso_t[-1,i]<-coefficient_lambda(lambda_grid$penalty[i],1)\n}\n\nRidge_coeffi<-as.data.frame(t(Ridge_t))\nLasso_coeffi<-as.data.frame(t(Lasso_t))\n\n\nggplot(data=Ridge_coeffi) + \n  geom_line(aes(x=penalty,y=ms_sub_class), color=\"1\") + \n  geom_line(aes(x=penalty,y=lot_frontage), color=\"2\") +\n  geom_line(aes(x=penalty,y=lot_area), color=\"3\") +\n  geom_line(aes(x=penalty,y=overall_qual,), color=\"4\") + \n  geom_line(aes(x=penalty,y=overall_cond), color=\"5\") +\n  geom_line(aes(x=penalty,y=year_built), color=\"6\") +\n  geom_line(aes(x=penalty,y=year_remod_add), color=\"7\")+\n  geom_line(aes(x=penalty,y=mas_vnr_area), color=\"8\")+\n  geom_line(aes(x=penalty,y=bsmt_fin_sf1), color=\"9\")+\n  geom_line(aes(x=penalty,y=bsmt_fin_sf2), color=\"10\")+\n  geom_line(aes(x=penalty,y=bsmt_unf_sf), color=\"11\")+ \n  labs(title=\"Coefficients in Ridge\", x =\"Lambda\", y = \"Coefficients\")\n\n\n\ncode\n\nggplot(data=Lasso_coeffi) + \n  geom_line(aes(x=penalty,y=ms_sub_class), color=\"1\") + \n  geom_line(aes(x=penalty,y=lot_frontage), color=\"2\") +\n  geom_line(aes(x=penalty,y=lot_area), color=\"3\") +\n  geom_line(aes(x=penalty,y=overall_qual,), color=\"4\") + \n  geom_line(aes(x=penalty,y=overall_cond), color=\"5\") +\n  geom_line(aes(x=penalty,y=year_built), color=\"6\") +\n  geom_line(aes(x=penalty,y=year_remod_add), color=\"7\")+\n  geom_line(aes(x=penalty,y=mas_vnr_area), color=\"8\")+\n  geom_line(aes(x=penalty,y=bsmt_fin_sf1), color=\"9\")+\n  geom_line(aes(x=penalty,y=bsmt_fin_sf2), color=\"10\")+\n  geom_line(aes(x=penalty,y=bsmt_unf_sf), color=\"11\")+\n  labs(title=\"Coefficients in Lasso\", x =\"Lambda\", y = \"Coefficients\")\n\n\n\n\nìœ„ì—ì„œ ë§Œë“  ë°ì´í„° í…Œì´ë¸”ì„ í† ëŒ€ë¡œ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³´ì•˜ë‹¤. ì•„ì§ ë³€ìˆ˜ë¥¼ ë‹¤ ë‹´ëŠ” ë°©ë²•ì„ ëª°ë¼ 11ê°œ ë³€ìˆ˜ë¥¼ ì„ ì •í•˜ì—¬ ë„£ì—ˆë‹¤.\nLasso ì˜ ê²½ìš° Penaltyê°€ ì¦ê°€í• ìˆ˜ë¡ ì„ ì •í•œ ëª¨ë“  ë³€ìˆ˜ë“¤ì´ ë‚˜ì¤‘ì—ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\nLasso ì™€ ë‹¬ë¦¬ Ridge ì—ì„œëŠ” ì™„ì „ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²½ìš°ê°€ ì—†ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n\n\ncode\n\nwrite.csv(Ridge_coeffi, row.names = FALSE,\n          \"Ridge.csv\")\n\nwrite.csv(Lasso_coeffi, row.names = FALSE,\n          \"Lasso.csv\")\n\n\n\n\n\n\n",
    "preview": "posts/3. Ridge & Lasso /Ridge-and-Lasso_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-05-31T11:12:43+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2. ë¸”ë™ìˆ„ì¦ˆ ëª¨í˜•ì„ í†µí•œ ì˜µì…˜ ê°€ê²© ê³„ì‚°/",
    "title": "Bermuda option from Black Scholes",
    "description": "ê¸ˆìœµê³µí•™::ë¸”ë™ìˆ„ì¦ˆ ëª¨í˜•ì„ í†µí•´ Bermuda Option ê°€ê²©ë¥¼ êµ¬í•´ë³´ì.",
    "author": [
      {
        "name": "WooramSeong",
        "url": {}
      }
    ],
    "date": "2020-09-18",
    "categories": [],
    "contents": "\n\nContents\nBermuda Option\n\nBermuda Option\nCase <- 100 ì‹œë‚˜ë¦¬ì˜¤ ê°¯ìˆ˜Ini.Stock<-100 ì´ˆê¸° ì£¼ê°€StrikePrice<-100 ì˜µì…˜ í–‰ì‚¬ ê°€ê²©year<- 10 ë…„ë„unit<- 12 ë‹¨ìœ„ (12: 12ê°œì›”, 365, 365ì¼)interval <- year X unit (êµ¬ê°„ì˜ ê°¯ìˆ˜)t <- 1/unit ë…„ë„ë¥¼ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ ì¤€ ìˆ˜Â  r <-0.03sigma<-0.2mu <- r-0.5 X sigma^2Bermuda.Start <- 50Bermuda.End <- 110 ë²„ë®¤ë‹¤ì˜ ë§ˆì§€ë§‰ êµ¬ê°„ì€ intervalê³¼ ê°™ê±°ë‚˜ ì‘ì•„ì•¼í•¨.\n\n\ncode\n\nBermuda_option <-function(CallorPut,Case,Ini.Stock,StrikePrice,year,unit,r,sigma,Bermuda.Start,Bermuda.End){\n  interval <- year * unit\n  t <- 1/unit\n  mu <- r-0.5*sigma^2\n  \n  set.seed(5)\n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  Bermuda.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  if (CallorPut == 1){  \n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,Ini.Stock*exp(Yield.table[i,j+1])-StrikePrice)  \n      }}} else if (CallorPut == 2){\n        for (i in 1:Case) {\n          for (j in 1:interval) {\n            \n            Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n            Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n          }}} else { print(\"Call =1 , Put =2 ë¡œ ì„¤ì •í•´ì£¼ì…”ì•¼ í•©ë‹ˆë‹¤.\")\n          }\n  \n  \n  \n  if (Bermuda.End>0 & Bermuda.End<=interval){\n    for (i in 1:Case) { \n      Bermuda.table[i,1]<-max(Payoff.table[i,Bermuda.Start:Bermuda.End],Payoff.table[i,interval])\n      \n    }  \n    value<-mean(Bermuda.table[,1])\n  }  else {value<-mean(Payoff.table[,interval])\n  }\n  \n  \n  return(value)\n}\n\n\n\n\n\ncode\n\nBermuda_putop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma,Bermuda.Start,Bermuda.End){\n  interval <- year * unit\n  t <- 1/unit\n  mu <- r-0.5*sigma^2\n  \n  set.seed(5)\n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  Bermuda.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  if (Bermuda.End>0 & Bermuda.End<=interval){\n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n      }\n      Bermuda.table[i,1]<-max(Payoff.table[i,Bermuda.Start:Bermuda.End],Payoff.table[i,interval])} \n    value<-mean(Bermuda.table[,1])\n  } else {\n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n      }}\n    value<-mean(Payoff.table[,interval])\n  }\n  \n  \n  return(value)\n}\n\nBermuda_callop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma,Bermuda.Start,Bermuda.End){\n  interval <- year * unit\n  t <- 1/unit\n  mu <- r-0.5*sigma^2\n  \nset.seed(5)\n\nrnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n\nYield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\nPayoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\nBermuda.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n\nif (Bermuda.End>0 & Bermuda.End<=interval){\n  for (i in 1:Case) {\n    for (j in 1:interval) {\n      \n      Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n      Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,Ini.Stock*exp(Yield.table[i,j+1])-StrikePrice)  \n    }\n    Bermuda.table[i,1]<-max(Payoff.table[i,Bermuda.Start:Bermuda.End],Payoff.table[i,interval])} \n  value<-mean(Bermuda.table[,1])\n} else {\n  for (i in 1:Case) {\n    for (j in 1:interval) {\n      \n      Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n      Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,Ini.Stock*exp(Yield.table[i,j+1])-StrikePrice)  \n    }}\n  value<-mean(Payoff.table[,interval])\n}\n\n\nreturn(value)\n}\n\nBermuda_putop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma,Bermuda.Start,Bermuda.End){\n  interval <- year * unit\n  t <- 1/unit\n  mu <- r-0.5*sigma^2\n  \n  set.seed(5)\n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  Bermuda.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  if (Bermuda.End>0 & Bermuda.End<=interval){\n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n      }\n      Bermuda.table[i,1]<-max(Payoff.table[i,Bermuda.Start:Bermuda.End],Payoff.table[i,interval])} \n    value<-mean(Bermuda.table[,1])\n  } else {\n    for (i in 1:Case) {\n      for (j in 1:interval) {\n        \n        Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n        Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n      }}\n    value<-mean(Payoff.table[,interval])\n  }\n  \n  \n  return(value)\n}\n\n\n\n\n\ncode\n\nEuropean_callop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma){\n  \n  \n  interval <- year * unit\n  t <- year/interval\n  mu <- r-0.5*sigma^2\n  set.seed(5)\n  #ì‹œë®¬ë ˆì´ì…˜ ë‚œìˆ˜ ê°’ì„ ê³ ì •ì‹œí‚¤ë ¤ë©´ set.seed(10) <10ì€ pageì™€ ê°™ìŒ> 10í˜ì´ì§€ì— ìˆëŠ” ë‚œìˆ˜ê°’. \n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  European.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  for (i in 1:Case) {\n    for (j in 1:interval) {\n      \n      Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n      Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,Ini.Stock*exp(Yield.table[i,j+1])-StrikePrice)  \n    }\n    European.table[i,1]<-Payoff.table[i,interval]\n  }\n  \n  value<-mean(European.table[,1])\n  \n  return(value)\n}\n\nEuropean_putop <-function(Case,Ini.Stock,StrikePrice,year,unit,r,sigma){\n  \n  \n  interval <- year * unit\n  t <- year/interval\n  mu <- r-0.5*sigma^2\n  set.seed(5)\n  #ì‹œë®¬ë ˆì´ì…˜ ë‚œìˆ˜ ê°’ì„ ê³ ì •ì‹œí‚¤ë ¤ë©´ set.seed(10) <10ì€ pageì™€ ê°™ìŒ> 10í˜ì´ì§€ì— ìˆëŠ” ë‚œìˆ˜ê°’. \n  \n  rnd.num <- as.data.frame(matrix(rnorm(Case*interval), nrow = Case, ncol=interval))\n  \n  Yield.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval+1))\n  Payoff.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = interval))\n  European.table <- as.data.frame(matrix(data = 0, nrow = Case, ncol = 1))\n  \n  for (i in 1:Case) {\n    for (j in 1:interval) {\n      \n      Yield.table[i,j+1] <-  Yield.table[i,j] + (mu*t+(sigma*sqrt(t)*rnd.num[i,j]))\n      Payoff.table[i,j]  <-  exp(-r*j/unit)*max(0,StrikePrice-Ini.Stock*exp(Yield.table[i,j+1]))  \n    }\n    European.table[i,1]<-Payoff.table[i,interval]\n  }\n  \n  value<-mean(European.table[,1])\n  \n  return(value)\n}\n\nBermuda_option(3,100,100,100,5,12,0.01,0.1,0,0)\n\n\n[1] \"Call =1 , Put =2 ë¡œ ì„¤ì •í•´ì£¼ì…”ì•¼ í•©ë‹ˆë‹¤.\"\n[1] 0\n\ncode\n\nBermuda_option(2,100,100,100,5,12,0.02,0.1,0,0)\n\n\n[1] 2.920645\n\ncode\n\nBermuda_option(1,100,100,80,5,12,0.03,0.1,0,0)\n\n\n[1] 33.56165\n\ncode\n\nBermuda_callop(100,100,80,5,12,0.02,0.2,20,40)\n\n\n[1] 54.56337\n\ncode\n\nBermuda_putop(100,100,100,5,12,0.01,0.1,0,0)\n\n\n[1] 4.413762\n\ncode\n\nEuropean_callop(100,100,100,5,12,0.01,0.1)\n\n\n[1] 11.41853\n\ncode\n\nEuropean_putop(100,100,100,5,12,0.01,0.1)\n\n\n[1] 4.413762\n\nIf you want to get the plots about bermuda options, Write this down.\n\n\ncode\n\nresult.table.r<-c(1:100)\nresult.table.sigma<-c(1:100)\nresult.table.StrikePrice<-c(1:100)\nresult.table.r2<-c(1:100)\nresult.table.sigma2<-c(1:100)\nresult.table.StrikePrice2<-c(1:100)\nsample.r<- seq(0.01,0.03,length.out = 100)\nsample.sigma<-seq(0.1,0.3,length.out = 100)\nsample.StrikePrice<-seq(80,120,length.out = 100)\n\n\n\nfor (i in 1:100) {\n  result.table.r[i]<-Bermuda_option(1,100,100,100,5,12,sample.r[i],0.2,20,40)\n  result.table.sigma[i]<-Bermuda_option(1,100,100,100,5,12,0.02,sample.sigma[i],20,40)\n  result.table.StrikePrice[i]<-Bermuda_option(1,100,100,sample.StrikePrice[i],5,12,0.02,0.2,20,40)\n  result.table.r2[i]<-Bermuda_option(1,100,100,100,5,12,sample.r[i],0.2,0,0)\n  result.table.sigma2[i]<-Bermuda_option(1,100,100,100,5,12,0.02,sample.sigma[i],0,0)\n  result.table.StrikePrice2[i]<-Bermuda_option(1,100,100,sample.StrikePrice[i],5,12,0.02,0.2,0,0)\n  \n}\n\nplot(sample.r,result.table.r,type = \"l\",col=\"red\",xlab = \"ë¬´ìœ„í—˜ì´ììœ¨\",ylab=\"ì½œì˜µì…˜ ê°€ê²©\",xlim=c(0.009,0.031),ylim=c(10,50))\nlines(sample.r,result.table.r2,type = \"l\")\nlegend(0.026,48,c(\"European\",\"Bermuda\"),col=c(\"black\",\"red\"),lty=1,cex=0.9)\n\nplot(sample.sigma,result.table.sigma,type = \"l\",col=\"red\",xlab = \"í‘œì¤€í¸ì°¨(ì‹œê·¸ë§ˆ)\",ylab=\"ì½œì˜µì…˜ ê°€ê²©\",xlim=c(0.05,0.40),ylim=c(10,50))\nlines(sample.sigma,result.table.sigma2,type = \"l\")\nlegend(0.32,49,c(\"European\",\"Bermuda\"),col=c(\"black\",\"red\"),lty=1,cex=0.9)\n\nplot(sample.StrikePrice,result.table.StrikePrice,type = \"l\",col=\"red\",xlab = \"í–‰ì‚¬ê°€ê²©\",ylab=\"ì½œì˜µì…˜ ê°€ê²©\",xlim=c(75,125),ylim=c(10,50))\nlines(sample.StrikePrice,result.table.StrikePrice2)\nlegend(114,49,c(\"European\",\"Bermuda\"),col=c(\"black\",\"red\"),lty=1,cex=0.9)\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-31T08:39:06+00:00",
    "input_file": {}
  },
  {
    "path": "posts/1. Monte Carlo ë¥¼ ì´ìš©í•œ NPV ìë™ ê³„ì‚°/",
    "title": "NPV Monte carlo",
    "description": "ì¬ë¬´ê´€ë¦¬::ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ Net Present Valueë¥¼ êµ¬í•´ë³´ì.",
    "author": [
      {
        "name": "WooramSeong",
        "url": {}
      }
    ],
    "date": "2020-03-18",
    "categories": [],
    "contents": "\n\nContents\nlibrary\nNPV Function with Tax (Monte carlo Simulation)\n\n\nlibrary\n\n\ncode\n\nlibrary(dplyr)\n\n\n\nNPV Function with Tax (Monte carlo Simulation)\nInitial Investment = ICASHFLOW before tax = CFGrowth rate = GTax rate = T Cost of Captial = CPeriod = N\n\n\ncode\n\nNPVRAM <-function(I,CF,G,T,C,N) {\n  PVCF = 0 \n  ACF= CF*(1-T)  ##After tax cash flow\n  for(i in 1:N){\n    PVCF[i]<-ACF*(1+G)^(i-1) / (1+C)^i \n  }\n  \n  sum(PVCF)-I}\n\n### Monte Function ###\n\nMonte <- function(PROB,RV) {if(PROB<=0.25){\n  return(RV[1])}\n  else if(PROB<=0.75) {\n    return(RV[2])\n  }\n  else  { \n    return(RV[3])\n  }}      \n\n\n\nNPVRAM(5000000,1000000,0.03,0.25,0.06,10)\n\n\n[1] 1239103\n\ncode\n\nA<-c(5500000,5000000,4500000)\nB<-c(900000,1000000,1100000)\nC<-c(0.02,0.03,0.04)\nD<-c(0.35,0.25,0.15)\nE<-c(0.07,0.06,0.05)\nF<-c(8,10,12)\n\n\nResult<-0\n\n\n### NPVMONTE function ###\n\n\nNPVMONTE <- function(x) {\n  \n  NPV1<-c(1:x)  \n  for(i in 1:x){\n    \n    I1<-Monte(runif(1),A)\n    CF1<-Monte(runif(1),B)\n    G1<-Monte(runif(1),C)\n    T1<-Monte(runif(1),D)\n    C1<-Monte(runif(1),E)\n    N1<-Monte(runif(1),F)\n    NPV1[i] <- NPVRAM(I1,CF1,G1,T1,C1,N1)                   \n  }\n  return(NPV1) \n}\n\nResult1<-NPVMONTE(10000)\nmean(Result1)  ## NPV ???Õ°?\n\n\n[1] 1238165\n\ncode\n\nvar(Result1)   ## NPV var ??\n\n\n[1] 1.33932e+12\n\ncode\n\nResult1<-as.data.frame(Result1)\ncolnames(Result1) <- \"NPV\"\nResult1<-arrange(Result1,NPV)\nPositive<- filter(Result1,NPV>0)  \nnrow(Positive)/nrow(Result1)     ## NPV ?? ?????? È®?? ##\n\n\n[1] 0.8516\n\ncode\n\nAM<- filter(Result1,NPV>1000000) ## Above  1Million\nBM<- filter(Result1,NPV<1000000) ## Below  1Million\nnrow(AM)/nrow(Result1)   ## NPV 1,000,000 ?Ê°??? È®??\n\n\n[1] 0.5539\n\ncode\n\nnrow(BM)/nrow(Result1)   ## NPV 1,000,000 ?Ì¸??? È®??\n\n\n[1] 0.4461\n\ncode\n\nResult1[100,1]   ## 1  percentile ##\n\n\n[1] -1091259\n\ncode\n\nResult1[1000,1]  ## 10 percentile ##\n\n\n[1] -205871.8\n\ncode\n\nResult1[9000,1]  ## 90 percentile ##\n\n\n[1] 2786013\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-31T08:13:22+00:00",
    "input_file": {}
  }
]
