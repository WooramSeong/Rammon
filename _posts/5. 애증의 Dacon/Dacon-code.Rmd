---
title: "ì• ì¦ì˜ Dacon"
description: How to execute Random Forest, Logistic, XGboost, LightGBM Model?

author:
  - name: WooramSeong
date: 05-31-2021
output:
  distill::distill_article:
    self_contained: false
    html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 5
    fig_height: 4
    theme: cosmo
    highlight: tango
    code_folding: code
---

ì´ë²ˆ ê¸€ì€ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì§€ëŠ” ì•Šìœ¼ë ¤ê³  í•œë‹¤.
ì™œëƒí•˜ë©´ ë„ˆë¬´ ì˜¤ë˜ê±¸ë¦¬ê¸° ë•Œë¬¸ì´ì£ ..

ì½”ë“œë§Œ ê°ìƒí•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.


# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬

```{r eval=FALSE}

library(parsnip)
library(xgboost)
library(magrittr)
library(tidymodels)
library(tidyverse) 
library(skimr)
library(knitr)
library(kableExtra)
library(ranger)
library(yardstick)
library(stacks)
library(randomForestExplainer)
library(corrplot)
library(tictoc)
library(nnet)
#install.packages('devtools')
#remotes::install_github("curso-r/treesnip")
library(treesnip)
#devtools::install_github("curso-r/rightgbm")
#rightgbm::install_lightgbm()  
library(lightgbm)

```

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
  
```{r eval=FALSE}
  file_path <- "C:/Users/GIGABYTE/Desktop/ì‘ìš©í†µê³„í•™/Dacon"

  
  files <-list.files(file_path)
  
  files

  #ê° ë³€ìˆ˜ì˜ ì´ë¦„ì„ `janitor` íŒ¨í‚¤ì§€ë¡œ ë§ë”í•˜ê²Œ ë°”ê¿”ì¤€ë‹¤.
  
  
  train <- read_csv(file.path(file_path, "train.csv"),
                    col_types = cols(
                      credit = col_factor(levels = c("0.0", "1.0", "2.0"))
                    )) %T>% 
    suppressMessages() %>% 
    janitor::clean_names()
  test <- read_csv(file.path(file_path, "test.csv")) %T>%
    suppressMessages() %>% 
    janitor::clean_names()
  
```  
 
# ë°ì´í„° ê¸°ë³¸ì •ë³´ í™•ì¸
```{r eval=FALSE}  
train %>% 
      head() %>% 
       kable() %>% 
     kableExtra::kable_styling("striped") %>% 
     kableExtra::scroll_box(width = "100%")

```  
  
#ê° ë°ì´í„° ì…‹ì˜ ë³€ìˆ˜ëª…ì„ ì‚´í´ë³´ì. 
  
ë¨¼ì € `test` ë°ì´í„°ì—ëŠ” ìš°ë¦¬ê°€ ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ ë³€ìˆ˜ì¸ `credit` ë³€ìˆ˜ê°€ ë“¤ì–´ìˆì§€ ì•Šì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 
  
  
ë°ì´í„°ë¥¼ í›‘ì–´ë³´ê¸° ìœ„í•´ì„œ `skim()` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì. ì´ í•¨ìˆ˜ëŠ” ë°ì´í„°ì— ë“¤ì–´ìˆëŠ” ë³€ìˆ˜ë“¤ì„ íƒ€ì… ë³„ë¡œ ë¶„ì„í•´ì„œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì¤€ë‹¤.
  
#  skim(train)

ê²°ê³¼ë¥¼ ì‚´í´ë³´ì. ë¨¼ì € ê²°ì¸¡ì¹˜ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë§ì´ ì—†ëŠ” ì°©í•œ? ë°ì´í„°ì´ë‹¤. character ë³€ìˆ˜ì˜ complete rateë¥¼ ì‚´í´ë³´ë©´ ëª¨ë“  ë³€ìˆ˜ê°€ 1ì´ê³ , `occyp_type` ë³€ìˆ˜ë§Œì´ ê²°ì¸¡ì¹˜ê°€ 8171ê°œê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë˜í•œ ê³ ë§™ê²Œë„ numeric ë³€ìˆ˜ì˜ ê²°ì¸¡ì¹˜ëŠ” í•˜ë‚˜ë„ ì—†ë‹¤!ğŸ˜†
  
  
ê°™ì€ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ `test` ì…‹ì„ ë³´ë©´ ë˜‘ê°™ì€ íŒ¨í„´ì„ ê°€ì§€ê³  ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
  
#  skim(test)
  
```{r eval=FALSE} 
# ì‹œê°í™”

  #ë² ì´ìŠ¤ ë¼ì¸ì„ ì¡ì€ ë¬¸ì„œì´ë‹ˆ ê°„ë‹¨í•˜ê²Œ ì‹œê°í™” í•˜ë‚˜ë§Œ í•˜ê³  ë„˜ì–´ê°€ì. (ì½”ë“œë¥¼ ì‘ìš©í•´ì„œ ë‹¤ë¥¸ ë³€ìˆ˜ì— ëŒ€í•œ ìƒê´€ ê´€ê³„ë¥¼ ë³¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.)
  

pre_train<-train[,c(-1,-17)]

pre_train %<>% mutate(income_Per_Familymember = income_total/family_size)

#ë³€ìˆ˜ ì¤‘ìš”ë„ê°€ ë†’ì€ Begin_month, income_total, yrs_birth, income_PM


#Preliminary_fit2 <- ranger(credit ~ ., data = pre_train , importance = "impurity_corrected")

#pvalues<-importance_pvalues(Preliminary_fit2, method = "janitza") 

#pvalues



#p-valueê°€ 0ì— ê°€ê¹Œìš´ Begin_month, days_birth, days_employed ì„ ë°œê²¬


#train %>%
  
#  ggplot(aes(x = factor(credit), y = income_total)) +
  
#  geom_boxplot()

## Creditì´ ë°”ë€Œì–´ë„ income_totalì˜ ìœ ì˜ë¯¸í•œ ê²°ê³¼ê°€ ë³´ì´ì§€ ì•ŠìŒ


#train %>%
  
#  ggplot(aes(x = factor(credit), y = days_birth)) +
  
#  geom_boxplot()

#train %>%
  
#  ggplot(aes(x = factor(credit), y = begin_month)) +
  
#  geom_boxplot()


train %>%
  
  ggplot(aes(x = factor(credit), y = days_employed)) +
  
  geom_boxplot()


## ë°ì´í„° 365,243 ì²˜ë¦¬ë¥¼ ì–´ë–»ê²Œ í•´ì•¼ í•˜ëŠ”ê°€ 365,243,ì„ ì œì™¸í•œ ìµœëŒ“ê°’ or ìµœì†Ÿê°’ or ì¤‘ì•™ê°’


train %>% filter(days_employed>300000)

train %>% filter(days_employed>300000)%>%select(income_type,days_employed)

min(Days_employed)

max(Days_employed)

median(Days_employed)

train$days_employed[train$days_employed == 365243] <- -1977

test$days_employed[test$days_employed == 365243] <- -1977

## Creditì´ ë°”ë€Œì–´ë„ income_totalì˜ ìœ ì˜ë¯¸í•œ ê²°ê³¼ê°€ ë³´ì´ì§€ ì•ŠìŒ

post_train<-train
post_train$credit<-as.numeric(post_train$credit)
corrplot(cor(post_train[,c(20,19,12,11,6)]))

#begin_Month ë°ì´í„° ì „ì²˜ë¦¬ê°€ í•µì‹¬ì¸ê±° ê°™ë‹¤.

train %>%
  
 ggplot(aes(x = factor(credit), y = income_total)) +
  
  geom_boxplot() +
  
  facet_grid(. ~ occyp_type)

dim(train)

min(train$income_total)


train %<>% filter(income_total<1250000 & income_total>27000)

dim(train)



colnames(train)

train %>%
  
 ggplot(aes(x = factor(credit), y = begin_month)) +
  
  geom_boxplot() +
  
  facet_grid(. ~ occyp_type)



#  train %>%
  
#  ggplot(aes(x = factor(credit), y = begin_month)) +
  
#  geom_boxplot() +
  
#  facet_grid(. ~ income_type)

  
  train %>%
    
    ggplot(aes(x = factor(credit), y = begin_month)) +
    
    geom_boxplot() +
    
    facet_grid(. ~ income_type)


train %>%

 ggplot(aes(x = factor(credit), y = family_size)) +

  geom_boxplot() 

train %>% select(income_type,begin_month) %>% filter(income_type=="Student")
train %>% select(credit,occyp_type,begin_month) %>% filter(occyp_type=='HR staff' & credit == "1.0") %>% arrange(desc(begin_month))
train %>% select(credit,occyp_type,begin_month) %>% filter(occyp_type=='Realty agents' & credit == "1.0") %>% arrange(begin_month)
train %>% select(credit,occyp_type,begin_month) %>% filter(occyp_type=='Waiters/barmen staff' & credit == "1.0") %>% arrange(begin_month)



intersect(which(train$occyp_type == 'HR staff'),which(train$begin_month == -8))
intersect(which(train$occyp_type == 'Realty agents'& train$credit== "1.0"),which(train$begin_month == -53))
intersect(which(train$occyp_type == 'Realty agents' & train$credit== "1.0"),which(train$begin_month == -43))
intersect(which(train$occyp_type == 'Waiters/barmen staff' & train$credit== "1.0"),which(train$begin_month == -56))
intersect(which(train$income_type == 'Student'),which(train$begin_month == -60))

#2990,11276,15649,20515,25602, í–‰ì„ ì œê±°í•˜ì

train<-train[c(-2990,-11276,-15649,-20515,-25602),]


## income_totalì´ ìƒê´€ê³„ìˆ˜ê°€ 0ì— ê°€ê¹Œì›Œì„œ ì‚¬ëŒìˆ˜ë¡œ ë‚˜ëˆ„ì–´ í‰ê·  ìˆ˜ì…ìœ¼ë¡œ ë³€ìˆ˜ë¥¼ ë§Œë“¤ë ¤ í–ˆìœ¼ë‚˜ ìƒê´€ê³„ìˆ˜ê°€ ì—­ì‹œë‚˜ ì ì—ˆë‹¤. 
#train$credit<-as.numeric(train$credit)
#train %<>% mutate(income_PM = income_total/family_size)
#train$income_PM
#sum(is.na(train$income_PM))
#cor(train[,c(20,19,12,11,6,21)])
#corrplot(cor(train[,c(20,19,12,11,21)]))

#ëª©í‘œ ë³€ìˆ˜ì¸ creditì€ ë‚®ì„ ìˆ˜ë¡ ë†’ì€ ì‹ ìš©ì˜ ì‹ ìš©ì¹´ë“œ ì‚¬ìš©ìë¥¼ ì˜ë¯¸ í•œë‹¤ê³  í•œë‹¤. Commercial associate ì¸ ê²½ìš° ì‹ ìš©ì´ ì œì¼ ë‚®ì€ ê·¸ë£¹ì˜ ìˆ˜ì…ì˜ ì¤‘ì•™ê°’ì´ ì œì¼ ë†’ë‹¤. ëˆì„ ë§ì´ ë²Œìˆ˜ë¡ ëˆ ê°šì€ ê°œë…ì´ ì—†ì–´ì§€ëŠ” ê²ƒì¸ê°€? ì¬ë¯¸ìˆëŠ” í˜„ìƒì´ë‹¤. í•™ìƒ í´ë˜ìŠ¤ì˜ ê²½ìš° train ë°ì´í„°ì— ì…‹ì´ ë§ì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¶”í›„ì— ë‹¤ë¥¸ í´ë˜ìŠ¤ë¡œ í†µí•©ì„ ì‹œí‚¤ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒì´ë‹¤.
```


## ì¤‘ë³µê°’ì„ ì œê±°í•´ë³´ì. 
```{r eval=FALSE} 
birth_table<-as.data.frame(table(train$days_birth))  
birth_table%>%filter(Freq>10)%>% arrange(desc(Freq))

#days_birth -12676,-15519,-14667 íƒìƒ‰!!

a1<-train%>%filter(days_birth == -12676) %>% select(index,days_employed,income_total,begin_month,days_birth,family_size,credit)%>%arrange(days_employed,income_total,begin_month,index)
a2<-train%>%filter(days_birth == -15519) %>% select(index,days_employed,income_total,begin_month,days_birth,family_size,credit)%>%arrange(days_employed,income_total,begin_month,index)
a3<-train%>%filter(days_birth == -14667) %>% select(index,days_employed,income_total,begin_month,days_birth,family_size,credit)%>%arrange(days_employed,income_total,begin_month,index)

train$credit<-as.numeric(train$credit)

#train_max<-train%>%group_by(days_employed,gender,income_total,credit,family_size,child_num) %>% mutate(begin_month=min(begin_month))
#train_min<-train%>%group_by(days_employed,gender,income_total,credit,family_size,child_num) %>% mutate(begin_month=max(begin_month))
#train_median<-train%>%group_by(days_employed,gender,income_total,credit,family_size,child_num) %>% mutate(begin_month=median(begin_month))

#corrplot(cor(train_max[,c(20,19,12,11,6)]))
#corrplot(cor(train_min[,c(20,19,12,11,6)]))
#corrplot(cor(train_median[,c(20,19,12,11,6)]))
#corrplot(cor(train[,c(20,19,12,11,6)]))


#ë°ì´í„° ì •ë ¬ í–‰ë²ˆí˜¸ê°€ ê°€ì¥ ë¹ ë¥¸ ì• ë“¤ë§Œ ë‚¨ê²Œ ë˜ê¸° ë•Œë¬¸ì— ë°œê¸‰ì¼ì´ ê°€ì¥ ì˜¤ë˜ëœ ê²ƒë“¤ì„ ë‚¨ê¸°ê¸°ë¡œ í•˜ì.
colnames(train)

#train_order <- train[order(train[,'begin_month'],decreasing = FALSE), ]
#train_order <- train_order[!duplicated(train[,c('days_birth','work_phone','phone','reality','house_type','car','days_employed','edu_type','occyp_type','child_num','income_total','income_type','family_size','begin_month','credit')]),]
#train_order$credit<-as.numeric(train_order$credit)
#corrplot(cor(post_train[,c(20,19,12,11,6)]))
#corrplot(cor(train_order[,c(20,19,12,11,6)]))

#ì´ë²ˆì—ëŠ” ìµœê·¼ ë°œê¸‰ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚¨ê¸°ê¸°ë¡œ í•˜ì.
#train_order <- train[order(train[,'begin_month'],decreasing = TRUE), ]
#train_order <- train_order[!duplicated(train[,c('days_birth','work_phone','phone','reality','house_type','car','days_employed','edu_type','occyp_type','child_num','income_total','income_type','family_size','begin_month','credit')]),]
#train_order$credit<-as.numeric(train_order$credit)
#corrplot(cor(post_train[,c(20,19,12,11,6)]))
#corrplot(cor(train_order[,c(20,19,12,11,6)]))

## Begin_month ì¤‘ë³µê°’ ì¤‘ ìµœê·¼ ë°œê¸‰ì¼ì„ ë‚¨ê¸´ ê²°ê³¼ ìƒê´€ ê´€ê³„ê°€ ë†’ì•„ì§€ëŠ” ê²°ê³¼ê°€ ë‚˜íƒ€ë‚¬ë‹¤.

#train_order_inc <- train[order(train[,'begin_month'],decreasing = FALSE), ]
#train_order_inc <- train_order_inc[!duplicated(train[,c('gender','edu_type','flag_mobil','email','days_birth','work_phone','phone','reality','house_type','car','days_employed','edu_type','occyp_type','child_num','income_total','income_type','family_size','credit')]),]
#train_order_dec <- train[order(train[,'begin_month'],decreasing = TRUE), ]
#train_order_dec <- train_order_dec[!duplicated(train[,c('gender','edu_type','flag_mobil','email','days_birth','work_phone','phone','reality','house_type','car','days_employed','edu_type','occyp_type','child_num','income_total','income_type','family_size','credit')]),]

#train_union<-union(train_order_dec,train_order_inc)
#train_order$credit<-as.numeric(train_order$credit)
#train_order_inc$credit<-as.numeric(train_order_inc$credit)
#train_order_dec$credit<-as.numeric(train_order_dec$credit)
#train_union$credit<-as.numeric(train_union$credit)

#corrplot(cor(post_train[,c(20,19,12,11,6)]))
#corrplot(cor(train_order_inc[,c(20,19,12,11,6)]))
#corrplot(cor(train_order_dec[,c(20,19,12,11,6)]))
#corrplot(cor(train_union[,c(20,19,12,11,6)]))
#train_order_dec

#train<-train_union
## ì¬ë°ŒëŠ”ì ì€ ìƒê´€ê´€ê³„ê°€ ë†’ë‹¤ê³  í•´ì„œ mean_log_lossê°€ ë‚®ì•„ì§€ëŠ”ê±´ ì•„ë‹˜
## ì˜¤íˆë ¤ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìª½ì—ì„œ log loss ê°’ì´ ë” í¬ê²Œ ë‚˜ì˜´ ~_~




# ì „ì²˜ë¦¬ í•˜ê¸°
#`tidymodels`ì—ì„œëŠ” ì „ì²˜ë¦¬ë¥¼ í•  ë•Œ `recipe` ë¼ëŠ” íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•œë‹¤. ì´ íŒ¨í‚¤ì§€ì—ëŠ” ì „ì²˜ë¦¬ë¥¼ í•˜ëŠ” ë°©ë²•ì„ ìŒì‹ ë ˆí”¼ì‹œ ì²˜ëŸ¼ ì ì–´ë†“ëŠ”ë‹¤ê³  ìƒê°í•˜ë©´ ì‰½ë‹¤.
## ì „ì²˜ë¦¬ ì‚¬í•­ë“¤



# ê²°ê³¼ê°’ì¸ credit ë³€ìˆ˜ì™€ character íƒ€ì…ì˜ ë³€ìˆ˜ë“¤ì„ factor ë³€ìˆ˜ë¡œ ë°”ê¿”ì£¼ì.

# ë‚˜ì´ì™€ ì§ì—…ì„ ê°€ì§„ ê¸°ê°„ì„ ë…„ìˆ˜ë¡œ ë°”ê¿”ì¤€ë‹¤.
```

# `recipe`ë¥¼ í†µí•œ ì „ì²˜ë¦¬ ì…ë ¥
```{r eval=FALSE} 
credit_recipe <- train %>% 
  
  recipe(credit ~ .) %>% 
  
  # age and employment period in yrs
  
  #step_mutate(yrs_birth = -ceiling(days_birth/365),
              
  #           yrs_employed = -ceiling(days_employed/365)) %>% 
  
  step_rm(index,flag_mobil, child_num) %>%
  
  #child_numì€ family sizeë‘ ìƒê´€ê³„ìˆ˜ê°€ ë§¤ìš° ë†’ì•„ ì œê±°í•´ì£¼ì—ˆë‹¤. 
  
  #flag_mobilì€ ë‹¤ ê°’ì´ 1ë¡œ ë™ì¼í•˜ì—¬ ì œê±°í•´ì£¼ì—ˆë‹¤.
  
  step_unknown(occyp_type) %>% 
  
  step_integer(all_nominal(), -all_outcomes()) %>% 
  
  step_corr(all_predictors(), -all_outcomes()) %>% 
  
  step_scale(all_predictors(), -all_outcomes()) %>%
  
  step_nzv(all_predictors(), -all_outcomes()) %>%
  
  step_center(all_predictors(), -all_outcomes()) %>% 
  
  prep(training = train)



print(credit_recipe)
```

# `juice`ë¥¼ í†µí•œ ì „ì²˜ë¦¬ ì¦™ì§œê¸°
```{r eval=FALSE} 

#`juice()` í•¨ìˆ˜ë¥¼ í†µí•´ì„œ recipeì— ì…ë ¥ëœ ì „ì²˜ë¦¬ë¥¼ ì§œë‚¸ ë°ì´í„°ë¥¼ ì–»ì–´ì˜¨ë‹¤.

train2 <- juice(credit_recipe)
test2 <- bake(credit_recipe, new_data = test)
head(train2)
head(test2)

train2$credit<-as.factor(train2$credit)


#ë‹¤ìŒê³¼ ê°™ì´ ê²°ì¸¡ì¹˜ ì—†ì´ ì˜ ì½”ë”©ëœ ë°ì´í„°ë¥¼ ì–»ì—ˆë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.




train2 %>%
    
  map_df(~sum(is.na(.))) %>%
  
  pivot_longer(cols = everything(),
               
               names_to = "variable",
               
               values_to = "na_count") %>% 
  
  filter(na_count > 0)

```

# íŠœë‹ ì¤€ë¹„í•˜ê¸°

`validation_split()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ì…‹ì„ ë¶„ë¦¬í•œë‹¤. í•œ ë‹¨ê³„ ë” ë‚˜ì•„ê°„ cross validationì€ `vfold_cv()`í•¨ìˆ˜ì—ì„œ ì œê³µí•˜ë‹ˆ ì°¾ì•„ë³´ë„ë¡ í•˜ì.


## 5 fold vs 10 foldë¥¼ ì‹¤í–‰í•´ë³´ì. 
```{r eval=FALSE} 

set.seed(2002)


validation_split <- vfold_cv(v=10, train2, strata = credit)

#validation_split <- validation_split(train2, prop = 0.3,   strata = credit)

validation_split
```


## stacking ì¤€ë¹„í•˜ê¸°
```{r eval=FALSE} 
ctrl_res <- control_stack_resamples()
ctrl_grid <- control_stack_grid()
```


# ëœë¤ í¬ë ˆìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•´ë³´ì, 

mtryì™€ min_nì„ ì–´ë–»ê²Œ ì •í• ì§€ë¥¼ í‰ê°€ì…‹ì„ í†µí•´ì„œ ê²°ì •í•  ê²ƒì´ë¯€ë¡œ, `tune()`ë¥¼ ì‚¬ìš©í•´ì„œ tidymodelsì—ê²Œ ì•Œë ¤ì£¼ë„ë¡ í•œë‹¤.

```{r eval=FALSE} 


cores <- parallel::detectCores() -1

cores



tune_spec <- rand_forest(mtry = tune(),
                         
                         min_n = tune(),
                         
                         trees = 1000) %>% 
  
  set_engine("ranger",
             
             num.threads = cores) %>% 
  
  set_mode("classification")

# from param tune

#param_grid <- tibble(mtry = c(4,4,4,4), min_n=c(3,4,5,6)) # mtry=3ìœ¼ë¡œ ê³ ì •ì‹œí‚¤ì
param_grid <- grid_random(finalize(mtry(), x = train2[,-1]), min_n(),size = 5000, filter = 2<mtry & mtry<5 & min_n<15 & 2<min_n)
param_grid

# ì›Œí¬ í”Œë¡œìš° ì„¤ì •

workflow <- workflow() %>%
  
  add_model(tune_spec) %>% 
  
  add_formula(credit ~ .)


# ëª¨ë¸ íŠœë‹ with tune_grid()



# Tuning trees

tic()

tune_result <- workflow %>% 
  
  tune_grid(validation_split,
            grid = param_grid,
            metrics = metric_set(mn_log_loss),
            control = ctrl_res)

toc()


tune_result %>% 
  
  collect_metrics()



# íŠœë‹ê²°ê³¼ ì‹œê°í™”

tune_result %>%
  
  collect_metrics() %>%
  
  filter(.metric == "mn_log_loss") %>% 
  
  ggplot(aes(mtry, mean, color = .metric)) +
  
  geom_line(size = 1.5) +
  
  scale_x_log10() +
  
  theme(legend.position = "none") +
  
  labs(title = "Mean Log loss")



tune_result %>% show_best()
tune_best <- tune_result %>% select_best(metric = "mn_log_loss")

tune_best$mtry

tune_best$min_n


```
1ì°¨ì‹œë„ (3,5), (3,6), (3,7), (3,8), (3,4) , minimum mean log loss  0.708 5 fold cv ìµœê·¼ ì‹ ìš©ì¹´ë“œ ë°œê¸‰ ê¸°ë¡ë§Œ ë‚¨ê¸°ê¸°
<br/>
2ì°¨ì‹œë„ (3,5), (3,7), (3,6), (3,4), (3,8) , minimum mean log loss  0.706 5 fold cv ì²«ë²ˆì§¸ ì‹ ìš©ì¹´ë“œ ë°œê¸‰ ê¸°ë¡ë§Œ ë‚¨ê¸°ê¸°
<br/>
3ì°¨ì‹œë„ (3,4), (3,5), (3,7), (3,6), (3,8) , minimum mean log loss  0.698 10 fold CV + ì²«ë²ˆì§¸ ì‹ ìš©ì¹´ë“œ ë°œê¸‰ ê¸°ë¡ë§Œ ë‚¨ê¸°ê¸°
<br/>
4ì°¨ì‹œë„ (4,4), (4,5) , minimum mean log loss  0.404 5 fold CV + ë™ì¼ ê³ ê°ì´ ì—¬ëŸ¬ë²ˆ ë°œê¸‰í•  ê²½ìš° begin_monthë¥¼ ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œë¡œ í†µì¼ 
<br/>
5ì°¨ì‹œë„ (4,4), (4,5) , minimum mean log loss  0.404 5 fold CV + ë™ì¼ ê³ ê°ì´ ì—¬ëŸ¬ë²ˆ ë°œê¸‰í•  ê²½ìš° begin_monthë¥¼ ê°€ì¥ ëŠ¦ì€ ë‚ ì§œë¡œ í†µì¼ 
<br/>
6ì°¨ì‹œë„ (3,5), (3,6) , minimum mean log loss  0.697 10 fold CV + ë³€ì£¼ ì œê±°ë¥¼ index ë‘ flag_mobil ë§Œ í•¨  


## íŠœë‹ëœ ëª¨ë¸ í•™ìŠµí•˜ê¸°
```{r eval=FALSE} 
rf_model <- 
  
  rand_forest(mtry = tune_best$mtry,
              
              min_n = tune_best$min_n,
              
              trees = 1000) %>% 
  
  set_engine("ranger", seed = 2022, 
             
             num.threads = cores, importance = 'impurity') %>% 
  
  set_mode("classification")


rf_model2 <- 
  
  rand_forest(mtry = tune_best$mtry,
              
              min_n = tune_best$min_n,
              
              trees = 1000) %>% 
  
  set_engine("ranger", seed = 2023, 
             
             num.threads = cores, importance = 'impurity') %>% 
  
  set_mode("classification")

rf_model3 <- 
  
  rand_forest(mtry = tune_best$mtry,
              
              min_n = tune_best$min_n,
              
              trees = 1000) %>% 
  
  set_engine("ranger", seed = 2024, 
             
             num.threads = cores, importance = 'impurity') %>% 
  
  set_mode("classification")



tictoc::tic()

rf_fit <- 
  
  rf_model %>% 
  
  fit(credit ~ ., data = train2)

tictoc::toc()

rf_fit2 <- 
  
  rf_model2 %>% 
  
  fit(credit ~ ., data = train2)

rf_fit3 <- 
  
  rf_model3 %>% 
  
  fit(credit ~ ., data = train2)

rf_fit
rf_fit2
rf_fit3
```
Ranger result


 Type:                             Probability estimation 

 Number of trees:                  1000 

 Sample size:                      24806 

 Number of independent variables:  16 

 Mtry:                             3 

 Target node size:                 5 

 Variable importance mode:         impurity 

 Splitrule:                        gini 

 OOB prediction error (Brier s.):  0.2288811 

# Logitstic modelì„ ëŒë ¤ë³´ì. 
```{r eval=FALSE} 
logit_spec <- multinom_reg(penalty = tune(),
                          mixture = tune()) %>%
  set_engine("glmnet")%>%
  set_mode("classification")
  

lambda_grid <- grid_regular(penalty(), 
                            mixture(),
                            levels = list(penalty = 100,
                                          mixture = 25))
lambda_grid

logit_workflow <-workflow() %>% 
  add_model(logit_spec) %>%
  add_formula(credit~.)

logit_result<- logit_workflow %>%
  tune_grid(validation_split,
            grid=lambda_grid,
            metrics = metric_set(mn_log_loss),
            control = ctrl_res) 

logit_result %>% collect_metrics()
logit_result %>% show_best()
```

1ì°¨ì‹œë„ Penalty 0.00870 Mixture 0.111, Penalty 0.00775 Mixture 0.111, minimum mean log loss  0.859 10-fold CV ì²«ë²ˆì§¸ ì‹ ìš©ì¹´ë“œ ë°œê¸‰ ê¸°ë¡ë§Œ ë‚¨ê¸°ê¸°
<br/>
2ì°¨ì‹œë„ Penalty 0.00955 Mixture 0.0833, Penalty 0.00775 Mixture 0.111, minimum mean log loss  0.863 10-fold CV ì‹ ìš©ì¹´ë“œ ì¤‘ë³µ ì œê±° X
<br/>


**ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨í˜•ì€ ì¤‘ë³µì„ ì œê±°í•˜ëŠ”ê²Œ ë‚«ê³ , ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨í˜•ì€ ì¤‘ë³µê°’ì„ ì‚­ì œ í•˜ì§€ ì•ŠëŠ”ê²Œ mean log lossê°€ ë” ì˜ ë‚˜ì˜´** 


```{r eval=FALSE} 
logit_tune_best <- logit_result %>% select_best(metric="mn_log_loss")
logit_tune_best$penalty

logit_model <- multinom_reg(penalty = logit_tune_best$penalty,
                  mixture = logit_tune_best$mixture) %>% 
                  set_engine("glmnet", seed = 2022, num.threads = cores) %>%            
                  set_mode("classification")

logit_fit <- logit_model %>% fit(credit~., data = train2)
logit_fit

logit_pred <- predict(logit_fit, test2, type="prob")
logit_pred
```


# XGboost ë¥¼ ì‹¤í–‰í•´ë³´ì

```{r eval=FALSE}

xgb_spec <- boost_tree(
  
  trees = 1000, 
  
  tree_depth = tune(), min_n = tune(), 
  
  loss_reduction = tune(),                     ## first three: model complexity
  
  sample_size = tune(), mtry = tune(),         ## randomness
  
  learn_rate = tune(),                         ## step size
  
) %>% 
  
  set_engine("xgboost") %>% 
  
  set_mode("classification")



xgb_spec



xgb_grid <- grid_latin_hypercube(
  
  tree_depth(),
  
  min_n(c(1,10)),
  
  loss_reduction(c(-1,0.1)),
  
  sample_size = sample_prop(range=c(0.5,1)),
  
  learn_rate(c(-1,0)),
  
  size = 100
  
)


xgb_grid



xgb_workflow <- workflow() %>%
    add_model(xgb_spec) %>% 
    add_formula(credit ~ .)


xgb_result<- xgb_workflow %>%
  
  tune_grid(validation_split,
            grid=xgb_grid,
            metrics = metric_set(mn_log_loss),
            control = ctrl_res) 

xgb_result %>% collect_metrics()
xgb_result %>% show_best()

xgb_tune_best <- xgb_result %>% select_best(metric="mn_log_loss")
```


1ì°¨ ê²°ê³¼ mtry 3 min_n 13 tree_depth 13 learn late 0.126 loss reduction 1.19, sample size 0.983  mean log loss 0.726
<br/>
2ì°¨ ê²°ê³¼ mtry 3 min_n 7 tree_depth 10 learn late 0.183 loss reduction 0.921, sample size 0.910  mean log loss 0.733
<br/>
3ì°¨ ê²°ê³¼ mtry 3 min_n 4 tree_depth 11 learn late 0.110 loss reduction 1.11, sample size 0.891  mean log loss 0.722
<br/>

```{r eval=FALSE}
xgb_model <- boost_tree(
   trees = 1000, 
   tree_depth = xgb_tune_best$tree_depth, 
   min_n = xgb_tune_best$min_n, 
   loss_reduction = xgb_tune_best$loss_reduction,## first three: model complexity
   sample_size = xgb_tune_best$sample_size, mtry = xgb_tune_best$mtry,         ## randomness
   learn_rate = xgb_tune_best$learn_rate,                         ## step size
  ) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

xgb_fit <- 
    xgb_model %>% fit(credit ~ ., data = train2)


xgb_fit
```

# light GBM ë¥¼ ì‹¤í–‰í•´ë³´ì
```{r eval=FALSE}
gbm_spec <- boost_tree(
  
  trees = 1000, 
  
  tree_depth = tune(), min_n = tune(), 
  
  loss_reduction = tune(),                     ## first three: model complexity
  
  sample_size = tune(), mtry = tune(),         ## randomness
  
  learn_rate = tune(),                         ## step size
  
) %>% 
  
  set_engine("lightgbm") %>% 
  
  set_mode("classification")

gbm_spec
gbm_grid <- grid_latin_hypercube(
  
  mtry(c(3,9)),
  
  tree_depth(c(11,15)),
  
  min_n(c(3,11)),
  
  loss_reduction(c(-1,0.1)),
  
  sample_size = sample_prop(range=c(0.5,1)),
  
  learn_rate(c(-1,0)),
  
  size = 100 
)


gbm_grid


gbm_workflow <- workflow() %>%
  add_model(gbm_spec) %>% 
  add_formula(credit ~ .)


tic()

gbm_result<- gbm_workflow %>%
    tune_grid(validation_split,
            grid=gbm_grid,
            metrics = metric_set(mn_log_loss),
            control = ctrl_res) 

toc()


gbm_result %>% collect_metrics()
gbm_result %>% show_best()

gbm_tune_best <- gbm_result %>% select_best(metric="mn_log_loss")
```
gbm 1ì°¨ ê²°ê³¼ mtry 4 min_n 13 tree_depth 15 learn late 0.11 loss reduction 0.141, sample size 0.845  mean log loss 0.725
<br/>
gbm 2ì°¨ ê²°ê³¼ mtry 8 min_n 3 tree_depth 14 learn late 0.104 loss reduction 0.327, sample size 0.582  mean log loss 0.718
<br/>
gbm 3ì°¨ ê²°ê³¼ mtry 7 min_n 11 tree_depth 14 learn late 0.121 loss reduction 0.126, sample size 0.882  mean log loss 0.717
<br/>
gbm 4ì°¨ ê²°ê³¼ mtry 8 min_n 6 tree_depth 15 learn late 0.124 loss reduction 0.242, sample size 0.675  mean log loss 0.715
<br/>
gbm 5ì°¨ ê²°ê³¼ mtry 9 min_n 11 tree_depth 15 learn late 0.169 loss reduction 0.148, sample size 0.664  mean log loss 0.716

```{r eval=FALSE}

**íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì¢€ ë” í•˜ë©´ ìœ ì˜ë¯¸í•œ ê²°ê³¼ê°€ ë‚˜ì˜¬ê±° ê°™ë‹¤.**


gbm_model <- boost_tree(
  trees = 1000, 
  tree_depth = gbm_tune_best$tree_depth, 
  min_n = gbm_tune_best$min_n, 
  loss_reduction = gbm_tune_best$loss_reduction,## first three: model complexity
  sample_size = gbm_tune_best$sample_size, mtry = gbm_tune_best$mtry,         ## randomness
  learn_rate = gbm_tune_best$learn_rate,                         ## step size
) %>% 
  set_engine("lightgbm") %>% 
  set_mode("classification")

gbm_fit <- 
  gbm_model %>% fit(credit ~ ., data = train2)

gbm_fit
```


# Stacking ì„ ì‹¤í–‰í•´ë³´ì. 

##ëœë¤í¬ë ˆìŠ¤íŠ¸ì—ì„œ 2ê°œ + ë¡œì§€ìŠ¤í‹± ëª¨í˜•ì—ì„œ 1ê°œ + xgboost ì—ì„œ 2ê°œ + lightGBM ì—ì„œ 2ê°œ 


## ê·¸ë¦¬ë“œ ì¬ì„¤ì • 

```{r eval=FALSE}


param_grid <- tibble(mtry=c(3,3),min_n=c(5,6))
lambda_grid <- tibble(mixture=c(0.1),penalty=c(0.0087))
xgb_grid <- tibble(mtry=c(3,3),min_n=c(13,4),tree_depth=c(13,11),learn_rate=c(0.126,0.11),loss_reduction=c(1.19,1.11),sample_size=c(0.983,0.891))
gbm_grid <- tibble(mtry=c(8,9),min_n=c(6,11),tree_depth=c(15,15),learn_rate=c(0.124,0.169),loss_reduction=c(0.242,0.148),sample_size=c(0.675,0.664))

tic()
tune_result <- workflow %>% 
  tune_grid(validation_split,
            grid = param_grid,
            metrics = metric_set(mn_log_loss),
            control = ctrl_res)

logit_result<- logit_workflow %>%
  tune_grid(validation_split,
            grid=lambda_grid,
            metrics = metric_set(mn_log_loss),
            control = ctrl_res) 

xgb_result<- xgb_workflow %>%
  tune_grid(validation_split,
            grid=xgb_grid,
            metrics = metric_set(mn_log_loss),
            control = ctrl_res) 

gbm_result<- gbm_workflow %>%
  tune_grid(validation_split,
            grid=gbm_grid,
            metrics = metric_set(mn_log_loss),
            control = ctrl_res) 
toc()

credit_stacking <- 
  stacks() %>% 
  add_candidates(tune_result) %>% 
  add_candidates(logit_result) %>%
  add_candidates(xgb_result)  %>%
  add_candidates(gbm_result)

credit_stacking
as_tibble(credit_stacking)

stacking_model <- credit_stacking %>% blend_predictions() %>% fit_members() 

stacking_model

final_result <- predict(stacking_model, test2,type = "prob")

final_result
```


# ì˜ˆì¸¡í•˜ê¸°

```{r eval=FALSE}
result1 <- predict(rf_fit, test2, type = "prob")
result1 %>% head()

result2 <- predict(rf_fit2, test2, type = "prob")
result2 %>% head()

result3 <- predict(rf_fit3, test2, type = "prob")
result3 %>% head()

result4 <- predict(xgb_fit, test2, type = "prob")
result4 %>% head()

result5 <- predict(gbm_fit, test2, type = "prob")
result5 %>% head()


result<-(result1+result2+result3)/3
result %>% head()

softresult<- (result3+logit_pred)/2
softresult %>% head()

submission <- read_csv(file.path(file_path, "sample_submission.csv"))

sub_col <- names(submission)

submission <- bind_cols(submission$index, final_result)

names(submission) <- sub_col

write.csv(submission, row.names = FALSE,
          
          "rammon33.csv")
  
